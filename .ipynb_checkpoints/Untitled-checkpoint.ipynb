{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94deb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (0.16.4)\n",
      "Requirement already satisfied: filelock in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (from huggingface_hub) (23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages (from requests->huggingface_hub) (2023.7.22)\n",
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "    \n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token: "
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378088cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"bigcode/starcoder\"\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "inputs = tokenizer.encode(\"def print_hello_world():\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22813f39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodellama/CodeLlama-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     12\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m sequences \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimport socket\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdef ping_exponential_backoff(host: str):\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:724\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m         tokenizer_class \u001b[38;5;241m=\u001b[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 724\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    725\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    726\u001b[0m         )\n\u001b[1;32m    727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'import socket\\n\\ndef ping_exponential_backoff(host: str):',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "646bf391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1130, in _get_module\n",
      "    if failed:\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/generation/utils.py\", line 27, in <module>\n",
      "    from ..integrations.deepspeed import is_deepspeed_zero3_enabled\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/integrations/__init__.py\", line 21, in <module>\n",
      "    from .deepspeed import (\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/integrations/deepspeed.py\", line 29, in <module>\n",
      "    from ..optimization import get_scheduler\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/optimization.py\", line 27, in <module>\n",
      "    from .trainer_utils import SchedulerType\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/trainer_utils.py\", line 32, in <module>\n",
      "    from .utils import (\n",
      "ImportError: cannot import name 'is_torch_xpu_available' from 'transformers.utils' (/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/__init__.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1130, in _get_module\n",
      "    if failed:\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py\", line 32, in <module>\n",
      "    from ...modeling_utils import PreTrainedModel\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 39, in <module>\n",
      "    from .generation import GenerationConfig, GenerationMixin\n",
      "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1120, in __getattr__\n",
      "    # Raise an error for users who might not realize that classes without \"TF\" are torch-only\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1132, in _get_module\n",
      "RuntimeError: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\n",
      "cannot import name 'is_torch_xpu_available' from 'transformers.utils' (/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/__init__.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_23049/1790998897.py\", line 1, in <module>\n",
      "    from transformers import AutoTokenizer, LlamaForCausalLM\n",
      "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1121, in __getattr__\n",
      "    if \"torch\" in backends and \"tf\" not in backends and not is_torch_available() and is_tf_available():\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1120, in __getattr__\n",
      "    # Raise an error for users who might not realize that classes without \"TF\" are torch-only\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1132, in _get_module\n",
      "RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):\n",
      "Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\n",
      "cannot import name 'is_torch_xpu_available' from 'transformers.utils' (/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "from human_eval.data import write_jsonl, read_problems\n",
    "from tqdm import tqdm\n",
    "\n",
    "# initialize the model\n",
    "\n",
    "model_path = \"Phind/Phind-CodeLlama-34B-v1\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# HumanEval helper\n",
    "\n",
    "def generate_one_completion(prompt: str):\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "\n",
    "    # Generate\n",
    "    generate_ids = model.generate(inputs.input_ids.to(\"cuda\"), max_new_tokens=256, do_sample=True, top_p=0.75, top_k=40, temperature=0.1)\n",
    "    completion = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    completion = completion.replace(prompt, \"\").split(\"\\n\\n\\n\")[0]\n",
    "\n",
    "    return completion\n",
    "\n",
    "# perform HumanEval\n",
    "problems = read_problems()\n",
    "\n",
    "num_samples_per_task = 1\n",
    "samples = [\n",
    "    dict(task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"]))\n",
    "    for task_id in tqdm(problems)\n",
    "    for _ in range(num_samples_per_task)\n",
    "]\n",
    "write_jsonl(\"samples.jsonl\", samples)\n",
    "\n",
    "# run `evaluate_functional_correctness samples.jsonl` in your HumanEval code sandbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a6521f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1130, in _get_module\n",
      "    if failed:\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/pipelines/__init__.py\", line 35, in <module>\n",
      "    from ..utils import (\n",
      "ImportError: cannot import name 'find_adapter_config_file' from 'transformers.utils' (/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/__init__.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_23049/1710170196.py\", line 2, in <module>\n",
      "    from transformers import pipeline\n",
      "  File \"<frozen importlib._bootstrap>\", line 1055, in _handle_fromlist\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1120, in __getattr__\n",
      "    # Raise an error for users who might not realize that classes without \"TF\" are torch-only\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/import_utils.py\", line 1132, in _get_module\n",
      "RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n",
      "cannot import name 'find_adapter_config_file' from 'transformers.utils' (/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/utils/__init__.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"codellama/CodeLlama-34b-Instruct-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb179488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d728ce200c741e8a36da8c86f91d49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodellama/CodeLlama-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     12\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m sequences \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimport socket\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdef ping_exponential_backoff(host: str):\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:724\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m         tokenizer_class \u001b[38;5;241m=\u001b[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 724\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    725\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    726\u001b[0m         )\n\u001b[1;32m    727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'import socket\\n\\ndef ping_exponential_backoff(host: str):',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6aade446",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodellama/CodeLlama-13b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     11\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     12\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m sequences \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimport socket\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdef ping_exponential_backoff(host: str):\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     17\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:724\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m         tokenizer_class \u001b[38;5;241m=\u001b[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 724\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    725\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    726\u001b[0m         )\n\u001b[1;32m    727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"codellama/CodeLlama-13b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'import socket\\n\\ndef ping_exponential_backoff(host: str):',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1248189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve singleplayer \n",
    "\n",
    "#1. travel planning\n",
    "#2. collaboratively planning a garden with custom ferns - display an actual fasta, sgrna, pdbs and a ship to lab button based on an addres -> ask for address -> 200 lines of code away \n",
    "#3. literate programming for excel - write to an excel -> make a cool pattern like /r/place\n",
    "#4. assist with building communities -> automate repetitive tasks ->\n",
    "#5. assist with planning projects -> when code gets checked into github -> \n",
    "#6. teach people to code and have more fun learning math -> see the entire process can be used to do cool stuff -> trig -> hundred triangles becoming an origami crane\n",
    "#7. [check] make a gant chart for cooking dinner -> snickerdoodles\n",
    "\n",
    "0. make my robot clean my house every 3 days \n",
    "1. order these groceries every 3 days -> trader joes \n",
    "\n",
    "\n",
    "2. after an event -> write a 3 page article of everything you experienced and what questions you had \n",
    "-> write queries about text that transform the text into something that has an intersection with other peoples notes on the same event and then bundle and document everyones experience\n",
    "\n",
    "today i went to a concert, it was cool. my favorite part was the 2nd song\n",
    "my favorite part was the lights\n",
    "half users favorite was lights, half was 2nd song and half was unknown.\n",
    "\n",
    "\n",
    "\n",
    "#find intersection of two documents \n",
    "\n",
    "#encoding = 500 dimension that has no real meaning -> just used for statistical correlation within document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10e38309",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#copy(Array.from(document.querySelectorAll()).map(el => el.textContent))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(soup)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mgetText\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m, in \u001b[0;36mgetText\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetText\u001b[39m(url):\n\u001b[0;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to get the webpage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "url = 'https://www.twitch.tv/cohhcarnage'\n",
    "\n",
    "def getText(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to get the webpage\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    chat = soup.find_all('.chat-line__message')\n",
    "    #copy(Array.from(document.querySelectorAll()).map(el => el.textContent))\n",
    "    print(soup)\n",
    "getText(url)\n",
    "\n",
    "\n",
    "\n",
    "#make a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e07f160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get the webpage\n",
      "Failed to get the webpage\n"
     ]
    }
   ],
   "source": [
    "#! pip install requests beautifulsoup4\n",
    "\n",
    "url = 'http://www.paulgraham.com/articles.html'\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_paul_graham_articles():\n",
    "    url = \"http://www.paulgraham.com/articles.html\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to get the webpage\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a'):\n",
    "        link = a_tag.get('href', None)\n",
    "        if link:\n",
    "            links.append(\"http://www.paulgraham.com/\" + link)\n",
    "            \n",
    "    return links\n",
    "\n",
    "def getText(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to get the webpage\")\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    body = soup.find('body')\n",
    "    \n",
    "    if body:\n",
    "        return body.get_text()\n",
    "\n",
    "\n",
    "article_links = scrape_paul_graham_articles()\n",
    "\n",
    "import json\n",
    "with open('all_pg.txt', 'w') as file :\n",
    "    json.dump([getText(link) for link in article_links], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8b8772e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"  asdfasdf\": [\"Ammendment:  asdfasdf\", \"Explanation:  asdf\", \"Suggestion:  asdf\", \"Explanation:  asdf\", \"Ammendment:  asdf\", \"Ammendment:  asdf\", \"Suggestion:  asdf\", \"Ammendment:  asdf\", \"Query:  asdfasdf\", \"Explanation:  asdf\", \"Suggestion:  asdf\", \"Explanation:  asdf\", \"Explanation:  asdf\", \"Query:  asdf\", \"Ammendment:  asdf\", \"Query:  asdf\", \"Explanation:  asdfasfasdfasdfasdfasfafsadfasdfasdfasfdfasfasfsfas\", \"Query:  asdfasdf\", \"Ammendment:  asdf\", \"Ammendment:  asdf\", \"Ammendment:  asdf\", \"Suggestion:  asdf\", \"Ammendment:  asdf\", \"Retraction:  asdf\", \"Retraction:  asdf\", \"Query:  asdfasdf\", \"Ammendment:  asdf\", \"Ammendment:  asdf\", \"Ammendment:  asdf\", \"Explanation:  asf\", \"Suggestion:  asdf\", \"Explanation:  asdf\", \"Query:  asdf\", \"Ammendment:  asdf\", \"Explanation:  asdfasdf\", \"Retraction:  asdf\", \"Ammendment:  asdf\", \"Explanation:  asdf\", \"Explanation:  asdf\", \"Query:  asdf\", \"Explanation:  asdf\", \"Retraction:  asdf\", \"Retraction:  asdfasdf\", \"Ammendment:  asdf\", \"Suggestion:  asdf\", \"Query:  asdf\", \"Retraction:  asdf\", \"Suggestion:  asdf\", \"Suggestion:  asdf\", \"Ammendment:  asdf\", \"Explanation:  asdfasfasdfasdfasdfasfafsadfasdfasdfasfdfasfasfsfas\", \"Explanation:  asdfasdf\", \"Retraction:  asdf\", \"Query:  asdf\", \"Retraction:  asdf\", \"Retraction:  asdf\", \"Suggestion:  asdf\", \"Suggestion:  asdf\", \"Suggestion:  asdf\", \"Explanation:  asd\", \"Explanation:  asdfasdf\", \"Explanation:  asdf\", \"Query:  asdf\", \"Query:  asdf\", \"Ammendment:  asdf\", \"Retraction:  asdf\", \"Ammendment:  asdf\", \"Explanation:  asdf\", \"Explanation:  asdfasfasdfasdfasdfasfafsadfasdfasdfasfdfasfasfsfas\", \"Query:  asdfasdf\", \"Suggestion:  asdf\", \"Retraction:  asdf\", \"Ammendment:  asdf\", \"Explanation:  asdf\", \"Suggestion:  asdf\", \"Explanation:  asdf\", \"Query:  asdf\", \"Explanation:  asdfasfasdfasdfasdfasfafsadfasdfasdfasfdfasfasfsfas\", \"Explanation:  asdfasdf\", \"Explanation:  asdf\", \"Retraction:  asdf\", \"Query:  asdf\", \"Explanation:  asdf\", \"Suggestion:  asdf\", \"Retraction:  asdf\", \"Ammendment:  asdf\", \"Explanation:  asdfasfasdfasdfasdfasfafsadfasdfasdfasfdfasfasfsfas\"], \"  there are 300 cats in the neighborhood .\": [\"Retraction:  there are 300 cats in the neighborhood .\", \"Suggestion:  there are 300 cats in the neighborhood .\", \"Ammendment:  there are 300 cats in the neighborhood .\", \"Retraction:  there are 300 cats in the neighborhood .\", \"Ammendment:  there are 300 cats in the neighborhood .\", \"Query:  there are 300 cats in the neighborhood .\", \"Suggestion:  there are 300 cats in the neighborhood .\", \"Retraction:  there are 300 cats in the neighborhood .\", \"Ammendment:  there are 300 cats in the neighborhood .\", \"Explanation:  there are 300 cats in the neighborhood .\"], \"  there was a train being made from kansas to seattle .\": [\"Retraction:  there was a train being made from kansas to seattle .\", \"Ammendment:  there was a train being made from kansas to seattle .\", \"Explanation:  there was a train being made from kansas to seattle .\", \"Ammendment:  there was a train being made from kansas to seattle .\", \"Suggestion:  there was a train being made from kansas to seattle .\", \"Suggestion:  there was a train being made from kansas to seattle .\", \"Retraction:  there was a train being made from kansas to seattle .\", \"Explanation:  there was a train being made from kansas to seattle .\", \"Retraction:  there was a train being made from kansas to seattle .\", \"Explanation:  there was a train being made from kansas to seattle .\"], \"  yesterday i ate toast .\": [\"Explanation:  yesterday i ate toast .\", \"Explanation:  yesterday i ate toast .\", \"Ammendment:  yesterday i ate toast .\", \"Query:  yesterday i ate toast .\", \"Ammendment:  yesterday i ate toast .\", \"Explanation:  yesterday i ate toast .\", \"Explanation:  yesterday i ate toast .\", \"Explanation:  yesterday i ate toast .\", \"Retraction:  yesterday i ate toast .\", \"Suggestion:  yesterday i ate toast .\"], \"  asf\": [\"Query:  asf\", \"Suggestion:  asf\", \"Query:  asf\", \"Suggestion:  asf\", \"Query:  asf\", \"Suggestion:  asf\", \"Suggestion:  asf\", \"Query:  asf\", \"Retraction:  asf\"], \"  fasdfas\": [\"Suggestion:  fasdfas\", \"Suggestion:  asd\", \"Suggestion:  asdfasfasdfasdfasdfasfafsadfasdfasdfasfdfasfasfsfas\", \"Suggestion:  asdfasfasdfasdfasdfasfafsadfasdfasdfasfdfasfasfsfas\", \"Suggestion:  asd\", \"Suggestion:  asdfasfasdfasdfasdfasfafsadfasdfasdfasfdfasfasfsfas\"], \"  asd\": [\"Ammendment:  asd\", \"Ammendment:  asd\", \"Ammendment:  asd\"], \"  fasdfasdfasdfa\": [\"Explanation:  fasdfasdfasdfa\", \"Explanation:  fasdfasdfasdfafad\"], \"  asdfasfasdfasdfasdfasfafsadfasdfasdfasfdfasfasfsfas\": [\"Query:  asdfasfasdfasdfasdfasfafsadfasdfasdfasfdfasfasfsfas\", \"Query:  asdfasfasdfasdfasdfasfafsadfasdfasdfasfdfasfasfsfas\"], \"  fasdfasdfa\": [\"Suggestion:  fasdfasdfa\"], \"  fasdfasdfasd\": [\"Suggestion:  fasdfasdfasd\"]}\n"
     ]
    }
   ],
   "source": [
    "with open('./database.txt') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1fe4299d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: openai in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (0.27.4)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sentence-transformers in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (2.2.2)\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.6.0 from https://files.pythonhosted.org/packages/83/8d/f65f8138365462ace54458a9e164f4b28ce1141361970190eef36bdef986/transformers-4.32.1-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.32.1-py3-none-any.whl.metadata (118 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: torchvision in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (1.11.2)\n",
      "Requirement already satisfied: nltk in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: filelock in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: fsspec in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.8.8)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-macosx_10_11_x86_64.whl (4.0 MB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\n",
      "Requirement already satisfied: click in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Collecting torch>=1.6.0 (from sentence-transformers)\n",
      "  Using cached torch-2.0.0-cp39-none-macosx_10_9_x86_64.whl (139.8 MB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: sympy in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
      "Installing collected packages: tokenizers, torch, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.4\n",
      "    Uninstalling tokenizers-0.9.4:\n",
      "      Successfully uninstalled tokenizers-0.9.4\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.7.1\n",
      "    Uninstalling torch-1.7.1:\n",
      "      Successfully uninstalled torch-1.7.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.2.2\n",
      "    Uninstalling transformers-4.2.2:\n",
      "      Successfully uninstalled transformers-4.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openai-whisper 20230314 requires numba, which is not installed.\n",
      "point-e 0.0.0 requires matplotlib, which is not installed.\n",
      "point-e 0.0.0 requires scikit-image, which is not installed.\n",
      "allennlp 1.5.0 requires torch<1.8.0,>=1.6.0, but you have torch 2.0.0 which is incompatible.\n",
      "allennlp 1.5.0 requires transformers<4.3,>=4.1, but you have transformers 4.32.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.13.3 torch-2.0.0 transformers-4.32.1\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: datasets in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (2.14.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: WiktionaryParser in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (0.0.97)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from WiktionaryParser) (4.12.2)\n",
      "Requirement already satisfied: requests in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from WiktionaryParser) (2.31.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->WiktionaryParser) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->WiktionaryParser) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->WiktionaryParser) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->WiktionaryParser) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->WiktionaryParser) (2023.7.22)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.65.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fsspec in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting faster-whisper\n",
      "  Obtaining dependency information for faster-whisper from https://files.pythonhosted.org/packages/5d/ef/a6afc7bcfec7bf7e647e315813c898ce031a3bb3a5baaac3a3bf7d9ecf7b/faster_whisper-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading faster_whisper-0.7.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting av==10.* (from faster-whisper)\n",
      "  Downloading av-10.0.0-cp39-cp39-macosx_10_9_x86_64.whl (26.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ctranslate2<4,>=3.17 (from faster-whisper)\n",
      "  Obtaining dependency information for ctranslate2<4,>=3.17 from https://files.pythonhosted.org/packages/58/97/e1e10bd003ab19b4b104849e18bbb5adcff42ad6e2618d616456a5902d90/ctranslate2-3.19.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading ctranslate2-3.19.0-cp39-cp39-macosx_10_9_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.13 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from faster-whisper) (0.16.4)\n",
      "Requirement already satisfied: tokenizers==0.13.* in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from faster-whisper) (0.13.3)\n",
      "Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n",
      "  Obtaining dependency information for onnxruntime<2,>=1.14 from https://files.pythonhosted.org/packages/a5/95/cccadd11fb503f51743dc12705ff38a1a2990600c0bdcf2758011800e081/onnxruntime-1.15.1-cp39-cp39-macosx_10_15_x86_64.whl.metadata\n",
      "  Downloading onnxruntime-1.15.1-cp39-cp39-macosx_10_15_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numpy in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from ctranslate2<4,>=3.17->faster-whisper) (1.25.2)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from ctranslate2<4,>=3.17->faster-whisper) (6.0)\n",
      "Requirement already satisfied: filelock in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.13->faster-whisper) (3.0.12)\n",
      "Requirement already satisfied: fsspec in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.13->faster-whisper) (2023.6.0)\n",
      "Requirement already satisfied: requests in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.13->faster-whisper) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.13->faster-whisper) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.13->faster-whisper) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.13->faster-whisper) (23.0)\n",
      "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers (from onnxruntime<2,>=1.14->faster-whisper)\n",
      "  Obtaining dependency information for flatbuffers from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Requirement already satisfied: protobuf in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (4.23.4)\n",
      "Requirement already satisfied: sympy in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.12)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
      "Downloading faster_whisper-0.7.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading ctranslate2-3.19.0-cp39-cp39-macosx_10_9_x86_64.whl (14.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.15.1-cp39-cp39-macosx_10_15_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Installing collected packages: flatbuffers, av, humanfriendly, ctranslate2, coloredlogs, onnxruntime, faster-whisper\n",
      "Successfully installed av-10.0.0 coloredlogs-15.0.1 ctranslate2-3.19.0 faster-whisper-0.7.1 flatbuffers-23.5.26 humanfriendly-10.0 onnxruntime-1.15.1\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pronouncing in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: cmudict>=0.4.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from pronouncing) (1.0.13)\n",
      "Requirement already satisfied: importlib-metadata<6.0.0,>=5.1.0 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from cmudict>=0.4.0->pronouncing) (5.2.0)\n",
      "Requirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from cmudict>=0.4.0->pronouncing) (5.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/adnanwahab/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata<6.0.0,>=5.1.0->cmudict>=0.4.0->pronouncing) (3.11.0)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[207], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModel\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/whisper/__init__.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_audio, log_mel_spectrogram, pad_or_trim\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecodingOptions, DecodingResult, decode, detect_language\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelDimensions, Whisper\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranscribe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transcribe\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/whisper/model.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decode \u001b[38;5;28;01mas\u001b[39;00m decode_function\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect_language \u001b[38;5;28;01mas\u001b[39;00m detect_language_function\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranscribe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transcribe \u001b[38;5;28;01mas\u001b[39;00m transcribe_function\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelDimensions\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     n_mels: \u001b[38;5;28mint\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/whisper/transcribe.py:20\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     FRAMES_PER_SECOND,\n\u001b[1;32m     12\u001b[0m     HOP_LENGTH,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     pad_or_trim,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecodingOptions, DecodingResult\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtiming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_word_timestamps\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LANGUAGES, TO_LANGUAGE_CODE, get_tokenizer\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     exact_div,\n\u001b[1;32m     24\u001b[0m     format_timestamp,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     str2bool,\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/whisper/timing.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, List\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install sentence-transformers\n",
    "! pip install datasets \n",
    "! pip install WiktionaryParser\n",
    "! pip install transformers\n",
    "!pip install faster-whisper\n",
    "\n",
    "!pip install pronouncing\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datasets import load_dataset\n",
    "import openai\n",
    "import time\n",
    "import transformers\n",
    "import os \n",
    "import whisper\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoplayback = '../data_sets/' + 'output.mp3'\n",
    "\n",
    "# model = whisper.load_model(\"base\")\n",
    "\n",
    "# # load audio and pad/trim it to fit 30 seconds\n",
    "# audio = whisper.load_audio(videoplayback)\n",
    "# #audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# # make log-Mel spectrogram and move to the same device as the model\n",
    "# mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# # detect the spoken language\n",
    "# _, probs = model.detect_language(mel)\n",
    "# print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# # decode the audio\n",
    "# options = whisper.DecodingOptions(fp16=False)\n",
    "# result = whisper.decode(model, mel, options)\n",
    "\n",
    "# # print the recognized text\n",
    "# print(result.text)\n",
    "# from faster_whisper import WhisperModel\n",
    "\n",
    "# model_size = \"large-v2\"\n",
    "\n",
    "# # Run on GPU with FP16\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "\n",
    "# # or run on GPU with INT8\n",
    "# # model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "# # or run on CPU with INT8\n",
    "# # model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "# segments, info = model.transcribe(videoplayback, beam_size=5)\n",
    "\n",
    "# print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "# for segment in segments:\n",
    "#     print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "#https://github.com/guillaumekln/faster-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cafe032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarity(sentences):\n",
    "    corpus_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    #print(corpus_embeddings, sentences)\n",
    "    clusters = util.community_detection(corpus_embeddings, min_community_size=2, threshold=0.55)\n",
    "    def process(item): return [sentences[i] for i in item]\n",
    "    result = [process(item) for item in clusters ]\n",
    "    # print(clusters)\n",
    "    print('result,result,result',result)\n",
    "    return result\n",
    "random.shuffle(getSimilarity(stream_comments))\n",
    "#if @ -> reply https://www.promptingguide.ai/tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f16b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = []\n",
    "# with open('./archive/test_Arabic_tweets_negative_20190413.tsv') as f:\n",
    "#     for line in f:\n",
    "#         lines.append(line.split('\\t')[1])\n",
    "\n",
    "# openai.api_key = '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'\n",
    "# len(lines)\n",
    "\n",
    "# def translate_text(text, source_language, target_language):\n",
    "#     print('translating ' + text)\n",
    "#     prompt = f\"Translate the following '{source_language}' text to '{target_language}': {text}\"\n",
    "#     first = time.perf_counter()\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=\"gpt-3.5-turbo\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates text.\"},\n",
    "#             {\"role\": \"user\", \"content\": prompt}\n",
    "#         ],\n",
    "#         max_tokens=150,\n",
    "#         n=1,\n",
    "#         stop=None,\n",
    "#         temperature=0.5,\n",
    "#     )\n",
    "#     second = time.perf_counter()\n",
    "#     print(second - first)\n",
    "#     translation = response.choices[0].message.content.strip()\n",
    "#     return translation\n",
    "\n",
    "# tweets = [translate_text(line, 'arabic', 'english') for line in lines[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPairs(documentOne, documentTwo):\n",
    "    pairs = []\n",
    "    cosine_scores = util.cos_sim(rhe[:664], pge)\n",
    "    for i in range(len(cosine_scores)-1):\n",
    "        for j in range(i+1, len(cosine_scores)):\n",
    "            pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\n",
    "\n",
    "#[ for tweet in tweets]\n",
    "\n",
    "\n",
    "#reduce 3 paul graham essay and a gwern article into common subset of sentences with most similarity \n",
    "#LSTM -> looks at 4 sentences at a time -> tracks \"meaning\" of sentence as a 584 column vector and then \n",
    "#summarizies them -> and then finds similar parts \n",
    "#two notes have to be intersecting to be mergable \n",
    "#start with two notes that are mostly intersecting\n",
    "#give everyone 90 minutes to write out everything they know about a topic - extemporaenous \n",
    "#merge notes \n",
    "#given n documents - reduce\n",
    "    #given two documents -> \n",
    "    #return a list of clusters and their rank\n",
    "        #rank each cluster by the relevance of each sentence to the title\n",
    "        # map - return clusterID\n",
    "        #within each cluster -> use NLTK to find the AST -> categorize as -> 'preposition, statement, question, jest, curiosity'\n",
    "        # pairwise match most similar sentences - merge - TBD or out of scope\n",
    "        \n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "def encode(s1): return model.encode(s1, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb379cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the right training dataset -> 300million reddit comments = 2 million downloads + research papers \n",
    "#what is the right training data set for \"understanding twitch stream\"\n",
    "\n",
    "#sentiment analysis\n",
    "#rank by humor\n",
    "#rank by similarity \n",
    "#reorganize within topic\n",
    "#assuming there are 5 topics\n",
    "#person writes essay -> their essay is 'graded' or visualized as a venn diagram between theirs and nexus or their and all others \n",
    "#run spelling corrector uestionL how to find 300 non stream people to use app -> pay 300 students 5 dollars = 1500 \n",
    "#find them a deal on \n",
    "#write an essay for 5 minutes \n",
    "\n",
    "\n",
    "\n",
    "#subject of sentence = non-person verb acted upon\n",
    "#each classification has key structures that \n",
    "\n",
    "#theory -> i think\n",
    "#mockery = save scumming ? \n",
    "\n",
    "#use sentiment to categorize \n",
    "#use feature vector to categorize\n",
    "#use tags to categorize \n",
    "#see which one is more accurate\n",
    "import random\n",
    "\n",
    "\n",
    "tag_map = {\n",
    "  \"CC\": \"Coordinating conjunction\",\n",
    "  \"CD\": \"Cardinal number\",\n",
    "  \"DT\": \"Determiner\",\n",
    "  \"EX\": \"Existential there\",\n",
    "  \"FW\": \"Foreign word\",\n",
    "  \"IN\": \"Preposition or subordinating conjunction\",\n",
    "  \"JJ\": \"Adjective\",\n",
    "  \"JJR\": \"Adjective, comparative\",\n",
    "  \"JJS\": \"Adjective, superlative\",\n",
    "  \"LS\": \"List item marker\",\n",
    "  \"MD\": \"Modal\",\n",
    "  \"NN\": \"Noun, singular or mass\",\n",
    "  \"NNS\": \"Noun, plural\",\n",
    "  \"NNP\": \"Proper noun, singular\",\n",
    "  \"NNPS\": \"Proper noun, plural\",\n",
    "  \"PDT\": \"Predeterminer\",\n",
    "  \"POS\": \"Possessive ending\",\n",
    "  \"PRP\": \"Personal pronoun\",\n",
    "  \"PRP$\": \"Possessive pronoun\",\n",
    "  \"RB\": \"Adverb\",\n",
    "  \"RBR\": \"Adverb, comparative\",\n",
    "  \"RBS\": \"Adverb, superlative\",\n",
    "  \"RP\": \"Particle\",\n",
    "  \"SYM\": \"Symbol\",\n",
    "  \"TO\": \"to\",\n",
    "  \"UH\": \"Interjection\",\n",
    "  \"VB\": \"Verb, base form\",\n",
    "  \"VBD\": \"Verb, past tense\",\n",
    "  \"VBG\": \"Verb, gerund or present participle\",\n",
    "  \"VBN\": \"Verb, past participle\",\n",
    "  \"VBP\": \"Verb, non-3rd person singular present\",\n",
    "  \"VBZ\": \"Verb, 3rd person singular present\",\n",
    "  \"WDT\": \"Wh-determiner\",\n",
    "  \"WP\": \"Wh-pronoun\",\n",
    "  \"WP$\": \"Possessive wh-pronoun\",\n",
    "  \"WRB\": \"Wh-adverb\",\n",
    "    \".\": 'unknown_variable',\n",
    "    \",\": '',\n",
    "    ':': '',\n",
    "    '``': '',\n",
    "    \"''\": ''\n",
    "}\n",
    "def getClassification(string):\n",
    "    p = int(random.random() * 5)\n",
    "    nouns = findNouns(string)\n",
    "    verb_most_acted_on = nouns #findNouns(string)[0] if len(nouns) > 0 else ''\n",
    "    return f'{classifications[p]}:  {\" \".join(verb_most_acted_on)}'\n",
    "\n",
    "def processTag(tagged_sentence):\n",
    "    return [(orig,tag_map[actual_tag]) for (orig,actual_tag) in tagged_sentence if actual_tag in tag_map and 'noun']\n",
    "\n",
    "def findNouns(string):\n",
    "    return [noun for noun,tag in processTag(pos_tag(word_tokenize(string))) ]\n",
    "#food \n",
    "\n",
    "#[findNouns(string) for string in stream_comments]\n",
    "#within cluster -> tag topic by most common word if its that type of topic\n",
    "#processTag(pos_tag(word_tokenize(stream_comments[0])))\n",
    "\n",
    "[getClassification(string) for string in stream_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d5a8c0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9143ac24e424405b819283f797c167c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b47c8324f9a4de29e7afecfc53efb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1b8f9182fa4c5b8ce419f78c83e311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb70aa9c38744fb86ae935620583f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f22116c561460ea6484ada2fbb4c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': ' That chatter was wrong you canâ€™t just pay something',\n",
       " 'labels': ['Commentary',\n",
       "  'observation',\n",
       "  'stament ',\n",
       "  'Objection',\n",
       "  'Expletive',\n",
       "  '',\n",
       "  'Clarification',\n",
       "  'Conclusion',\n",
       "  'Retraction',\n",
       "  'Explanation',\n",
       "  'Mockery',\n",
       "  'Suggestion',\n",
       "  'Continuation',\n",
       "  'Query',\n",
       "  'Recitation',\n",
       "  'Evaluation',\n",
       "  'Answer',\n",
       "  'Ammendment',\n",
       "  'Theory',\n",
       "  'Qualification '],\n",
       " 'scores': [0.15687057375907898,\n",
       "  0.11215373873710632,\n",
       "  0.10924730449914932,\n",
       "  0.09347198903560638,\n",
       "  0.056935880333185196,\n",
       "  0.05337909609079361,\n",
       "  0.05303404480218887,\n",
       "  0.05030188336968422,\n",
       "  0.04959076642990112,\n",
       "  0.04209166765213013,\n",
       "  0.03855651989579201,\n",
       "  0.034422166645526886,\n",
       "  0.03373663127422333,\n",
       "  0.027700986713171005,\n",
       "  0.020344629883766174,\n",
       "  0.01711978204548359,\n",
       "  0.015175099484622478,\n",
       "  0.01473050843924284,\n",
       "  0.014638352207839489,\n",
       "  0.006498449016362429]}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_comments = [\n",
    "    \"ZeroTepMusic: 12\",\n",
    "    \"lysinehd: crabs in a bucket\",\n",
    "    \"bouillabased: my life's biggest mistake was trusting my parents, especially when they told me education was the key to success.\",\n",
    "    \"kylanc6: Americas is full of so many people with fucking peasant brain\",\n",
    "    \"Replying to @spyderfrommars: btw, so many people have filed for bankruptcy and that used to mean having your debts forgiven. Probably many chatters parents have. SO MANY PEOPLE IN THE POLITICAL ARENA HAVE HAD THEIR DEBTS FORGIVEN. It's so stupid to use that arguementshainybug: and bankruptcy doesn't wipe student loans \",\n",
    "    \"MarIsMar: Corpa\",\n",
    "    \"IsThatSalem: I hate this\",\n",
    "    \"eronin37:   \",\n",
    "    \"v3sh_:  HYPERCLAP turn education into businesses\",\n",
    "    \"hondewberry: CHATTERS NO, LET'S DO FUN SHIT BABY WHAT YOU DOIN\",\n",
    "    \"JEZZ_7: Corpa\",\n",
    "    \"xTrashPandaKingx: @HasanAbi you dont understand they're like one or two lucky breaks from being the 1%\",\n",
    "    \"fearandrespect: Man that's a weird boot to be licking, chatter what the hell\",\n",
    "    \"PrettyKrazy: profit motive destroys humanity\",\n",
    "    \"Sutiibun_: \",\n",
    "    \"qwertyopsd2: Chatting really Hasan?\",\n",
    "    \"Tamarama02: \\\"I couldn't care less.\\\"\",\n",
    "    \"whitneythegoth: YEP LEECHES\",\n",
    "    \"FriedWaffles:  just don't get sick or hurt\",\n",
    "    \"redeyeink: gatekeeping and control over labor conditions\",\n",
    "    \"BigDddyNick: sugar dads in chat?\",\n",
    "    \"austrom: Corpa\",\n",
    "    \"HalalChad_: America is a massive corporation\",\n",
    "    \"russianspy619: Chatting I'm very smart\",\n",
    "    \"dankusdingus: hasCapital\",\n",
    "    \"dr_desu:  gimme gimme gimme\",\n",
    "    \"LateAndNever: Pivo . o O ( Corpa ðŸ”«  )\",\n",
    "    \"phoneofff: @ashlynnicoleramirez report yourself as poor and never tell them anything ever again\",\n",
    "    \"ADK_215: they want to make money of your student loan debt how do you guys not see this\",\n",
    "    \"Leafy_Sh4de: Hey Hasan! My man! I know I longed for some twitch political commentary\",\n",
    "    \"WhyYouGotNecklace: YEP just looking for more capital avenues\",\n",
    "    \"meredyke: To profit in anyway imaginable\",\n",
    "    \"imLunchy: Corpa private prisons\",\n",
    "    \"thottopic666: YEP ó €€\",\n",
    "    \"thelookoutshift: that chatter didn't pay their loan and the govt took back their degree via lobotomy @hasanabi\",\n",
    "    \"whataburgerfancyketchup: Knowledge is power Hasan. Its that simple. Keep people dumb, keep them powerless. @HasanAbi\",\n",
    "    \"aquamiguel: Chatting ó €€\",\n",
    "    \"ya_plis: they need a controlled working force\",\n",
    "    \"ShakeN_Bake: Fuck them\",\n",
    "    \"moogerfooger_: can't get blood from a stone Crungo\",\n",
    "    \"sandsim: making 17 year olds take loans out OMEGALUL\",\n",
    "    \"stovetotheface: keep the masses dumb\",\n",
    "    \"Resubscribe: @luckypompom qalla_s modCheck\",\n",
    "    \"tr0piKEL1: I do that on medical bills as protest. I pay like $25/mo on $4,000 hospital bills  they donâ€™t report it to credit as long as you pay.\",\n",
    "    \"rentcontrolryan: just get a full ride scholarship EZ Chatting\",\n",
    "    \"shoriu_: chat is so annoying today\",\n",
    "    \"TheUh0hOreo:  HYPERCLAP\",\n",
    "    \"Replying to @ashlynnicoleramirez: income driven repayment plan and stay poor forevershainybug: me lmao\",\n",
    "    \"Shonnicus: paying a portion, you are still getting killed on the backend with interest. That's why they are fine with you not making full payments\",\n",
    "    \"catboy_rai: housing, food, etc.\",\n",
    "    \"YukiTsunoda__: \",\n",
    "    \"hashoe23: TATE\",\n",
    "    \"Faviahn: Because they're told they have to pay more in taxes and have less money when they're already struggling.\",\n",
    "    \"HerrosRevenge: ultimatley this argument boils down to \\\"its your fault for wanting to live and be happy\\\" most people dont want to go to school to work for the rest of their lives while paying to do so\",\n",
    "    \"seeayy: almost every aspect of higher education is profitable\",\n",
    "    \"TehAdamBomb: profit motives drive innocation \",\n",
    "    \"lordcharliesheen: YEE HAWWWW\",\n",
    "    \"HUGEGAMER96: Military YEP\",\n",
    "    \"clandestinie: Affordable but not always available\",\n",
    "    \"big_dykeenergy: iâ€™m below the income threshold so i get a payment of zero. i should clarify that i WORK FOR THE GOVERNMENT and iâ€™m not paid enough to meet the thresholds\",\n",
    "    \"aquamiguel: Corpa Clap\",\n",
    "    \"bignachysosa: Healthcare doesnt have to be free but it SHOULDNT be private\",\n",
    "    \"Cypres_warluckHyan8: americas kindaa fuked rn\",\n",
    "    \"ZuzieZozo: I literally had a free operation\",\n",
    "    \"quarantinewolf: Chatting Hasan @Hasanabi @Hasanthehun @Freedomeaglefuck\",\n",
    "    \"IgiveBluebells: Educaiton is overpriced in the US\",\n",
    "    \"librapelican: theres a reason my sociology class presented american exceptionalism as a form of propaganda\",\n",
    "    \"xygeek: @hasanabi allow for bankruptcy, then normalize bankruptcy at graduation. Problem solved. KEKW\",\n",
    "    \"Skill_Cylinder: YEP just join the military\",\n",
    "    \"Hagasha: \",\n",
    "    \"bigstephfan: and here healthcare is so fucking expensive.\",\n",
    "    \"BOATPARADE: can't have an educated proletariat\",\n",
    "    \"RoguePr1nc355: Now it is about political ideology\",\n",
    "    \"Zony66: you cant even have a fucking hobby without people asking you \\\"well how are you gunna make any money with that?\\\"\",\n",
    "    \"cas3_: no war but class war\",\n",
    "    \"Replying to @tr0piKEL1: I do that on medical bills as protest. I pay like $25/mo on $4,000 hospital bills rhyzKEK they donâ€™t report it to credit as long as you pay.aspiration89: YEP ó €€\",\n",
    "    \"SimUser:  You want my number to not go up???\",\n",
    "     \"narjuh: more than South Korea?\",\n",
    "    \"punishedribcorn: Education isnt free for the same reason healthcare isnt free. Because you cant live without it @hasanabi\",\n",
    "    \"GanjarDanks: @hasanabi true reason that education isn't free and student loans reign supreme is slabs\",\n",
    "    \"ok_eevee:  Paywall the labor force @hasanabi\",\n",
    "    \"kintu: there are worse neoliberal hellholes out there Aware\",\n",
    "    \"esquerdomacho: I can get a free heart transplant in Brazil if I want MmmHmm\",\n",
    "    \"lagsanaglasscoke: Corpa hehe\",\n",
    "    \"ComradeCussy: Freedom ain't free brother @HasanAbi\",\n",
    "    \"eronin37:   ðŸ’µ\",\n",
    "    \"calimarx: Itâ€™s to maintain order\",\n",
    "    \"PrettyKrazy: profit motive deprives every successful system\",\n",
    "    \"SpanoNanoChano: even textbooks are a literal racket\",\n",
    "    \"bignachysosa: Federalize it let the government deal with paying hospitals and doctors\",\n",
    "    \"happppy_ant: YEP\",\n",
    "    \"lardball1: @HasanAbi an educated proletariat is dynamite, like that reagan advisor said\",\n",
    "    \"Replying to @ZuzieZozo: I literally had a free operationBurnzorr: You are one person\",\n",
    "    \"PoogDoog: HE SAID THE THING LETSGO\",\n",
    "    \"1337h4x: BALD POTATO PEELER OMEGALUL\",\n",
    "    \"cms100210: All these things exist in countries hence it can work\",\n",
    "    \"HVYHTTRS_: The biggest scam in college is the BOOKS, some good docs about it\",\n",
    "    \"Eevee_Sprinkle:  Keep on licking the boot, GED Andy's.\",\n",
    "    \"bakhtiari_veneco: Stupid question, is South Korea less capitalistic than America? @hasanabi\",\n",
    "    \"JaychanLive:  WineTime PROFIT FIRST  WineTime\",\n",
    "    \"whataburgerfancyketchup: Thats Me Pog\",\n",
    "    \"dumpster27: message deleted by a moderator.\",\n",
    "    \"moogerfooger_: like paying less than $15 min wage\",\n",
    "    \"happppy_ant: YEP control\",\n",
    "    \"Shroomie1707: Do you think you should be able to run for president if you are in jail @hasanabi\",\n",
    "    \"DavidTheDaybed: D:\",\n",
    "    \"Replying to @bignachysosa: Healthcare doesnt have to be free but it SHOULDNT be privateqwertyopsd2: it should be free\",\n",
    "    \"Zpectr3: I pay like 300 Euros for university every semester in germany , but like 250 are for public transport. This shit is insane in the us @HasanAbi\",\n",
    "    \"RamenBellic: @Baldpotatopeeler we just need to decommodify education.\",\n",
    "    \"dicesettle: Lol. Don't do that\",\n",
    "    \"mrbuddybuddy: KEKWait\",\n",
    "    \"RowdyRoran: bro has been following for 3 years and is asking this now?\",\n",
    "    \"sassoune: SORRY WE CAN BAIL OUT CMBS AT 30% purchase price - but when it comes to student loans weâ€™re back to archaic - loan - predatory interest gurg payback or go die\",\n",
    "    \"Darksoul9669: @hasanabi yeah man it was my own actions that had every part of my schooling telling me to take out loans and go to college as the only option and there being basically no downside. Really interesting how high school blows right through how devastating these loans were gonna be during these discussions when i was fucking 17 YEARS OLD\",\n",
    "    \"Tetratera: university is free for everyone in argentina including foreigners, and you don't even have to take a standardized test, only have finished high school (and know upper intermediate spanish)\",\n",
    "    \"lysinehd: permanent desperate underclass\",\n",
    "    \"FALS3_g0D: crusing debt made to keep you a servant to the system\",\n",
    "    \"atsign_: literally other countries can do it for free. is america not exceptional enough to do it?\",\n",
    "    \"dumpster27: message deleted by a moderator.\",\n",
    "    \"Fossabot: @dumpster27, Excessive spamming [warning]\",\n",
    "    \"WeasleyLittleLiar: Did not used to cost that much\",\n",
    "    \"rex__havoc: @hasanabi Have you talked about the new IDR plan \\\"SAVE\\\"? you're payments can be as low as 0/month\",\n",
    "    \"thottopic666: every single aspect of this country was designed to suck the citizens dry as efficiently as possible\",\n",
    "    \"c_d1999: Ask that chatter why donâ€™t we charge for public high school!??\",\n",
    "    \"thehappyparadox: YEP\",\n",
    "    \"Skill_Cylinder: YEP\",\n",
    "    \"kaimehra: yep\",\n",
    "    \"kait516: YEP YEP YEP\",\n",
    "    \"canola_oil: YEP\",\n",
    "    \"thottopic666: YEP\",\n",
    "    \"Hagasha: YEP\"\n",
    "]\n",
    "\n",
    "stream_comments += [\n",
    "    \"MER_AKI: bro thinks hes him lol\",\n",
    "    \"o7draco: ECO DEMON FRFR\",\n",
    "    \"tko0_: UR SO LUCKY\",\n",
    "    \"SparkYYY_123: SO LUCKY\",\n",
    "    \"extratiarestrial: EWWWW\",\n",
    "    \"tomas2brazy: Derke moment\",\n",
    "    \"abhi_142: ECO king\",\n",
    "    \"psygonnn: yeah yeah tarik we know you are going pro\",\n",
    "    \"autumn0999: LOL\",\n",
    "    \"betasimp42: Derke you was right Aware\",\n",
    "    \"grandpafroggys: eco demon\",\n",
    "    \"xDieWithPridex: whats his dpi and sens?\",\n",
    "    \"lowertaxrates: KEKW ur insane sometimes\",\n",
    "    \"PhanzGFX: A real one would get an ace there\",\n",
    "    \"gangliaa: he predicted this\",\n",
    "    \"MrKing8: KEKW\",\n",
    "    \"AyoJabo: ECOOOO FRAGGGGGGER\",\n",
    "    \"MandyLynx: calm down buddy\",\n",
    "    \"Neon_Phaser: derke said it\",\n",
    "    \"Fossabot: Hey, are you following tarik on Twitter? http://twitter.com/tarik\",\n",
    "    \"rishon26: STOP OVERPEEKING LMFAO\",\n",
    "    \"nopointgamer: eco frags\",\n",
    "    \"demon_sl4: any cs2 news?\",\n",
    "    \"aidenvovn420: overheat\",\n",
    "    \"danielmacttv: You are him\",\n",
    "    \"atinyspec: hallo\",\n",
    "    \"GorillaTangie: KEKW\",\n",
    "    \"ghost_khtab: KEKW\",\n",
    "    \"ä¸ä¹šä»¨ä¹‚ (tlex): KLÄ°Ä°PPPPPPPP\",\n",
    "    \"oikawies: well ur consistent at overheating\",\n",
    "    \"ZqCyzreN: ecobra\",\n",
    "    \"ayoub_hh: ns\",\n",
    "    \"bearrynice: @tarik you can satchel? Since when Lil bro\",\n",
    "    \"KorHun_Official: kangkang gets 5 here @tarik\",\n",
    "    \"suus001: OHHHH SHIT\",\n",
    "    \"ub_zinio: overheaaat\",\n",
    "    \"ironman_ap: sup ? @Derke\",\n",
    "    \"Schabii97: DERKE W\",\n",
    "    \"Grediann: overpeak = die Shruge\",\n",
    "    \"jaybird1014: SIT DOWN PLS\",\n",
    "    \"ayswoosh: @Derke how were champs?\",\n",
    "    \"nishikoto: NASTY\",\n",
    "    \"thickymonster: !duo\",\n",
    "    \"wddcruz: 3King\",\n",
    "    \"sqawg: Lil bro humbled himself\",\n",
    "    \"Replying to @thickymonster: !duoFossabot: Asuna AYAYA\",\n",
    "    \"littlesmchallowen: do that next round kekw\",\n",
    "    \"davidakachuwy: COOKED then OVERPEEKED\",\n",
    "    \"MER_AKI: you are not himothy\",\n",
    "    \"lotace:     \",\n",
    "    \"ditt0o: we've got huge bets don't ROZA\",\n",
    "    \"gme16: that spray transfer was lit as\",\n",
    "    \"iicpr: overheat\",\n",
    "    \"daymare5: it was horrible\",\n",
    "    \"Sigfreed: NA BRAIN KEKW\",\n",
    "    \"samsaraeyess: that spray transfer made me ink\",\n",
    "    \"hwhevevsvb: no\",\n",
    "    \"suus001: TUROK TUROK TUROK\",\n",
    "    \"SilintNight: OMEGALUL\",\n",
    "    \"Derke: NO\",\n",
    "    \"mr_01ne: Derke knew it\",\n",
    "    \"Replying to @Derke: i told uQuanFuPanda: deadass\",\n",
    "    \"dioholic: terue\",\n",
    "    \"ta3sk1: THIS TEAM IS FUCKING GOATED TARIK/STEW/ASUNA GGZ\",\n",
    "    \"rue__s: heeey\",\n",
    "    \"abcdgwenchana: overheat on eco\",\n",
    "    \"AdderallBeforeBed: bet you can't do it again MmmHmm\",\n",
    "    \"sissimou: fax\",\n",
    "    \"xcrimsoncrookx: bro thinks the transfer was intentional AINTNOWAY\",\n",
    "    \"adityasanas001: Ecodemon\",\n",
    "    \"dioholic: true\",\n",
    "    \"Lefluu: stew did everything there @tarik\",\n",
    "    \"CosmicDeven: two eco frags and we start talking shit on derke KEKW\",\n",
    "    \"Benjjamin: If you get 3 you're allowed to throw\",\n",
    "    \"afor_f: its true\",\n",
    "    \"Derke: IF ITS 5V1\",\n",
    "    \"Harnasiek03: true\",\n",
    "    \"lowertaxrates: no?\",\n",
    "    \"ketosaiba11: replace jinggg no?\",\n",
    "    \"theak44: BLABBERING BLABBERING\",\n",
    "    \"Derke: AND I DIE FIRST\",\n",
    "    \"laiiiny: You should apply for observer in VCT\",\n",
    "    \"itsrawkus: wake up\",\n",
    "    \"tripharder: ahh yes the rule\",\n",
    "    \"shruggy8: TRUEING\",\n",
    "    \"autumn0999: nice fucking shots tho\",\n",
    "    \"Derke: ITS MY FAULT\",\n",
    "    \"rishon26: @Derke get this man on fnatic\",\n",
    "    \"OzGunAim: !sens\",\n",
    "    \"Fossabot: CSGO: 1.5 @ 800 DPI, VALORANT: .471 800 DPI\",\n",
    "    \"JRD_Nath: \",\n",
    "    \"xkillo147: True, NA rule\",\n",
    "    \"gentlecpu: KEKW if you get 1 it's not your fault\",\n",
    "    \"jinsoooo: if you get 2 you go for the ace\",\n",
    "    \"rightylucy: Lkekw\",\n",
    "    \"lionbrav3: C9 VIBEZ\",\n",
    "    \"emil__val: KEKW KEKW\",\n",
    "    \"maareeyyyy: !mouse\",\n",
    "    \"FarmerFelox: In NA if you get 1 go for 5\",\n",
    "    \"abcdgwenchana: eco frag\",\n",
    "    \"alirezathe1: !res\",\n",
    "    \"Fossabot: DeathAdder V3 Pro\",\n",
    "    \"Fossabot: Val 16:10 (1680x1050) - CSGO: 1280x960\",\n",
    "    \"CaliKillz3: TRUEING\",\n",
    "    \"rentr04: homie turned up cuz derke is watching. respect\",\n",
    "    \"hyp3r10n2: @tarik gets 3 wins round then overfaces and gets mad for it xD\",\n",
    "    \"rightylucy: KEKW\",\n",
    "    \"riyuoh: IF U GET 3 YOU CAN OVERHEAT 100%\",\n",
    "    \"PiquesGaming: thats facts tho\",\n",
    "    \"gkhn94: Dayi bi kere turkce konus be\",\n",
    "    \"Sigfreed: LOOK ITS A 1V1 NOW\",\n",
    "    \"ä¸ä¹šä»¨ä¹‚ (tlex): KLÄ°PPP\",\n",
    "    \"myinnerfaye: Himothy is that you?\",\n",
    "    \"Maximus6267: KEKW no way\",\n",
    "    \"tsylogy: @Derke 5V1 DSG Aware\",\n",
    "    \"ub_zinio: derkes fault\",\n",
    "    \"shruggy8: gonna lose PepeLaugh\",\n",
    "    \"Sigfreed: ITS A FUCKING 1V1 NOW\",\n",
    "    \"Replying to @lionbrav3: C9 VIBEZXeppaa: ?\",\n",
    "    \"ItsTavyy: maybe you need to peek more @tarik\",\n",
    "    \"danielmacttv: Its DERKEâ€™s fault\",\n",
    "    \"dexterityCS: KEKW ó €€\",\n",
    "    \"Rickz10K: KEKW KEKW KEKW KEKW\",\n",
    "    \"kaizo_rm: HUH ó €€\",\n",
    "    \"slaxxxyyyy: Fair enuff\",\n",
    "    \"diipsy9: AYOO HH\",\n",
    "    \"h1k1k0_: HUH\",\n",
    "    \"m0gi08: !gekko\",\n",
    "    \"Apollo_Neptune: HUH\",\n",
    "    \"Fossabot: LilBro it's gekkin time ezz\",\n",
    "    \"siwa33: Close gamba mods\",\n",
    "    \"wahbi_79: HUH\",\n",
    "    \"xelzttv: HUH\",\n",
    "    \"emil__val: KEKW\",\n",
    "    \"ig5mindhacker: HUH\",\n",
    "    \"krasqu33: HUH\",\n",
    "    \"aqilus: HUH\",\n",
    "    \"Jordbaermelk: WOT\",\n",
    "    \"derkesdoormat: @derke OOO DERKE'S HERE HII\",\n",
    "    \"beepbopp11: HUH\",\n",
    "    \"cenk4k: HUH\",\n",
    "    \"mrsteallyourcat: HUH\",\n",
    "    \"xclaassic: true\",\n",
    "    \"Aethielle: HUH\",\n",
    "    \"cyb_eric: HUH\",\n",
    "    \"shruggy8: Sadge\",\n",
    "    \"mesme_R: HUH\"\n",
    "]\n",
    "stream_comments = [comment.split(':')[1] for comment in stream_comments]\n",
    "stream_comments = [comment for comment in stream_comments if len(comment.strip()) > 0]\n",
    "    \n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "#You can then use this pipeline to classify sequences into any of the class names you specify.\n",
    "classifications = \"\"\"Retraction\n",
    "Explanation\n",
    "Query\n",
    "Suggestion\n",
    "Ammendment\n",
    "Expletive\n",
    "Answer\n",
    "Recitation\n",
    "stament \n",
    "observation\n",
    "Commentary\n",
    "Conclusion\n",
    "Mockery\n",
    "Qualification \n",
    "Objection\n",
    "Theory\n",
    "Continuation\n",
    "Evaluation\n",
    "Clarification\n",
    "\"\"\".split('\\n')\n",
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = classifications\n",
    "classifier(stream_comments[0], candidate_labels)\n",
    "# with open('./stream-comments.json','r+') as file:\n",
    "#       # First we load existing data into a dict.\n",
    "#     file_data = json.load(file)\n",
    "#     # Join new_data with file_data inside emp_details\n",
    "#     file_data += stream_comments\n",
    "#     # Sets file's current position at offset.\n",
    "#     file.seek(0)\n",
    "#     # convert back to json.\n",
    "#     json.dump(file_data, file, indent = 4)\n",
    " \n",
    "#copy(Array.from(document.querySelectorAll('.chat-line__message')).map(el => el.textContent))\n",
    "\n",
    "# from transformers import pipeline\n",
    "# classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\", device=0)\n",
    "# sequence_to_classify = \"Angela Merkel is a politician in Germany and leader of the CDU\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "20cd6dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There are 300 cats in the neighborhood.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "input_sentence = \"there are 300 cats in the neighborhood .\"\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "batch = tokenizer(input_sentence, return_tensors='pt')\n",
    "generated_ids = model.generate(batch['input_ids'])\n",
    "generated_sentence = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(generated_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30bf66fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': ' That chatter was wrong you canâ€™t just pay something',\n",
       " 'labels': ['Commentary',\n",
       "  'Objection',\n",
       "  'observation',\n",
       "  'Retraction',\n",
       "  'Recitation',\n",
       "  'Mockery',\n",
       "  '',\n",
       "  'Explanation',\n",
       "  'Clarification',\n",
       "  'Expletive',\n",
       "  'Evaluation',\n",
       "  'Query',\n",
       "  'stament ',\n",
       "  'Continuation',\n",
       "  'Theory',\n",
       "  'Ammendment',\n",
       "  'Qualification ',\n",
       "  'Suggestion',\n",
       "  'Conclusion',\n",
       "  'Answer'],\n",
       " 'scores': [0.3218739330768585,\n",
       "  0.12368783354759216,\n",
       "  0.11010725796222687,\n",
       "  0.0929412841796875,\n",
       "  0.0642746165394783,\n",
       "  0.04800968989729881,\n",
       "  0.03449365124106407,\n",
       "  0.032560210675001144,\n",
       "  0.028999583795666695,\n",
       "  0.027890585362911224,\n",
       "  0.02410063147544861,\n",
       "  0.018279941752552986,\n",
       "  0.013321823440492153,\n",
       "  0.011183790862560272,\n",
       "  0.010250714607536793,\n",
       "  0.00916434358805418,\n",
       "  0.00787479430437088,\n",
       "  0.007707780227065086,\n",
       "  0.007357117719948292,\n",
       "  0.00592039292678237]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(stream_comments[0], classifications, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5964b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in [1, 8, 64, 256]:\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Streaming batch_size={batch_size}\")\n",
    "    for out in tqdm(pipe(dataset, batch_size=batch_size), total=len(dataset)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd497491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.920957326889038\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "one = time.time()\n",
    "isClassified = [classifier(seq, classifications, multi_label=False, batch_size=256) for seq in stream_comments[:20]]\n",
    "two = time.time()\n",
    "print(two - one)\n",
    "#260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e95bf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': ' That chatter was wrong you canâ€™t just pay something',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' 12', 'labels': [], 'scores': []},\n",
       " {'sequence': ' crabs in a bucket', 'labels': [], 'scores': []},\n",
       " {'sequence': \" my life's biggest mistake was trusting my parents, especially when they told me education was the key to success.\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Americas is full of so many people with fucking peasant brain',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': \" btw, so many people have filed for bankruptcy and that used to mean having your debts forgiven. Probably many chatters parents have. SO MANY PEOPLE IN THE POLITICAL ARENA HAVE HAD THEIR DEBTS FORGIVEN. It's so stupid to use that arguementshainybug\",\n",
       "  'labels': ['observation'],\n",
       "  'scores': [0.6428787708282471]},\n",
       " {'sequence': ' Corpa', 'labels': [], 'scores': []},\n",
       " {'sequence': ' I hate this', 'labels': [], 'scores': []},\n",
       " {'sequence': '  HYPERCLAP turn education into businesses',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': \" CHATTERS NO, LET'S DO FUN SHIT BABY WHAT YOU DOIN\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Corpa', 'labels': [], 'scores': []},\n",
       " {'sequence': \" @HasanAbi you dont understand they're like one or two lucky breaks from being the 1%\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': \" Man that's a weird boot to be licking, chatter what the hell\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' profit motive destroys humanity', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Chatting really Hasan?', 'labels': [], 'scores': []},\n",
       " {'sequence': ' \"I couldn\\'t care less.\"', 'labels': [], 'scores': []},\n",
       " {'sequence': ' YEP LEECHES', 'labels': [], 'scores': []},\n",
       " {'sequence': \"  just don't get sick or hurt\", 'labels': [], 'scores': []},\n",
       " {'sequence': ' gatekeeping and control over labor conditions',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' sugar dads in chat?', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Corpa', 'labels': [], 'scores': []},\n",
       " {'sequence': ' America is a massive corporation', 'labels': [], 'scores': []},\n",
       " {'sequence': \" Chatting I'm very smart\", 'labels': [], 'scores': []},\n",
       " {'sequence': ' hasCapital', 'labels': [], 'scores': []},\n",
       " {'sequence': '  gimme gimme gimme', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Pivo . o O ( Corpa ðŸ”«  )', 'labels': [], 'scores': []},\n",
       " {'sequence': ' @ashlynnicoleramirez report yourself as poor and never tell them anything ever again',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' they want to make money of your student loan debt how do you guys not see this',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Hey Hasan! My man! I know I longed for some twitch political commentary',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' YEP just looking for more capital avenues',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' To profit in anyway imaginable', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Corpa private prisons', 'labels': [], 'scores': []},\n",
       " {'sequence': ' YEP \\U000e0000', 'labels': [], 'scores': []},\n",
       " {'sequence': \" that chatter didn't pay their loan and the govt took back their degree via lobotomy @hasanabi\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Knowledge is power Hasan. Its that simple. Keep people dumb, keep them powerless. @HasanAbi',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Chatting \\U000e0000', 'labels': [], 'scores': []},\n",
       " {'sequence': ' they need a controlled working force',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Fuck them', 'labels': [], 'scores': []},\n",
       " {'sequence': \" can't get blood from a stone Crungo\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' making 17 year olds take loans out OMEGALUL',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' keep the masses dumb', 'labels': [], 'scores': []},\n",
       " {'sequence': ' @luckypompom qalla_s modCheck', 'labels': [], 'scores': []},\n",
       " {'sequence': ' I do that on medical bills as protest. I pay like $25/mo on $4,000 hospital bills  they donâ€™t report it to credit as long as you pay.',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' just get a full ride scholarship EZ Chatting',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' chat is so annoying today', 'labels': [], 'scores': []},\n",
       " {'sequence': '  HYPERCLAP', 'labels': [], 'scores': []},\n",
       " {'sequence': ' income driven repayment plan and stay poor forevershainybug',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': \" paying a portion, you are still getting killed on the backend with interest. That's why they are fine with you not making full payments\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' housing, food, etc.', 'labels': [], 'scores': []},\n",
       " {'sequence': ' TATE', 'labels': [], 'scores': []},\n",
       " {'sequence': \" Because they're told they have to pay more in taxes and have less money when they're already struggling.\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' this country fucken sucks', 'labels': [], 'scores': []},\n",
       " {'sequence': ' ultimatley this argument boils down to \"its your fault for wanting to live and be happy\" most people dont want to go to school to work for the rest of their lives while paying to do so',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' almost every aspect of higher education is profitable',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Daddy chill \\U0001fae0', 'labels': [], 'scores': []},\n",
       " {'sequence': ' profit motives drive innocation ', 'labels': [], 'scores': []},\n",
       " {'sequence': ' YEE HAWWWW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Military YEP', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Affordable but not always available',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' iâ€™m below the income threshold so i get a payment of zero. i should clarify that i WORK FOR THE GOVERNMENT and iâ€™m not paid enough to meet the thresholds',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Corpa Clap', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Healthcare doesnt have to be free but it SHOULDNT be private',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' americas kindaa fuked rn', 'labels': [], 'scores': []},\n",
       " {'sequence': ' I literally had a free operation', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Chatting Hasan @Hasanabi @Hasanthehun @Freedomeaglefuck',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Educaiton is overpriced in the US',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' theres a reason my sociology class presented american exceptionalism as a form of propaganda',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' @hasanabi allow for bankruptcy, then normalize bankruptcy at graduation. Problem solved. KEKW',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' YEP just join the military', 'labels': [], 'scores': []},\n",
       " {'sequence': ' and here healthcare is so fucking expensive.',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': \" can't have an educated proletariat\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Now it is about political ideology',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' you cant even have a fucking hobby without people asking you \"well how are you gunna make any money with that?\"',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' no war but class war', 'labels': [], 'scores': []},\n",
       " {'sequence': ' I do that on medical bills as protest. I pay like $25/mo on $4,000 hospital bills rhyzKEK they donâ€™t report it to credit as long as you pay.aspiration89',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': '  You want my number to not go up???',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': '  dumb consumer slaves', 'labels': [], 'scores': []},\n",
       " {'sequence': ' more than South Korea?', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Education isnt free for the same reason healthcare isnt free. Because you cant live without it @hasanabi',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': \" @hasanabi true reason that education isn't free and student loans reign supreme is slabs\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': '  Paywall the labor force @hasanabi',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' there are worse neoliberal hellholes out there Aware',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' I can get a free heart transplant in Brazil if I want MmmHmm',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Corpa hehe', 'labels': [], 'scores': []},\n",
       " {'sequence': \" Freedom ain't free brother @HasanAbi\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': '   ðŸ’µ', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Itâ€™s to maintain order', 'labels': [], 'scores': []},\n",
       " {'sequence': ' profit motive deprives every successful system',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' even textbooks are a literal racket',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Federalize it let the government deal with paying hospitals and doctors',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' YEP', 'labels': [], 'scores': []},\n",
       " {'sequence': ' @HasanAbi an educated proletariat is dynamite, like that reagan advisor said',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' I literally had a free operationBurnzorr',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' HE SAID THE THING LETSGO', 'labels': [], 'scores': []},\n",
       " {'sequence': ' BALD POTATO PEELER OMEGALUL', 'labels': [], 'scores': []},\n",
       " {'sequence': ' All these things exist in countries hence it can work',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' The biggest scam in college is the BOOKS, some good docs about it',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': \"  Keep on licking the boot, GED Andy's.\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Stupid question, is South Korea less capitalistic than America? @hasanabi',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': '  WineTime PROFIT FIRST  WineTime', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Thats Me Pog', 'labels': [], 'scores': []},\n",
       " {'sequence': ' message deleted by a moderator.',\n",
       "  'labels': ['Retraction'],\n",
       "  'scores': [0.6210706830024719]},\n",
       " {'sequence': ' like paying less than $15 min wage',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' YEP control', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Do you think you should be able to run for president if you are in jail @hasanabi',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' D', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Healthcare doesnt have to be free but it SHOULDNT be privateqwertyopsd2',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' I pay like 300 Euros for university every semester in germany , but like 250 are for public transport. This shit is insane in the us @HasanAbi',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' @Baldpotatopeeler we just need to decommodify education.',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': \" Lol. Don't do that\", 'labels': [], 'scores': []},\n",
       " {'sequence': ' KEKWait', 'labels': [], 'scores': []},\n",
       " {'sequence': ' bro has been following for 3 years and is asking this now?',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' SORRY WE CAN BAIL OUT CMBS AT 30% purchase price - but when it comes to student loans weâ€™re back to archaic - loan - predatory interest gurg payback or go die',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' @hasanabi yeah man it was my own actions that had every part of my schooling telling me to take out loans and go to college as the only option and there being basically no downside. Really interesting how high school blows right through how devastating these loans were gonna be during these discussions when i was fucking 17 YEARS OLD',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': \" university is free for everyone in argentina including foreigners, and you don't even have to take a standardized test, only have finished high school (and know upper intermediate spanish)\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' permanent desperate underclass', 'labels': [], 'scores': []},\n",
       " {'sequence': ' crusing debt made to keep you a servant to the system',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' literally scamming children', 'labels': [], 'scores': []},\n",
       " {'sequence': ' literally other countries can do it for free. is america not exceptional enough to do it?',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' message deleted by a moderator.',\n",
       "  'labels': ['Retraction'],\n",
       "  'scores': [0.6210706830024719]},\n",
       " {'sequence': ' @dumpster27, Excessive spamming [warning]',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Did not used to cost that much', 'labels': [], 'scores': []},\n",
       " {'sequence': ' @hasanabi Have you talked about the new IDR plan \"SAVE\"? you\\'re payments can be as low as 0/month',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' every single aspect of this country was designed to suck the citizens dry as efficiently as possible',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Ask that chatter why donâ€™t we charge for public high school!??',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' YEP', 'labels': [], 'scores': []},\n",
       " {'sequence': ' YEP', 'labels': [], 'scores': []},\n",
       " {'sequence': ' yep', 'labels': [], 'scores': []},\n",
       " {'sequence': ' YEP YEP YEP', 'labels': [], 'scores': []},\n",
       " {'sequence': ' YEP', 'labels': [], 'scores': []},\n",
       " {'sequence': ' YEP', 'labels': [], 'scores': []},\n",
       " {'sequence': ' YEP', 'labels': [], 'scores': []},\n",
       " {'sequence': ' bro thinks hes him lol', 'labels': [], 'scores': []},\n",
       " {'sequence': ' That spray so mad u really him', 'labels': [], 'scores': []},\n",
       " {'sequence': ' ECO DEMON FRFR', 'labels': [], 'scores': []},\n",
       " {'sequence': ' UR SO LUCKY', 'labels': [], 'scores': []},\n",
       " {'sequence': ' SO LUCKY', 'labels': [], 'scores': []},\n",
       " {'sequence': ' EWWWW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Derke moment', 'labels': [], 'scores': []},\n",
       " {'sequence': ' ECO king', 'labels': [], 'scores': []},\n",
       " {'sequence': ' yeah yeah tarik we know you are going pro',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' LOL', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Derke you was right Aware', 'labels': [], 'scores': []},\n",
       " {'sequence': ' eco demon', 'labels': [], 'scores': []},\n",
       " {'sequence': ' whats his dpi and sens?', 'labels': [], 'scores': []},\n",
       " {'sequence': ' KEKW ur insane sometimes', 'labels': [], 'scores': []},\n",
       " {'sequence': ' A real one would get an ace there',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' he predicted this', 'labels': [], 'scores': []},\n",
       " {'sequence': ' KEKW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' ECOOOO FRAGGGGGGER', 'labels': [], 'scores': []},\n",
       " {'sequence': ' calm down buddy', 'labels': [], 'scores': []},\n",
       " {'sequence': ' derke said it', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Hey, are you following tarik on Twitter? http',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' STOP OVERPEEKING LMFAO', 'labels': [], 'scores': []},\n",
       " {'sequence': ' eco frags', 'labels': [], 'scores': []},\n",
       " {'sequence': ' any cs2 news?', 'labels': [], 'scores': []},\n",
       " {'sequence': ' overheat', 'labels': [], 'scores': []},\n",
       " {'sequence': ' You are him', 'labels': [], 'scores': []},\n",
       " {'sequence': ' hallo', 'labels': [], 'scores': []},\n",
       " {'sequence': ' KEKW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' KEKW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' KLÄ°Ä°PPPPPPPP', 'labels': [], 'scores': []},\n",
       " {'sequence': ' well ur consistent at overheating',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' ecobra', 'labels': [], 'scores': []},\n",
       " {'sequence': ' ns', 'labels': [], 'scores': []},\n",
       " {'sequence': ' @tarik you can satchel? Since when Lil bro',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' kangkang gets 5 here @tarik', 'labels': [], 'scores': []},\n",
       " {'sequence': ' OHHHH SHIT', 'labels': [], 'scores': []},\n",
       " {'sequence': ' overheaaat', 'labels': [], 'scores': []},\n",
       " {'sequence': ' sup ? @Derke', 'labels': [], 'scores': []},\n",
       " {'sequence': ' DERKE W', 'labels': [], 'scores': []},\n",
       " {'sequence': ' overpeak = die Shruge', 'labels': [], 'scores': []},\n",
       " {'sequence': ' SIT DOWN PLS', 'labels': [], 'scores': []},\n",
       " {'sequence': ' @Derke how were champs?', 'labels': [], 'scores': []},\n",
       " {'sequence': ' NASTY', 'labels': [], 'scores': []},\n",
       " {'sequence': ' !duo', 'labels': [], 'scores': []},\n",
       " {'sequence': ' 3King', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Lil bro humbled himself', 'labels': [], 'scores': []},\n",
       " {'sequence': ' !duoFossabot', 'labels': [], 'scores': []},\n",
       " {'sequence': ' do that next round kekw', 'labels': [], 'scores': []},\n",
       " {'sequence': ' COOKED then OVERPEEKED', 'labels': [], 'scores': []},\n",
       " {'sequence': ' you are not himothy', 'labels': [], 'scores': []},\n",
       " {'sequence': \" we've got huge bets don't ROZA\", 'labels': [], 'scores': []},\n",
       " {'sequence': ' that spray transfer was lit as', 'labels': [], 'scores': []},\n",
       " {'sequence': ' overheat', 'labels': [], 'scores': []},\n",
       " {'sequence': ' it was horrible', 'labels': [], 'scores': []},\n",
       " {'sequence': ' NA BRAIN KEKW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' that spray transfer made me ink', 'labels': [], 'scores': []},\n",
       " {'sequence': ' no', 'labels': [], 'scores': []},\n",
       " {'sequence': ' TUROK TUROK TUROK', 'labels': [], 'scores': []},\n",
       " {'sequence': ' OMEGALUL', 'labels': [], 'scores': []},\n",
       " {'sequence': ' NO', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Derke knew it', 'labels': [], 'scores': []},\n",
       " {'sequence': ' i told uQuanFuPanda', 'labels': [], 'scores': []},\n",
       " {'sequence': ' terue', 'labels': [], 'scores': []},\n",
       " {'sequence': ' THIS TEAM IS FUCKING GOATED TARIK/STEW/ASUNA GGZ',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' heeey', 'labels': [], 'scores': []},\n",
       " {'sequence': ' overheat on eco', 'labels': [], 'scores': []},\n",
       " {'sequence': \" bet you can't do it again MmmHmm\", 'labels': [], 'scores': []},\n",
       " {'sequence': ' fax', 'labels': [], 'scores': []},\n",
       " {'sequence': ' bro thinks the transfer was intentional AINTNOWAY',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Ecodemon', 'labels': [], 'scores': []},\n",
       " {'sequence': ' true', 'labels': [], 'scores': []},\n",
       " {'sequence': ' stew did everything there @tarik', 'labels': [], 'scores': []},\n",
       " {'sequence': ' two eco frags and we start talking shit on derke KEKW',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': \" If you get 3 you're allowed to throw\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' its true', 'labels': [], 'scores': []},\n",
       " {'sequence': ' IF ITS 5V1', 'labels': [], 'scores': []},\n",
       " {'sequence': ' true', 'labels': [], 'scores': []},\n",
       " {'sequence': ' no?', 'labels': [], 'scores': []},\n",
       " {'sequence': ' replace jinggg no?', 'labels': [], 'scores': []},\n",
       " {'sequence': ' BLABBERING BLABBERING', 'labels': [], 'scores': []},\n",
       " {'sequence': ' AND I DIE FIRST', 'labels': [], 'scores': []},\n",
       " {'sequence': ' You should apply for observer in VCT',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' wake up', 'labels': [], 'scores': []},\n",
       " {'sequence': ' ahh yes the rule', 'labels': [], 'scores': []},\n",
       " {'sequence': ' TRUEING', 'labels': [], 'scores': []},\n",
       " {'sequence': ' nice fucking shots tho', 'labels': [], 'scores': []},\n",
       " {'sequence': ' ITS MY FAULT', 'labels': [], 'scores': []},\n",
       " {'sequence': ' @Derke get this man on fnatic', 'labels': [], 'scores': []},\n",
       " {'sequence': ' !sens', 'labels': [], 'scores': []},\n",
       " {'sequence': ' CSGO', 'labels': [], 'scores': []},\n",
       " {'sequence': ' @tarik UR BICEPS ARE HUGE!!!', 'labels': [], 'scores': []},\n",
       " {'sequence': ' True, NA rule', 'labels': [], 'scores': []},\n",
       " {'sequence': \" KEKW if you get 1 it's not your fault\",\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' if you get 2 you go for the ace', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Lkekw', 'labels': [], 'scores': []},\n",
       " {'sequence': ' C9 VIBEZ', 'labels': [], 'scores': []},\n",
       " {'sequence': ' KEKW KEKW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' !mouse', 'labels': [], 'scores': []},\n",
       " {'sequence': ' In NA if you get 1 go for 5', 'labels': [], 'scores': []},\n",
       " {'sequence': ' eco frag', 'labels': [], 'scores': []},\n",
       " {'sequence': ' !res', 'labels': [], 'scores': []},\n",
       " {'sequence': ' DeathAdder V3 Pro', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Val 16', 'labels': [], 'scores': []},\n",
       " {'sequence': ' TRUEING', 'labels': [], 'scores': []},\n",
       " {'sequence': ' homie turned up cuz derke is watching. respect',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' @tarik gets 3 wins round then overfaces and gets mad for it xD',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' KEKW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' IF U GET 3 YOU CAN OVERHEAT 100%', 'labels': [], 'scores': []},\n",
       " {'sequence': ' thats facts tho', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Dayi bi kere turkce konus be', 'labels': [], 'scores': []},\n",
       " {'sequence': ' LOOK ITS A 1V1 NOW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' KLÄ°PPP', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Himothy is that you?', 'labels': [], 'scores': []},\n",
       " {'sequence': ' KEKW no way', 'labels': [], 'scores': []},\n",
       " {'sequence': ' @Derke 5V1 DSG Aware', 'labels': [], 'scores': []},\n",
       " {'sequence': ' derkes fault', 'labels': [], 'scores': []},\n",
       " {'sequence': ' arabic blood', 'labels': [], 'scores': []},\n",
       " {'sequence': ' gonna lose PepeLaugh', 'labels': [], 'scores': []},\n",
       " {'sequence': ' ITS A FUCKING 1V1 NOW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' C9 VIBEZXeppaa', 'labels': [], 'scores': []},\n",
       " {'sequence': ' maybe you need to peek more @tarik',\n",
       "  'labels': [],\n",
       "  'scores': []},\n",
       " {'sequence': ' Its DERKEâ€™s fault', 'labels': [], 'scores': []},\n",
       " {'sequence': ' KEKW \\U000e0000', 'labels': [], 'scores': []},\n",
       " {'sequence': ' KEKW KEKW KEKW KEKW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH \\U000e0000', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Fair enuff', 'labels': [], 'scores': []},\n",
       " {'sequence': ' AYOO HH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' !gekko', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': \" LilBro it's gekkin time ezz\", 'labels': [], 'scores': []},\n",
       " {'sequence': ' Close gamba mods', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' KEKW', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' WOT', 'labels': [], 'scores': []},\n",
       " {'sequence': \" @derke OOO DERKE'S HERE HII\", 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' true', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []},\n",
       " {'sequence': ' Sadge', 'labels': [], 'scores': []},\n",
       " {'sequence': ' HUH', 'labels': [], 'scores': []}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isClassified\n",
    "\n",
    "for i in isClassified:\n",
    "    i['labels'] = [s for index, s in enumerate(i['labels']) if i['scores'][index] > .5]\n",
    "    i['scores'] = [s for index, s in enumerate(i['scores']) if s > .5]\n",
    "\n",
    "            \n",
    "isClassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a31ed1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awahab/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Angela Merkel is a politician in Germany and leader of the CDU', 'labels': ['politics', 'economy', 'environment', 'entertainment'], 'scores': [0.982321560382843, 0.007280202116817236, 0.005891879089176655, 0.004506275057792664]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\")\n",
    "sequence_to_classify = \"Angela Merkel is a politician in Germany and leader of the CDU\"\n",
    "candidate_labels = [\"politics\", \"economy\", \"entertainment\", \"environment\"]\n",
    "output = classifier(sequence_to_classify, candidate_labels, multi_label=False)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b8b56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     1,    260,  64817,  55118,    857,   1298,    260, 154064,    349,\n",
       "            282,  12556,    473,    260, 153067,    443,    260,  94867,      2,\n",
       "            260,  64708,  54272,    273,    340,    288,  13977,    305,   5264,\n",
       "              2]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(input[\"input_ids\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "device = torch.device('cuda:0')\n",
    "#torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "premise = \"Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU\"\n",
    "hypothesis = \"Emmanuel Macron is the President of France\"\n",
    "\n",
    "input = tokenizer(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "output = model(input[\"input_ids\"].to('cpu'))  # device = \"cuda:0\" or \"cpu\"\n",
    "prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cc9c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Angela Merkel is a politician in Germany and leader of the CDU', 'labels': ['politics', 'economy', 'environment', 'entertainment'], 'scores': [0.982321560382843, 0.007280202116817236, 0.005891879089176655, 0.004506275057792664]}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee83fcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextInputSequence must be str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 303\u001b[0m\n\u001b[1;32m    300\u001b[0m hypothesis \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis example is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# run through model pre-trained on MNLI\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpremise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43monly_first\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m logits \u001b[38;5;241m=\u001b[39m nli_model(x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# we throw away \"neutral\" (dim 1) and take the probability of\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# \"entailment\" (2) as the probability of the label being true \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2373\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[38;5;129m@add_end_docstrings\u001b[39m(\n\u001b[1;32m   2337\u001b[0m     ENCODE_KWARGS_DOCSTRING,\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2356\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2357\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2359\u001b[0m \u001b[38;5;124;03m    Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\u001b[39;00m\n\u001b[1;32m   2360\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2371\u001b[0m \u001b[38;5;124;03m            method).\u001b[39;00m\n\u001b[1;32m   2372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2373\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2383\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2781\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2771\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2772\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2773\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2774\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2778\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2779\u001b[0m )\n\u001b[0;32m-> 2781\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2784\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/models/bart/tokenization_bart_fast.py:272\u001b[0m, in \u001b[0;36mBartTokenizerFast._encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_split_into_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     )\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:517\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    497\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    516\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 517\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/models/bart/tokenization_bart_fast.py:261\u001b[0m, in \u001b[0;36mBartTokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_split_into_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m     )\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py465/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:445\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    438\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    439\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    443\u001b[0m )\n\u001b[0;32m--> 445\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    457\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    459\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    469\u001b[0m ]\n",
      "\u001b[0;31mTypeError\u001b[0m: TextInputSequence must be str"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "stream_comments = [\n",
    "    \"its_pam_ela: That chatter was wrong you canâ€™t just pay something\",\n",
    "    \"ZeroTepMusic: 12\",\n",
    "    \"lysinehd: crabs in a bucket\",\n",
    "    \"bouillabased: my life's biggest mistake was trusting my parents, especially when they told me education was the key to success.\",\n",
    "    \"kylanc6: Americas is full of so many people with fucking peasant brain\",\n",
    "    \"Replying to @spyderfrommars: btw, so many people have filed for bankruptcy and that used to mean having your debts forgiven. Probably many chatters parents have. SO MANY PEOPLE IN THE POLITICAL ARENA HAVE HAD THEIR DEBTS FORGIVEN. It's so stupid to use that arguementshainybug: and bankruptcy doesn't wipe student loans \",\n",
    "    \"MarIsMar: Corpa\",\n",
    "    \"IsThatSalem: I hate this\",\n",
    "    \"eronin37:   \",\n",
    "    \"v3sh_:  HYPERCLAP turn education into businesses\",\n",
    "    \"hondewberry: CHATTERS NO, LET'S DO FUN SHIT BABY WHAT YOU DOIN\",\n",
    "    \"JEZZ_7: Corpa\",\n",
    "    \"xTrashPandaKingx: @HasanAbi you dont understand they're like one or two lucky breaks from being the 1%\",\n",
    "    \"fearandrespect: Man that's a weird boot to be licking, chatter what the hell\",\n",
    "    \"PrettyKrazy: profit motive destroys humanity\",\n",
    "    \"Sutiibun_: \",\n",
    "    \"qwertyopsd2: Chatting really Hasan?\",\n",
    "    \"Tamarama02: \\\"I couldn't care less.\\\"\",\n",
    "    \"whitneythegoth: YEP LEECHES\",\n",
    "    \"FriedWaffles:  just don't get sick or hurt\",\n",
    "    \"redeyeink: gatekeeping and control over labor conditions\",\n",
    "    \"BigDddyNick: sugar dads in chat?\",\n",
    "    \"austrom: Corpa\",\n",
    "    \"HalalChad_: America is a massive corporation\",\n",
    "    \"russianspy619: Chatting I'm very smart\",\n",
    "    \"dankusdingus: hasCapital\",\n",
    "    \"dr_desu:  gimme gimme gimme\",\n",
    "    \"LateAndNever: Pivo . o O ( Corpa ðŸ”«  )\",\n",
    "    \"phoneofff: @ashlynnicoleramirez report yourself as poor and never tell them anything ever again\",\n",
    "    \"ADK_215: they want to make money of your student loan debt how do you guys not see this\",\n",
    "    \"Leafy_Sh4de: Hey Hasan! My man! I know I longed for some twitch political commentary\",\n",
    "    \"WhyYouGotNecklace: YEP just looking for more capital avenues\",\n",
    "    \"meredyke: To profit in anyway imaginable\",\n",
    "    \"imLunchy: Corpa private prisons\",\n",
    "    \"thottopic666: YEP ó €€\",\n",
    "    \"thelookoutshift: that chatter didn't pay their loan and the govt took back their degree via lobotomy @hasanabi\",\n",
    "    \"whataburgerfancyketchup: Knowledge is power Hasan. Its that simple. Keep people dumb, keep them powerless. @HasanAbi\",\n",
    "    \"aquamiguel: Chatting ó €€\",\n",
    "    \"ya_plis: they need a controlled working force\",\n",
    "    \"ShakeN_Bake: Fuck them\",\n",
    "    \"moogerfooger_: can't get blood from a stone Crungo\",\n",
    "    \"sandsim: making 17 year olds take loans out OMEGALUL\",\n",
    "    \"stovetotheface: keep the masses dumb\",\n",
    "    \"Resubscribe: @luckypompom qalla_s modCheck\",\n",
    "    \"tr0piKEL1: I do that on medical bills as protest. I pay like $25/mo on $4,000 hospital bills  they donâ€™t report it to credit as long as you pay.\",\n",
    "    \"rentcontrolryan: just get a full ride scholarship EZ Chatting\",\n",
    "    \"shoriu_: chat is so annoying today\",\n",
    "    \"TheUh0hOreo:  HYPERCLAP\",\n",
    "    \"Replying to @ashlynnicoleramirez: income driven repayment plan and stay poor forevershainybug: me lmao\",\n",
    "    \"Shonnicus: paying a portion, you are still getting killed on the backend with interest. That's why they are fine with you not making full payments\",\n",
    "    \"catboy_rai: housing, food, etc.\",\n",
    "    \"YukiTsunoda__: \",\n",
    "    \"hashoe23: TATE\",\n",
    "    \"Faviahn: Because they're told they have to pay more in taxes and have less money when they're already struggling.\",\n",
    "    \"duskdeserter: this country fucken sucks\",\n",
    "    \"HerrosRevenge: ultimatley this argument boils down to \\\"its your fault for wanting to live and be happy\\\" most people dont want to go to school to work for the rest of their lives while paying to do so\",\n",
    "    \"seeayy: almost every aspect of higher education is profitable\",\n",
    "    \"chelseymakes: Daddy chill ðŸ« \",\n",
    "    \"TehAdamBomb: profit motives drive innocation \",\n",
    "    \"lordcharliesheen: YEE HAWWWW\",\n",
    "    \"HUGEGAMER96: Military YEP\",\n",
    "    \"clandestinie: Affordable but not always available\",\n",
    "    \"big_dykeenergy: iâ€™m below the income threshold so i get a payment of zero. i should clarify that i WORK FOR THE GOVERNMENT and iâ€™m not paid enough to meet the thresholds\",\n",
    "    \"aquamiguel: Corpa Clap\",\n",
    "    \"bignachysosa: Healthcare doesnt have to be free but it SHOULDNT be private\",\n",
    "    \"Cypres_warluckHyan8: americas kindaa fuked rn\",\n",
    "    \"ZuzieZozo: I literally had a free operation\",\n",
    "    \"quarantinewolf: Chatting Hasan @Hasanabi @Hasanthehun @Freedomeaglefuck\",\n",
    "    \"IgiveBluebells: Educaiton is overpriced in the US\",\n",
    "    \"librapelican: theres a reason my sociology class presented american exceptionalism as a form of propaganda\",\n",
    "    \"xygeek: @hasanabi allow for bankruptcy, then normalize bankruptcy at graduation. Problem solved. KEKW\",\n",
    "    \"Skill_Cylinder: YEP just join the military\",\n",
    "    \"Hagasha: \",\n",
    "    \"bigstephfan: and here healthcare is so fucking expensive.\",\n",
    "    \"BOATPARADE: can't have an educated proletariat\",\n",
    "    \"RoguePr1nc355: Now it is about political ideology\",\n",
    "    \"Zony66: you cant even have a fucking hobby without people asking you \\\"well how are you gunna make any money with that?\\\"\",\n",
    "    \"cas3_: no war but class war\",\n",
    "    \"Replying to @tr0piKEL1: I do that on medical bills as protest. I pay like $25/mo on $4,000 hospital bills rhyzKEK they donâ€™t report it to credit as long as you pay.aspiration89: YEP ó €€\",\n",
    "    \"SimUser:  You want my number to not go up???\",\n",
    "    \"politicsenjoyer:  dumb consumer slaves\",\n",
    "    \"narjuh: more than South Korea?\",\n",
    "    \"punishedribcorn: Education isnt free for the same reason healthcare isnt free. Because you cant live without it @hasanabi\",\n",
    "    \"GanjarDanks: @hasanabi true reason that education isn't free and student loans reign supreme is slabs\",\n",
    "    \"ok_eevee:  Paywall the labor force @hasanabi\",\n",
    "    \"kintu: there are worse neoliberal hellholes out there Aware\",\n",
    "    \"esquerdomacho: I can get a free heart transplant in Brazil if I want MmmHmm\",\n",
    "    \"lagsanaglasscoke: Corpa hehe\",\n",
    "    \"ComradeCussy: Freedom ain't free brother @HasanAbi\",\n",
    "    \"eronin37:   ðŸ’µ\",\n",
    "    \"calimarx: Itâ€™s to maintain order\",\n",
    "    \"PrettyKrazy: profit motive deprives every successful system\",\n",
    "    \"SpanoNanoChano: even textbooks are a literal racket\",\n",
    "    \"bignachysosa: Federalize it let the government deal with paying hospitals and doctors\",\n",
    "    \"happppy_ant: YEP\",\n",
    "    \"lardball1: @HasanAbi an educated proletariat is dynamite, like that reagan advisor said\",\n",
    "    \"Replying to @ZuzieZozo: I literally had a free operationBurnzorr: You are one person\",\n",
    "    \"PoogDoog: HE SAID THE THING LETSGO\",\n",
    "    \"1337h4x: BALD POTATO PEELER OMEGALUL\",\n",
    "    \"cms100210: All these things exist in countries hence it can work\",\n",
    "    \"HVYHTTRS_: The biggest scam in college is the BOOKS, some good docs about it\",\n",
    "    \"Eevee_Sprinkle:  Keep on licking the boot, GED Andy's.\",\n",
    "    \"bakhtiari_veneco: Stupid question, is South Korea less capitalistic than America? @hasanabi\",\n",
    "    \"JaychanLive:  WineTime PROFIT FIRST  WineTime\",\n",
    "    \"whataburgerfancyketchup: Thats Me Pog\",\n",
    "    \"dumpster27: message deleted by a moderator.\",\n",
    "    \"moogerfooger_: like paying less than $15 min wage\",\n",
    "    \"happppy_ant: YEP control\",\n",
    "    \"Shroomie1707: Do you think you should be able to run for president if you are in jail @hasanabi\",\n",
    "    \"DavidTheDaybed: D:\",\n",
    "    \"Replying to @bignachysosa: Healthcare doesnt have to be free but it SHOULDNT be privateqwertyopsd2: it should be free\",\n",
    "    \"Zpectr3: I pay like 300 Euros for university every semester in germany , but like 250 are for public transport. This shit is insane in the us @HasanAbi\",\n",
    "    \"RamenBellic: @Baldpotatopeeler we just need to decommodify education.\",\n",
    "    \"dicesettle: Lol. Don't do that\",\n",
    "    \"mrbuddybuddy: KEKWait\",\n",
    "    \"RowdyRoran: bro has been following for 3 years and is asking this now?\",\n",
    "    \"sassoune: SORRY WE CAN BAIL OUT CMBS AT 30% purchase price - but when it comes to student loans weâ€™re back to archaic - loan - predatory interest gurg payback or go die\",\n",
    "    \"Darksoul9669: @hasanabi yeah man it was my own actions that had every part of my schooling telling me to take out loans and go to college as the only option and there being basically no downside. Really interesting how high school blows right through how devastating these loans were gonna be during these discussions when i was fucking 17 YEARS OLD\",\n",
    "    \"Tetratera: university is free for everyone in argentina including foreigners, and you don't even have to take a standardized test, only have finished high school (and know upper intermediate spanish)\",\n",
    "    \"lysinehd: permanent desperate underclass\",\n",
    "    \"FALS3_g0D: crusing debt made to keep you a servant to the system\",\n",
    "    \"sandsim: literally scamming children\",\n",
    "    \"atsign_: literally other countries can do it for free. is america not exceptional enough to do it?\",\n",
    "    \"dumpster27: message deleted by a moderator.\",\n",
    "    \"Fossabot: @dumpster27, Excessive spamming [warning]\",\n",
    "    \"WeasleyLittleLiar: Did not used to cost that much\",\n",
    "    \"rex__havoc: @hasanabi Have you talked about the new IDR plan \\\"SAVE\\\"? you're payments can be as low as 0/month\",\n",
    "    \"thottopic666: every single aspect of this country was designed to suck the citizens dry as efficiently as possible\",\n",
    "    \"c_d1999: Ask that chatter why donâ€™t we charge for public high school!??\",\n",
    "    \"thehappyparadox: YEP\",\n",
    "    \"Skill_Cylinder: YEP\",\n",
    "    \"kaimehra: yep\",\n",
    "    \"kait516: YEP YEP YEP\",\n",
    "    \"canola_oil: YEP\",\n",
    "    \"thottopic666: YEP\",\n",
    "    \"Hagasha: YEP\"\n",
    "]\n",
    "\n",
    "stream_comments += [\n",
    "    \"MER_AKI: bro thinks hes him lol\",\n",
    "    \"xmas31: That spray so mad u really him\",\n",
    "    \"o7draco: ECO DEMON FRFR\",\n",
    "    \"tko0_: UR SO LUCKY\",\n",
    "    \"SparkYYY_123: SO LUCKY\",\n",
    "    \"extratiarestrial: EWWWW\",\n",
    "    \"tomas2brazy: Derke moment\",\n",
    "    \"abhi_142: ECO king\",\n",
    "    \"psygonnn: yeah yeah tarik we know you are going pro\",\n",
    "    \"autumn0999: LOL\",\n",
    "    \"betasimp42: Derke you was right Aware\",\n",
    "    \"grandpafroggys: eco demon\",\n",
    "    \"xDieWithPridex: whats his dpi and sens?\",\n",
    "    \"lowertaxrates: KEKW ur insane sometimes\",\n",
    "    \"PhanzGFX: A real one would get an ace there\",\n",
    "    \"gangliaa: he predicted this\",\n",
    "    \"MrKing8: KEKW\",\n",
    "    \"AyoJabo: ECOOOO FRAGGGGGGER\",\n",
    "    \"MandyLynx: calm down buddy\",\n",
    "    \"Neon_Phaser: derke said it\",\n",
    "    \"Fossabot: Hey, are you following tarik on Twitter? http://twitter.com/tarik\",\n",
    "    \"rishon26: STOP OVERPEEKING LMFAO\",\n",
    "    \"nopointgamer: eco frags\",\n",
    "    \"demon_sl4: any cs2 news?\",\n",
    "    \"aidenvovn420: overheat\",\n",
    "    \"danielmacttv: You are him\",\n",
    "    \"atinyspec: hallo\",\n",
    "    \"GorillaTangie: KEKW\",\n",
    "    \"ghost_khtab: KEKW\",\n",
    "    \"ä¸ä¹šä»¨ä¹‚ (tlex): KLÄ°Ä°PPPPPPPP\",\n",
    "    \"oikawies: well ur consistent at overheating\",\n",
    "    \"ZqCyzreN: ecobra\",\n",
    "    \"ayoub_hh: ns\",\n",
    "    \"bearrynice: @tarik you can satchel? Since when Lil bro\",\n",
    "    \"KorHun_Official: kangkang gets 5 here @tarik\",\n",
    "    \"suus001: OHHHH SHIT\",\n",
    "    \"ub_zinio: overheaaat\",\n",
    "    \"ironman_ap: sup ? @Derke\",\n",
    "    \"Schabii97: DERKE W\",\n",
    "    \"Grediann: overpeak = die Shruge\",\n",
    "    \"jaybird1014: SIT DOWN PLS\",\n",
    "    \"ayswoosh: @Derke how were champs?\",\n",
    "    \"nishikoto: NASTY\",\n",
    "    \"thickymonster: !duo\",\n",
    "    \"wddcruz: 3King\",\n",
    "    \"sqawg: Lil bro humbled himself\",\n",
    "    \"Replying to @thickymonster: !duoFossabot: Asuna AYAYA\",\n",
    "    \"littlesmchallowen: do that next round kekw\",\n",
    "    \"davidakachuwy: COOKED then OVERPEEKED\",\n",
    "    \"MER_AKI: you are not himothy\",\n",
    "    \"lotace:     \",\n",
    "    \"ditt0o: we've got huge bets don't ROZA\",\n",
    "    \"gme16: that spray transfer was lit as\",\n",
    "    \"iicpr: overheat\",\n",
    "    \"daymare5: it was horrible\",\n",
    "    \"Sigfreed: NA BRAIN KEKW\",\n",
    "    \"samsaraeyess: that spray transfer made me ink\",\n",
    "    \"hwhevevsvb: no\",\n",
    "    \"suus001: TUROK TUROK TUROK\",\n",
    "    \"SilintNight: OMEGALUL\",\n",
    "    \"Derke: NO\",\n",
    "    \"mr_01ne: Derke knew it\",\n",
    "    \"Replying to @Derke: i told uQuanFuPanda: deadass\",\n",
    "    \"dioholic: terue\",\n",
    "    \"ta3sk1: THIS TEAM IS FUCKING GOATED TARIK/STEW/ASUNA GGZ\",\n",
    "    \"rue__s: heeey\",\n",
    "    \"abcdgwenchana: overheat on eco\",\n",
    "    \"AdderallBeforeBed: bet you can't do it again MmmHmm\",\n",
    "    \"sissimou: fax\",\n",
    "    \"xcrimsoncrookx: bro thinks the transfer was intentional AINTNOWAY\",\n",
    "    \"adityasanas001: Ecodemon\",\n",
    "    \"dioholic: true\",\n",
    "    \"Lefluu: stew did everything there @tarik\",\n",
    "    \"CosmicDeven: two eco frags and we start talking shit on derke KEKW\",\n",
    "    \"Benjjamin: If you get 3 you're allowed to throw\",\n",
    "    \"afor_f: its true\",\n",
    "    \"Derke: IF ITS 5V1\",\n",
    "    \"Harnasiek03: true\",\n",
    "    \"lowertaxrates: no?\",\n",
    "    \"ketosaiba11: replace jinggg no?\",\n",
    "    \"theak44: BLABBERING BLABBERING\",\n",
    "    \"Derke: AND I DIE FIRST\",\n",
    "    \"laiiiny: You should apply for observer in VCT\",\n",
    "    \"itsrawkus: wake up\",\n",
    "    \"tripharder: ahh yes the rule\",\n",
    "    \"shruggy8: TRUEING\",\n",
    "    \"autumn0999: nice fucking shots tho\",\n",
    "    \"Derke: ITS MY FAULT\",\n",
    "    \"rishon26: @Derke get this man on fnatic\",\n",
    "    \"OzGunAim: !sens\",\n",
    "    \"Fossabot: CSGO: 1.5 @ 800 DPI, VALORANT: .471 800 DPI\",\n",
    "    \"JRD_Nath: \",\n",
    "    \"xdpotatolord: @tarik UR BICEPS ARE HUGE!!!\",\n",
    "    \"xkillo147: True, NA rule\",\n",
    "    \"gentlecpu: KEKW if you get 1 it's not your fault\",\n",
    "    \"jinsoooo: if you get 2 you go for the ace\",\n",
    "    \"rightylucy: Lkekw\",\n",
    "    \"lionbrav3: C9 VIBEZ\",\n",
    "    \"emil__val: KEKW KEKW\",\n",
    "    \"maareeyyyy: !mouse\",\n",
    "    \"FarmerFelox: In NA if you get 1 go for 5\",\n",
    "    \"abcdgwenchana: eco frag\",\n",
    "    \"alirezathe1: !res\",\n",
    "    \"Fossabot: DeathAdder V3 Pro\",\n",
    "    \"Fossabot: Val 16:10 (1680x1050) - CSGO: 1280x960\",\n",
    "    \"CaliKillz3: TRUEING\",\n",
    "    \"rentr04: homie turned up cuz derke is watching. respect\",\n",
    "    \"hyp3r10n2: @tarik gets 3 wins round then overfaces and gets mad for it xD\",\n",
    "    \"rightylucy: KEKW\",\n",
    "    \"riyuoh: IF U GET 3 YOU CAN OVERHEAT 100%\",\n",
    "    \"PiquesGaming: thats facts tho\",\n",
    "    \"gkhn94: Dayi bi kere turkce konus be\",\n",
    "    \"Sigfreed: LOOK ITS A 1V1 NOW\",\n",
    "    \"ä¸ä¹šä»¨ä¹‚ (tlex): KLÄ°PPP\",\n",
    "    \"myinnerfaye: Himothy is that you?\",\n",
    "    \"Maximus6267: KEKW no way\",\n",
    "    \"tsylogy: @Derke 5V1 DSG Aware\",\n",
    "    \"ub_zinio: derkes fault\",\n",
    "    \"itzzero3: arabic blood\",\n",
    "    \"shruggy8: gonna lose PepeLaugh\",\n",
    "    \"Sigfreed: ITS A FUCKING 1V1 NOW\",\n",
    "    \"Replying to @lionbrav3: C9 VIBEZXeppaa: ?\",\n",
    "    \"ItsTavyy: maybe you need to peek more @tarik\",\n",
    "    \"danielmacttv: Its DERKEâ€™s fault\",\n",
    "    \"dexterityCS: KEKW ó €€\",\n",
    "    \"Rickz10K: KEKW KEKW KEKW KEKW\",\n",
    "    \"kaizo_rm: HUH ó €€\",\n",
    "    \"slaxxxyyyy: Fair enuff\",\n",
    "    \"diipsy9: AYOO HH\",\n",
    "    \"h1k1k0_: HUH\",\n",
    "    \"m0gi08: !gekko\",\n",
    "    \"Apollo_Neptune: HUH\",\n",
    "    \"Fossabot: LilBro it's gekkin time ezz\",\n",
    "    \"siwa33: Close gamba mods\",\n",
    "    \"wahbi_79: HUH\",\n",
    "    \"xelzttv: HUH\",\n",
    "    \"emil__val: KEKW\",\n",
    "    \"ig5mindhacker: HUH\",\n",
    "    \"krasqu33: HUH\",\n",
    "    \"aqilus: HUH\",\n",
    "    \"Jordbaermelk: WOT\",\n",
    "    \"derkesdoormat: @derke OOO DERKE'S HERE HII\",\n",
    "    \"beepbopp11: HUH\",\n",
    "    \"cenk4k: HUH\",\n",
    "    \"mrsteallyourcat: HUH\",\n",
    "    \"xclaassic: true\",\n",
    "    \"Aethielle: HUH\",\n",
    "    \"cyb_eric: HUH\",\n",
    "    \"shruggy8: Sadge\",\n",
    "    \"mesme_R: HUH\"\n",
    "]\n",
    "stream_comments = [comment.split(':')[1] for comment in stream_comments]\n",
    "stream_comments = [comment for comment in stream_comments if len(comment.strip()) > 0]\n",
    "    \n",
    "premise = stream_comments[:10]\n",
    "label = 'cool'\n",
    "hypothesis = f'This example is {label}.'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                     truncation_strategy='only_first')\n",
    "logits = nli_model(x.to('cpu'))[0]\n",
    "\n",
    "# we throw away \"neutral\" (dim 1) and take the probability of\n",
    "# \"entailment\" (2) as the probability of the label being true \n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "prob_label_is_true = probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7fb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_label_is_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "(summarizer(stream_comments, max_length=20, min_length=5, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fb3fe0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stream_comments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m sequence_to_classify \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone day I will see the world\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m candidate_labels \u001b[38;5;241m=\u001b[39m classifications\n\u001b[0;32m---> 28\u001b[0m classifier(\u001b[43mstream_comments\u001b[49m[\u001b[38;5;241m0\u001b[39m], candidate_labels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stream_comments' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c5c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "sentiments = sentiment_pipeline(stream_comments)\n",
    "\n",
    "[f\"{stream_comments[i]}   {sentiments[i]['label']}  {sentiments[i]['score']}\" for i, char in enumerate(sentiments)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "+from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n",
    "inputs = tokenizer(\"eat lots of green beans and black eyed \", return_tensors=\"pt\").input_ids\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "[tokenizer.decode(string, skip_special_tokens=True) for string in outputs]\n",
    "\n",
    "\n",
    "#get summary of them\n",
    "#organize by cluster\n",
    "    #sentiment\n",
    "    \n",
    "    \n",
    "#download clips https://youtu.be/HigmUsGEEww -> get transcript to provide tagging\n",
    "#download https://www.youtube.com/watch?v=HigmUsGEEww&feature=youtu.be&ab_channel=Joe-Astro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4101c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d3d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, max pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "## Setting to use the 0th GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "## Setting to use the bart-large-cnn model for summarization\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "## To use the t5-base model for summarization:\n",
    "## summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\n",
    "text = \"\"\"One month after the United States began what has become a troubled rollout of a national COVID vaccination campaign, the effort is finally gathering real steam.\n",
    "Close to a million doses -- over 951,000, to be more exact -- made their way into the arms of Americans in the past 24 hours, the U.S. Centers for Disease Control and Prevention reported Wednesday. That's the largest number of shots given in one day since the rollout began and a big jump from the previous day, when just under 340,000 doses were given, CBS News reported.\n",
    "That number is likely to jump quickly after the federal government on Tuesday gave states the OK to vaccinate anyone over 65 and said it would release all the doses of vaccine it has available for distribution. Meanwhile, a number of states have now opened mass vaccination sites in an effort to get larger numbers of people inoculated, CBS News reported.\"\"\"\n",
    "#Summarize\n",
    "summary_text = summarizer('\\n'.join(stream_comments)[:1022], max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
    "print(summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68091c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getClusters(sentences):\n",
    "    clusters = util.community_detection(encode(sentences), min_community_size=1, threshold=0.5)\n",
    "    clusters\n",
    "\n",
    "    def process(item): return [sentences[i] for i in item]\n",
    "    result = [process(item) for item in clusters ]\n",
    "    return result\n",
    "\n",
    "clusters = getClusters(stream_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648dab9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiktionaryparser import WiktionaryParser\n",
    "\n",
    "parser = WiktionaryParser()\n",
    "word = parser.fetch('eating')\n",
    "#another_word = parser.fetch('test', 'french')\n",
    "# parser.set_default_language('french')\n",
    "# parser.exclude_part_of_speech('noun')\n",
    "# parser.include_relation('alternative forms')\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb40cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def generalize_word(word):\n",
    "    try:\n",
    "        similar_words = w2v_model.most_similar(positive=[word], topn=5)\n",
    "        generalized_word = similar_words[0][0]  # Get the most similar word\n",
    "        return generalized_word\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "word = \"cake\"\n",
    "generalized_word = generalize_word(word)\n",
    "if generalized_word:\n",
    "    print(f\"The generalized term for '{word}' is '{generalized_word}'.\")\n",
    "else:\n",
    "    print(f\"No generalization found for '{word}'.\")\n",
    "\n",
    "word = \"burger\"\n",
    "generalized_word = generalize_word(word)\n",
    "if generalized_word:\n",
    "    print(f\"The generalized term for '{word}' is '{generalized_word}'.\")\n",
    "else:\n",
    "    print(f\"No generalization found for '{word}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorize heckling\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def generalize_word(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    \n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    # Get the first synset (most common meaning)\n",
    "    synset = synsets[0]\n",
    "    \n",
    "    # Find hypernyms (more general terms)\n",
    "    hypernyms = synset.hypernyms()\n",
    "    \n",
    "    if not hypernyms:\n",
    "        return None\n",
    "    \n",
    "    # Get the first hypernym (more general term)\n",
    "    hypernym = hypernyms[0]\n",
    "    \n",
    "    # Extract the lemma name of the hypernym\n",
    "    generalized_word = hypernym.lemmas()[0].name()\n",
    "    \n",
    "    return generalized_word\n",
    "\n",
    "word = \"toast\"\n",
    "generalized_word = generalize_word(word)\n",
    "if generalized_word:\n",
    "    print(f\"The generalized term for '{word}' is '{generalized_word}'.\")\n",
    "else:\n",
    "    print(f\"No generalization found for '{word}'.\")\n",
    "\n",
    "word = \"hamburger\"\n",
    "generalized_word = generalize_word(word)\n",
    "if generalized_word:\n",
    "    print(f\"The generalized term for '{word}' is '{generalized_word}'.\")\n",
    "else:\n",
    "    print(f\"No generalization found for '{word}'.\")\n",
    "    \n",
    "    \n",
    "t = wordnet.synsets('popcorn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b4ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicTitle(text):\n",
    "    print('translating ' + text)\n",
    "    prompt = f\"How would you classify '{text}' as a topic?\"\n",
    "    first = time.perf_counter()\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates text.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    second = time.perf_counter()\n",
    "    print(second - first)\n",
    "    translation = response.choices[0].message.content.strip()\n",
    "    return translation\n",
    "\n",
    "getTopicTitle(stream_comments[10])\n",
    "\n",
    "stream_comments[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc60c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rh= \"\"\"Talk at Bellcore, 7 March 1986\n",
    "\n",
    "The title of my talk is \"You and Your Research.\" It is not about managing research, it is about how you individually do your research. I could give a talk on the other subject â€” but it's not, it's about you. I'm not talking about ordinary run-of-the-mill research; I'm talking about great research. And for the sake of describing great research I'll occasionally say Nobel-Prize type of work. It doesn't have to gain the Nobel Prize, but I mean those kinds of things which we perceive are significant things. Relativity, if you want, Shannon's information theory, any number of outstanding theories â€” that's the kind of thing I'm talking about.\n",
    "\n",
    "Now, how did I come to do this study? At Los Alamos I was brought in to run the computing machines which other people had got going, so those scientists and physicists could get back to business. I saw I was a stooge. I saw that although physically I was the same, they were different. And to put the thing bluntly, I was envious. I wanted to know why they were so different from me. I saw Feynman up close. I saw Fermi and Teller. I saw Oppenheimer. I saw Hans Bethe: he was my boss. I saw quite a few very capable people. I became very interested in the difference between those who do and those who might have done.\n",
    "\n",
    "When I came to Bell Labs, I came into a very productive department. Bode was the department head at the time; Shannon was there, and there were other people. I continued examining the questions, \"Why?\" and \"What is the difference?\" I continued subsequently by reading biographies, autobiographies, asking people questions such as: \"How did you come to do this?\" I tried to find out what are the differences. And that's what this talk is about.\n",
    "\n",
    "Now, why is this talk important? I think it is important because, as far as I know, each of you has one life to live. Even if you believe in reincarnation it doesn't do you any good from one life to the next! Why shouldn't you do significant things in this one life, however you define significant? I'm not going to define it â€” you know what I mean. I will talk mainly about science because that is what I have studied. But so far as I know, and I've been told by others, much of what I say applies to many fields. Outstanding work is characterized very much the same way in most fields, but I will confine myself to science.\n",
    "\n",
    "In order to get at you individually, I must talk in the first person. I have to get you to drop modesty and say to yourself, \"Yes, I would like to do first-class work.\" Our society frowns on people who set out to do really good work. You're not supposed to; luck is supposed to descend on you and you do great things by chance. Well, that's a kind of dumb thing to say. I say, why shouldn't you set out to do something significant. You don't have to tell other people, but shouldn't you say to yourself, \"Yes, I would like to do something significant.\"\n",
    "\n",
    "In order to get to the second stage, I have to drop modesty and talk in the first person about what I've seen, what I've done, and what I've heard. I'm going to talk about people, some of whom you know, and I trust that when we leave, you won't quote me as saying some of the things I said.\n",
    "\n",
    "Let me start not logically, but psychologically. I find that the major objection is that people think great science is done by luck. It's all a matter of luck. Well, consider Einstein. Note how many different things he did that were good. Was it all luck? Wasn't it a little too repetitive? Consider Shannon. He didn't do just information theory. Several years before, he did some other good things and some which are still locked up in the security of cryptography. He did many good things.\n",
    "\n",
    "You see again and again that it is more than one thing from a good person. Once in a while a person does only one thing in his whole life, and we'll talk about that later, but a lot of times there is repetition. I claim that luck will not cover everything. And I will cite Pasteur who said, \"Luck favors the prepared mind.\" And I think that says it the way I believe it. There is indeed an element of luck, and no, there isn't. The prepared mind sooner or later finds something important and does it. So yes, it is luck. The particular thing you do is luck, but that you do something is not.\n",
    "\n",
    "For example, when I came to Bell Labs, I shared an office for a while with Shannon. At the same time he was doing information theory, I was doing coding theory. It is suspicious that the two of us did it at the same place and at the same time â€” it was in the atmosphere. And you can say, \"Yes, it was luck.\" On the other hand you can say, \"But why of all the people in Bell Labs then were those the two who did it?\" Yes, it is partly luck, and partly it is the prepared mind; but \"partly\" is the other thing I'm going to talk about. So, although I'll come back several more times to luck, I want to dispose of this matter of luck as being the sole criterion whether you do great work or not. I claim you have some, but not total, control over it. And I will quote, finally, Newton on the matter. Newton said, \"If others would think as hard as I did, then they would get similar results.\"\n",
    "\n",
    "One of the characteristics you see, and many people have it including great scientists, is that usually when they were young they had independent thoughts and had the courage to pursue them. For example, Einstein, somewhere around 12 or 14, asked himself the question, \"What would a light wave look like if I went with the velocity of light to look at it?\" Now he knew that electromagnetic theory says you cannot have a stationary local maximum. But if he moved along with the velocity of light, he would see a local maximum. He could see a contradiction at the age of 12, 14, or somewhere around there, that everything was not right and that the velocity of light had something peculiar. Is it luck that he finally created special relativity? Early on, he had laid down some of the pieces by thinking of the fragments. Now that's the necessary but not sufficient condition. All of these items I will talk about are both luck and not luck.\n",
    "\n",
    "How about having lots of brains? It sounds good. Most of you in this room probably have more than enough brains to do first-class work. But great work is something else than mere brains. Brains are measured in various ways. In mathematics, theoretical physics, astrophysics, typically brains correlates to a great extent with the ability to manipulate symbols. And so the typical IQ test is apt to score them fairly high. On the other hand, in other fields it is something different. For example, Bill Pfann, the fellow who did zone melting, came into my office one day. He had this idea dimly in his mind about what he wanted and he had some equations. It was pretty clear to me that this man didn't know much mathematics and he wasn't really articulate. His problem seemed interesting so I took it home and did a little work. I finally showed him how to run computers so he could compute his own answers. I gave him the power to compute. He went ahead, with negligible recognition from his own department, but ultimately he has collected all the prizes in the field. Once he got well started, his shyness, his awkwardness, his inarticulateness, fell away and he became much more productive in many other ways. Certainly he became much more articulate.\n",
    "\n",
    "And I can cite another person in the same way. I trust he isn't in the audience, i.e. a fellow named Clogston. I met him when I was working on a problem with John Pierce's group and I didn't think he had much. I asked my friends who had been with him at school, \"Was he like that in graduate school?\" \"Yes,\" they replied. Well I would have fired the fellow, but J. R. Pierce was smart and kept him on. Clogston finally did the Clogston cable. After that there was a steady stream of good ideas. One success brought him confidence and courage.\n",
    "\n",
    "One of the characteristics of successful scientists is having courage. Once you get your courage up and believe that you can do important problems, then you can. If you think you can't, almost surely you are not going to. Courage is one of the things that Shannon had supremely. You have only to think of his major theorem. He wants to create a method of coding, but he doesn't know what to do so he makes a random code. Then he is stuck. And then he asks the impossible question, \"What would the average random code do?\" He then proves that the average code is arbitrarily good, and that therefore there must be at least one good code. Who but a man of infinite courage could have dared to think those thoughts? That is the characteristic of great scientists; they have courage. They will go forward under incredible circumstances; they think and continue to think.\n",
    "\n",
    "Age is another factor which the physicists particularly worry about. They always are saying that you have got to do it when you are young or you will never do it. Einstein did things very early, and all the quantum mechanic fellows were disgustingly young when they did their best work. Most mathematicians, theoretical physicists, and astrophysicists do what we consider their best work when they are young. It is not that they don't do good work in their old age but what we value most is often what they did early. On the other hand, in music, politics and literature, often what we consider their best work was done late. I don't know how whatever field you are in fits this scale, but age has some effect.\n",
    "\n",
    "But let me say why age seems to have the effect it does. In the first place if you do some good work you will find yourself on all kinds of committees and unable to do any more work. You may find yourself as I saw Brattain when he got a Nobel Prize. The day the prize was announced we all assembled in Arnold Auditorium; all three winners got up and made speeches. The third one, Brattain, practically with tears in his eyes, said, \"I know about this Nobel-Prize effect and I am not going to let it affect me; I am going to remain good old Walter Brattain.\" Well I said to myself, \"That is nice.\" But in a few weeks I saw it was affecting him. Now he could only work on great problems.\n",
    "\n",
    "When you are famous it is hard to work on small problems. This is what did Shannon in. After information theory, what do you do for an encore? The great scientists often make this error. They fail to continue to plant the little acorns from which the mighty oak trees grow. They try to get the big thing right off. And that isn't the way things go. So that is another reason why you find that when you get early recognition it seems to sterilize you. In fact I will give you my favorite quotation of many years. The Institute for Advanced Study in Princeton, in my opinion, has ruined more good scientists than any institution has created, judged by what they did before they came and judged by what they did after. Not that they weren't good afterwards, but they were superb before they got there and were only good afterwards.\n",
    "\n",
    "This brings up the subject, out of order perhaps, of working conditions. What most people think are the best working conditions, are not. Very clearly they are not because people are often most productive when working conditions are bad. One of the better times of the Cambridge Physical Laboratories was when they had practically shacks â€” they did some of the best physics ever.\n",
    "\n",
    "I give you a story from my own private life. Early on it became evident to me that Bell Laboratories was not going to give me the conventional acre of programming people to program computing machines in absolute binary. It was clear they weren't going to. But that was the way everybody did it. I could go to the West Coast and get a job with the airplane companies without any trouble, but the exciting people were at Bell Labs and the fellows out there in the airplane companies were not. I thought for a long while about, \"Did I want to go or not?\" and I wondered how I could get the best of two possible worlds. I finally said to myself, \"Hamming, you think the machines can do practically everything. Why can't you make them write programs?\" What appeared at first to me as a defect forced me into automatic programming very early. What appears to be a fault, often, by a change of viewpoint, turns out to be one of the greatest assets you can have. But you are not likely to think that when you first look the thing and say, \"Gee, I'm never going to get enough programmers, so how can I ever do any great programming?\"\n",
    "\n",
    "And there are many other stories of the same kind; Grace Hopper has similar ones. I think that if you look carefully you will see that often the great scientists, by turning the problem around a bit, changed a defect to an asset. For example, many scientists when they found they couldn't do a problem finally began to study why not. They then turned it around the other way and said, \"But of course, this is what it is\" and got an important result. So ideal working conditions are very strange. The ones you want aren't always the best ones for you.\n",
    "\n",
    "Now for the matter of drive. You observe that most great scientists have tremendous drive. I worked for ten years with John Tukey at Bell Labs. He had tremendous drive. One day about three or four years after I joined, I discovered that John Tukey was slightly younger than I was. John was a genius and I clearly was not. Well I went storming into Bode's office and said, \"How can anybody my age know as much as John Tukey does?\" He leaned back in his chair, put his hands behind his head, grinned slightly, and said, \"You would be surprised Hamming, how much you would know if you worked as hard as he did that many years.\" I simply slunk out of the office!\n",
    "\n",
    "What Bode was saying was this: Knowledge and productivity are like compound interest. Given two people of approximately the same ability and one person who works ten percent more than the other, the latter will more than twice outproduce the former. The more you know, the more you learn; the more you learn, the more you can do; the more you can do, the more the opportunity â€” it is very much like compound interest. I don't want to give you a rate, but it is a very high rate. Given two people with exactly the same ability, the one person who manages day in and day out to get in one more hour of thinking will be tremendously more productive over a lifetime. I took Bode's remark to heart; I spent a good deal more of my time for some years trying to work a bit harder and I found, in fact, I could get more work done. I don't like to say it in front of my wife, but I did sort of neglect her sometimes; I needed to study. You have to neglect things if you intend to get what you want done. There's no question about this.\n",
    "\n",
    "On this matter of drive Edison says, \"Genius is 99% perspiration and 1% inspiration.\" He may have been exaggerating, but the idea is that solid work, steadily applied, gets you surprisingly far. The steady application of effort with a little bit more work, intelligently applied is what does it. That's the trouble; drive, misapplied, doesn't get you anywhere. I've often wondered why so many of my good friends at Bell Labs who worked as hard or harder than I did, didn't have so much to show for it. The misapplication of effort is a very serious matter. Just hard work is not enough - it must be applied sensibly.\n",
    "\n",
    "There's another trait on the side which I want to talk about; that trait is ambiguity. It took me a while to discover its importance. Most people like to believe something is or is not true. Great scientists tolerate ambiguity very well. They believe the theory enough to go ahead; they doubt it enough to notice the errors and faults so they can step forward and create the new replacement theory. If you believe too much you'll never notice the flaws; if you doubt too much you won't get started. It requires a lovely balance. But most great scientists are well aware of why their theories are true and they are also well aware of some slight misfits which don't quite fit and they don't forget it. Darwin writes in his autobiography that he found it necessary to write down every piece of evidence which appeared to contradict his beliefs because otherwise they would disappear from his mind. When you find apparent flaws you've got to be sensitive and keep track of those things, and keep an eye out for how they can be explained or how the theory can be changed to fit them. Those are often the great contributions. Great contributions are rarely done by adding another decimal place. It comes down to an emotional commitment. Most great scientists are completely committed to their problem. Those who don't become committed seldom produce outstanding, first-class work.\n",
    "\n",
    "Now again, emotional commitment is not enough. It is a necessary condition apparently. And I think I can tell you the reason why. Everybody who has studied creativity is driven finally to saying, \"creativity comes out of your subconscious.\" Somehow, suddenly, there it is. It just appears. Well, we know very little about the subconscious; but one thing you are pretty well aware of is that your dreams also come out of your subconscious. And you're aware your dreams are, to a fair extent, a reworking of the experiences of the day. If you are deeply immersed and committed to a topic, day after day after day, your subconscious has nothing to do but work on your problem. And so you wake up one morning, or on some afternoon, and there's the answer. For those who don't get committed to their current problem, the subconscious goofs off on other things and doesn't produce the big result. So the way to manage yourself is that when you have a real important problem you don't let anything else get the center of your attention â€” you keep your thoughts on the problem. Keep your subconscious starved so it has to work on your problem, so you can sleep peacefully and get the answer in the morning, free.\n",
    "\n",
    "Now Alan Chynoweth mentioned that I used to eat at the physics table. I had been eating with the mathematicians and I found out that I already knew a fair amount of mathematics; in fact, I wasn't learning much. The physics table was, as he said, an exciting place, but I think he exaggerated on how much I contributed. It was very interesting to listen to Shockley, Brattain, Bardeen, J. B. Johnson, Ken McKay and other people, and I was learning a lot. But unfortunately a Nobel Prize came, and a promotion came, and what was left was the dregs. Nobody wanted what was left. Well, there was no use eating with them!\n",
    "\n",
    "Over on the other side of the dining hall was a chemistry table. I had worked with one of the fellows, Dave McCall; furthermore he was courting our secretary at the time. I went over and said, \"Do you mind if I join you?\" They can't say no, so I started eating with them for a while. And I started asking, \"What are the important problems of your field?\" And after a week or so, \"What important problems are you working on?\" And after some more time I came in one day and said, \"If what you are doing is not important, and if you don't think it is going to lead to something important, why are you at Bell Labs working on it?\" I wasn't welcomed after that; I had to find somebody else to eat with! That was in the spring.\n",
    "\n",
    "In the fall, Dave McCall stopped me in the hall and said, \"Hamming, that remark of yours got underneath my skin. I thought about it all summer, i.e. what were the important problems in my field. I haven't changed my research,\" he says, \"but I think it was well worthwhile.\" And I said, \"Thank you Dave,\" and went on. I noticed a couple of months later he was made the head of the department. I noticed the other day he was a Member of the National Academy of Engineering. I noticed he has succeeded. I have never heard the names of any of the other fellows at that table mentioned in science and scientific circles. They were unable to ask themselves, \"What are the important problems in my field?\"\n",
    "\n",
    "If you do not work on an important problem, it's unlikely you'll do important work. It's perfectly obvious. Great scientists have thought through, in a careful way, a number of important problems in their field, and they keep an eye on wondering how to attack them. Let me warn you, \"important problem\" must be phrased carefully. The three outstanding problems in physics, in a certain sense, were never worked on while I was at Bell Labs. By important I mean guaranteed a Nobel Prize and any sum of money you want to mention. We didn't work on (1) time travel, (2) teleportation, and (3) antigravity. They are not important problems because we do not have an attack. It's not the consequence that makes a problem important, it is that you have a reasonable attack. That is what makes a problem important. When I say that most scientists don't work on important problems, I mean it in that sense. The average scientist, so far as I can make out, spends almost all his time working on problems which he believes will not be important and he also doesn't believe that they will lead to important problems.\n",
    "\n",
    "I spoke earlier about planting acorns so that oaks will grow. You can't always know exactly where to be, but you can keep active in places where something might happen. And even if you believe that great science is a matter of luck, you can stand on a mountain top where lightning strikes; you don't have to hide in the valley where you're safe. But the average scientist does routine safe work almost all the time and so he (or she) doesn't produce much. It's that simple. If you want to do great work, you clearly must work on important problems, and you should have an idea.\n",
    "\n",
    "Along those lines at some urging from John Tukey and others, I finally adopted what I called \"Great Thoughts Time.\" When I went to lunch Friday noon, I would only discuss great thoughts after that. By great thoughts I mean ones like: \"What will be the role of computers in all of AT&T?\", \"How will computers change science?\" For example, I came up with the observation at that time that nine out of ten experiments were done in the lab and one in ten on the computer. I made a remark to the vice presidents one time, that it would be reversed, i.e. nine out of ten experiments would be done on the computer and one in ten in the lab. They knew I was a crazy mathematician and had no sense of reality. I knew they were wrong and they've been proved wrong while I have been proved right. They built laboratories when they didn't need them. I saw that computers were transforming science because I spent a lot of time asking \"What will be the impact of computers on science and how can I change it?\" I asked myself, \"How is it going to change Bell Labs?\" I remarked one time, in the same address, that more than one-half of the people at Bell Labs will be interacting closely with computing machines before I leave. Well, you all have terminals now. I thought hard about where was my field going, where were the opportunities, and what were the important things to do. Let me go there so there is a chance I can do important things.\n",
    "\n",
    "Most great scientists know many important problems. They have something between 10 and 20 important problems for which they are looking for an attack. And when they see a new idea come up, one hears them say \"Well that bears on this problem.\" They drop all the other things and get after it. Now I can tell you a horror story that was told to me but I can't vouch for the truth of it. I was sitting in an airport talking to a friend of mine from Los Alamos about how it was lucky that the fission experiment occurred over in Europe when it did because that got us working on the atomic bomb here in the US. He said \"No; at Berkeley we had gathered a bunch of data; we didn't get around to reducing it because we were building some more equipment, but if we had reduced that data we would have found fission.\" They had it in their hands and they didn't pursue it. They came in second!\n",
    "\n",
    "The great scientists, when an opportunity opens up, get after it and they pursue it. They drop all other things. They get rid of other things and they get after an idea because they had already thought the thing through. Their minds are prepared; they see the opportunity and they go after it. Now of course lots of times it doesn't work out, but you don't have to hit many of them to do some great science. It's kind of easy. One of the chief tricks is to live a long time!\n",
    "\n",
    "Another trait, it took me a while to notice. I noticed the following facts about people who work with the door open or the door closed. I notice that if you have the door to your office closed, you get more work done today and tomorrow, and you are more productive than most. But 10 years later somehow you don't know quite know what problems are worth working on; all the hard work you do is sort of tangential in importance. He who works with the door open gets all kinds of interruptions, but he also occasionally gets clues as to what the world is and what might be important. Now I cannot prove the cause and effect sequence because you might say, \"The closed door is symbolic of a closed mind.\" I don't know. But I can say there is a pretty good correlation between those who work with the doors open and those who ultimately do important things, although people who work with doors closed often work harder. Somehow they seem to work on slightly the wrong thing â€” not much, but enough that they miss fame.\n",
    "\n",
    "I want to talk on another topic. It is based on the song which I think many of you know, \"It ain't what you do, it's the way that you do it.\" I'll start with an example of my own. I was conned into doing on a digital computer, in the absolute binary days, a problem which the best analog computers couldn't do. And I was getting an answer. When I thought carefully and said to myself, \"You know, Hamming, you're going to have to file a report on this military job; after you spend a lot of money you're going to have to account for it and every analog installation is going to want the report to see if they can't find flaws in it.\" I was doing the required integration by a rather crummy method, to say the least, but I was getting the answer. And I realized that in truth the problem was not just to get the answer; it was to demonstrate for the first time, and beyond question, that I could beat the analog computer on its own ground with a digital machine. I reworked the method of solution, created a theory which was nice and elegant, and changed the way we computed the answer; the results were no different. The published report had an elegant method which was later known for years as \"Hamming's Method of Integrating Differential Equations.\" It is somewhat obsolete now, but for a while it was a very good method. By changing the problem slightly, I did important work rather than trivial work.\n",
    "\n",
    "In the same way, when using the machine up in the attic in the early days, I was solving one problem after another after another; a fair number were successful and there were a few failures. I went home one Friday after finishing a problem, and curiously enough I wasn't happy; I was depressed. I could see life being a long sequence of one problem after another after another. After quite a while of thinking I decided, \"No, I should be in the mass production of a variable product. I should be concerned with all of next year's problems, not just the one in front of my face.\" By changing the question I still got the same kind of results or better, but I changed things and did important work. I attacked the major problem â€” How do I conquer machines and do all of next year's problems when I don't know what they are going to be? How do I prepare for it? How do I do this one so I'll be on top of it? How do I obey Newton's rule? He said, \"If I have seen further than others, it is because I've stood on the shoulders of giants.\" These days we stand on each other's feet!\n",
    "\n",
    "You should do your job in such a fashion that others can build on top of it, so they will indeed say, \"Yes, I've stood on so and so's shoulders and I saw further.\" The essence of science is cumulative. By changing a problem slightly you can often do great work rather than merely good work. Instead of attacking isolated problems, I made the resolution that I would never again solve an isolated problem except as characteristic of a class.\n",
    "\n",
    "Now if you are much of a mathematician you know that the effort to generalize often means that the solution is simple. Often by stopping and saying, \"This is the problem he wants but this is characteristic of so and so. Yes, I can attack the whole class with a far superior method than the particular one because I was earlier embedded in needless detail.\" The business of abstraction frequently makes things simple. Furthermore, I filed away the methods and prepared for the future problems.\n",
    "\n",
    "To end this part, I'll remind you, \"It is a poor workman who blames his tools â€” the good man gets on with the job, given what he's got, and gets the best answer he can.\" And I suggest that by altering the problem, by looking at the thing differently, you can make a great deal of difference in your final productivity because you can either do it in such a fashion that people can indeed build on what you've done, or you can do it in such a fashion that the next person has to essentially duplicate again what you've done. It isn't just a matter of the job, it's the way you write the report, the way you write the paper, the whole attitude. It's just as easy to do a broad, general job as one very special case. And it's much more satisfying and rewarding!\n",
    "\n",
    "I have now come down to a topic which is very distasteful; it is not sufficient to do a job, you have to sell it. \"Selling\" to a scientist is an awkward thing to do. It's very ugly; you shouldn't have to do it. The world is supposed to be waiting, and when you do something great, they should rush out and welcome it. But the fact is everyone is busy with their own work. You must present it so well that they will set aside what they are doing, look at what you've done, read it, and come back and say, \"Yes, that was good.\" I suggest that when you open a journal, as you turn the pages, you ask why you read some articles and not others. You had better write your report so when it is published in the Physical Review, or wherever else you want it, as the readers are turning the pages they won't just turn your pages but they will stop and read yours. If they don't stop and read it, you won't get credit.\n",
    "\n",
    "There are three things you have to do in selling. You have to learn to write clearly and well so that people will read it, you must learn to give reasonably formal talks, and you also must learn to give informal talks. We had a lot of so-called `back room scientists.' In a conference, they would keep quiet. Three weeks later after a decision was made they filed a report saying why you should do so and so. Well, it was too late. They would not stand up right in the middle of a hot conference, in the middle of activity, and say, \"We should do this for these reasons.\" You need to master that form of communication as well as prepared speeches.\n",
    "\n",
    "When I first started, I got practically physically ill while giving a speech, and I was very, very nervous. I realized I either had to learn to give speeches smoothly or I would essentially partially cripple my whole career. The first time IBM asked me to give a speech in New York one evening, I decided I was going to give a really good speech, a speech that was wanted, not a technical one but a broad one, and at the end if they liked it, I'd quietly say, \"Any time you want one I'll come in and give you one.\" As a result, I got a great deal of practice giving speeches to a limited audience and I got over being afraid. Furthermore, I could also then study what methods were effective and what were ineffective.\n",
    "\n",
    "While going to meetings I had already been studying why some papers are remembered and most are not. The technical person wants to give a highly limited technical talk. Most of the time the audience wants a broad general talk and wants much more survey and background than the speaker is willing to give. As a result, many talks are ineffective. The speaker names a topic and suddenly plunges into the details he's solved. Few people in the audience may follow. You should paint a general picture to say why it's important, and then slowly give a sketch of what was done. Then a larger number of people will say, \"Yes, Joe has done that,\" or \"Mary has done that; I really see where it is; yes, Mary really gave a good talk; I understand what Mary has done.\" The tendency is to give a highly restricted, safe talk; this is usually ineffective. Furthermore, many talks are filled with far too much information. So I say this idea of selling is obvious.\n",
    "\n",
    "Let me summarize. You've got to work on important problems. I deny that it is all luck, but I admit there is a fair element of luck. I subscribe to Pasteur's \"Luck favors the prepared mind.\" I favor heavily what I did. Friday afternoons for years â€” great thoughts only â€” means that I committed 10% of my time trying to understand the bigger problems in the field, i.e. what was and what was not important. I found in the early days I had believed `this' and yet had spent all week marching in `that' direction. It was kind of foolish. If I really believe the action is over there, why do I march in this direction? I either had to change my goal or change what I did. So I changed something I did and I marched in the direction I thought was important. It's that easy.\n",
    "\n",
    "Now you might tell me you haven't got control over what you have to work on. Well, when you first begin, you may not. But once you're moderately successful, there are more people asking for results than you can deliver and you have some power of choice, but not completely. I'll tell you a story about that, and it bears on the subject of educating your boss. I had a boss named Schelkunoff; he was, and still is, a very good friend of mine. Some military person came to me and demanded some answers by Friday. Well, I had already dedicated my computing resources to reducing data on the fly for a group of scientists; I was knee deep in short, small, important problems. This military person wanted me to solve his problem by the end of the day on Friday. I said, \"No, I'll give it to you Monday. I can work on it over the weekend. I'm not going to do it now.\" He goes down to my boss, Schelkunoff, and Schelkunoff says, \"You must run this for him; he's got to have it by Friday.\" I tell him, \"Why do I?\" He says, \"You have to.\" I said, \"Fine, Sergei, but you're sitting in your office Friday afternoon catching the late bus home to watch as this fellow walks out that door.\" I gave the military person the answers late Friday afternoon. I then went to Schelkunoff's office and sat down; as the man goes out I say, \"You see Schelkunoff, this fellow has nothing under his arm; but I gave him the answers.\" On Monday morning Schelkunoff called him up and said, \"Did you come in to work over the weekend?\" I could hear, as it were, a pause as the fellow ran through his mind of what was going to happen; but he knew he would have had to sign in, and he'd better not say he had when he hadn't, so he said he hadn't. Ever after that Schelkunoff said, \"You set your deadlines; you can change them.\"\n",
    "\n",
    "One lesson was sufficient to educate my boss as to why I didn't want to do big jobs that displaced exploratory research and why I was justified in not doing crash jobs which absorb all the research computing facilities. I wanted instead to use the facilities to compute a large number of small problems. Again, in the early days, I was limited in computing capacity and it was clear, in my area, that a \"mathematician had no use for machines.\" But I needed more machine capacity. Every time I had to tell some scientist in some other area, \"No I can't; I haven't the machine capacity,\" he complained. I said \"Go tell your Vice President that Hamming needs more computing capacity.\" After a while I could see what was happening up there at the top; many people said to my Vice President, \"Your man needs more computing capacity.\" I got it!\n",
    "\n",
    "I also did a second thing. When I loaned what little programming power we had to help in the early days of computing, I said, \"We are not getting the recognition for our programmers that they deserve. When you publish a paper you will thank that programmer or you aren't getting any more help from me. That programmer is going to be thanked by name; she's worked hard.\" I waited a couple of years. I then went through a year of BSTJ articles and counted what fraction thanked some programmer. I took it into the boss and said, \"That's the central role computing is playing in Bell Labs; if the BSTJ is important, that's how important computing is.\" He had to give in. You can educate your bosses. It's a hard job. In this talk I'm only viewing from the bottom up; I'm not viewing from the top down. But I am telling you how you can get what you want in spite of top management. You have to sell your ideas there also.\n",
    "\n",
    "Well I now come down to the topic, \"Is the effort to be a great scientist worth it?\" To answer this, you must ask people. When you get beyond their modesty, most people will say, \"Yes, doing really first-class work, and knowing it, is as good as wine, women and song put together,\" or if it's a woman she says, \"It is as good as wine, men and song put together.\" And if you look at the bosses, they tend to come back or ask for reports, trying to participate in those moments of discovery. They're always in the way. So evidently those who have done it, want to do it again. But it is a limited survey. I have never dared to go out and ask those who didn't do great work how they felt about the matter. It's a biased sample, but I still think it is worth the struggle. I think it is very definitely worth the struggle to try and do first-class work because the truth is, the value is in the struggle more than it is in the result. The struggle to make something of yourself seems to be worthwhile in itself. The success and fame are sort of dividends, in my opinion.\n",
    "\n",
    "I've told you how to do it. It is so easy, so why do so many people, with all their talents, fail? For example, my opinion, to this day, is that there are in the mathematics department at Bell Labs quite a few people far more able and far better endowed than I, but they didn't produce as much. Some of them did produce more than I did; Shannon produced more than I did, and some others produced a lot, but I was highly productive against a lot of other fellows who were better equipped. Why is it so? What happened to them? Why do so many of the people who have great promise, fail?\n",
    "\n",
    "Well, one of the reasons is drive and commitment. The people who do great work with less ability but who are committed to it, get more done that those who have great skill and dabble in it, who work during the day and go home and do other things and come back and work the next day. They don't have the deep commitment that is apparently necessary for really first-class work. They turn out lots of good work, but we were talking, remember, about first-class work. There is a difference. Good people, very talented people, almost always turn out good work. We're talking about the outstanding work, the type of work that gets the Nobel Prize and gets recognition.\n",
    "\n",
    "The second thing is, I think, the problem of personality defects. Now I'll cite a fellow whom I met out in Irvine. He had been the head of a computing center and he was temporarily on assignment as a special assistant to the president of the university. It was obvious he had a job with a great future. He took me into his office one time and showed me his method of getting letters done and how he took care of his correspondence. He pointed out how inefficient the secretary was. He kept all his letters stacked around there; he knew where everything was. And he would, on his word processor, get the letter out. He was bragging how marvelous it was and how he could get so much more work done without the secretary's interference. Well, behind his back, I talked to the secretary. The secretary said, \"Of course I can't help him; I don't get his mail. He won't give me the stuff to log in; I don't know where he puts it on the floor. Of course I can't help him.\" So I went to him and said, \"Look, if you adopt the present method and do what you can do single-handedly, you can go just that far and no farther than you can do single-handedly. If you will learn to work with the system, you can go as far as the system will support you.\" And, he never went any further. He had his personality defect of wanting total control and was not willing to recognize that you need the support of the system.\n",
    "\n",
    "You find this happening again and again; good scientists will fight the system rather than learn to work with the system and take advantage of all the system has to offer. It has a lot, if you learn how to use it. It takes patience, but you can learn how to use the system pretty well, and you can learn how to get around it. After all, if you want a decision `No', you just go to your boss and get a `No' easy. If you want to do something, don't ask, do it. Present him with an accomplished fact. Don't give him a chance to tell you `No'. But if you want a `No', it's easy to get a `No'.\n",
    "\n",
    "Another personality defect is ego assertion and I'll speak in this case of my own experience. I came from Los Alamos and in the early days I was using a machine in New York at 590 Madison Avenue where we merely rented time. I was still dressing in western clothes, big slash pockets, a bolo and all those things. I vaguely noticed that I was not getting as good service as other people. So I set out to measure. You came in and you waited for your turn; I felt I was not getting a fair deal. I said to myself, \"Why? No Vice President at IBM said, `Give Hamming a bad time'. It is the secretaries at the bottom who are doing this. When a slot appears, they'll rush to find someone to slip in, but they go out and find somebody else. Now, why? I haven't mistreated them.\" Answer: I wasn't dressing the way they felt somebody in that situation should. It came down to just that â€” I wasn't dressing properly. I had to make the decision â€” was I going to assert my ego and dress the way I wanted to and have it steadily drain my effort from my professional life, or was I going to appear to conform better? I decided I would make an effort to appear to conform properly. The moment I did, I got much better service. And now, as an old colorful character, I get better service than other people.\n",
    "\n",
    "You should dress according to the expectations of the audience spoken to. If I am going to give an address at the MIT computer center, I dress with a bolo and an old corduroy jacket or something else. I know enough not to let my clothes, my appearance, my manners get in the way of what I care about. An enormous number of scientists feel they must assert their ego and do their thing their way. They have got to be able to do this, that, or the other thing, and they pay a steady price.\n",
    "\n",
    "John Tukey almost always dressed very casually. He would go into an important office and it would take a long time before the other fellow realized that this is a first-class man and he had better listen. For a long time John has had to overcome this kind of hostility. It's wasted effort! I didn't say you should conform; I said \"The appearance of conforming gets you a long way.\" If you chose to assert your ego in any number of ways, \"I am going to do it my way,\" you pay a small steady price throughout the whole of your professional career. And this, over a whole lifetime, adds up to an enormous amount of needless trouble.\n",
    "\n",
    "By taking the trouble to tell jokes to the secretaries and being a little friendly, I got superb secretarial help. For instance, one time for some idiot reason all the reproducing services at Murray Hill were tied up. Don't ask me how, but they were. I wanted something done. My secretary called up somebody at Holmdel, hopped the company car, made the hour-long trip down and got it reproduced, and then came back. It was a payoff for the times I had made an effort to cheer her up, tell her jokes and be friendly; it was that little extra work that later paid off for me. By realizing you have to use the system and studying how to get the system to do your work, you learn how to adapt the system to your desires. Or you can fight it steadily, as a small undeclared war, for the whole of your life.\n",
    "\n",
    "And I think John Tukey paid a terrible price needlessly. He was a genius anyhow, but I think it would have been far better, and far simpler, had he been willing to conform a little bit instead of ego asserting. He is going to dress the way he wants all of the time. It applies not only to dress but to a thousand other things; people will continue to fight the system. Not that you shouldn't occasionally!\n",
    "\n",
    "When they moved the library from the middle of Murray Hill to the far end, a friend of mine put in a request for a bicycle. Well, the organization was not dumb. They waited awhile and sent back a map of the grounds saying, \"Will you please indicate on this map what paths you are going to take so we can get an insurance policy covering you.\" A few more weeks went by. They then asked, \"Where are you going to store the bicycle and how will it be locked so we can do so and so.\" He finally realized that of course he was going to be red-taped to death so he gave in. He rose to be the President of Bell Laboratories.\n",
    "\n",
    "Barney Oliver was a good man. He wrote a letter one time to the IEEE. At that time the official shelf space at Bell Labs was so much and the height of the IEEE Proceedings at that time was larger; and since you couldn't change the size of the official shelf space he wrote this letter to the IEEE Publication person saying, since so many IEEE members were at Bell Labs and since the official space was so high the journal size should be changed. He sent it for his boss's signature. Back came a carbon with his signature, but he still doesn't know whether the original was sent or not. I am not saying you shouldn't make gestures of reform. I am saying that my study of able people is that they don't get themselves committed to that kind of warfare. They play it a little bit and drop it and get on with their work.\n",
    "\n",
    "Many a second-rate fellow gets caught up in some little twitting of the system, and carries it through to warfare. He expends his energy in a foolish project. Now you are going to tell me that somebody has to change the system. I agree; somebody's has to. Which do you want to be? The person who changes the system or the person who does first-class science? Which person is it that you want to be? Be clear, when you fight the system and struggle with it, what you are doing, how far to go out of amusement, and how much to waste your effort fighting the system. My advice is to let somebody else do it and you get on with becoming a first-class scientist. Very few of you have the ability to both reform the system and become a first-class scientist.\n",
    "\n",
    "On the other hand, we can't always give in. There are times when a certain amount of rebellion is sensible. I have observed almost all scientists enjoy a certain amount of twitting the system for the sheer love of it. What it comes down to basically is that you cannot be original in one area without having originality in others. Originality is being different. You can't be an original scientist without having some other original characteristics. But many a scientist has let his quirks in other places make him pay a far higher price than is necessary for the ego satisfaction he or she gets. I'm not against all ego assertion; I'm against some.\n",
    "\n",
    "Another fault is anger. Often a scientist becomes angry, and this is no way to handle things. Amusement, yes, anger, no. Anger is misdirected. You should follow and cooperate rather than struggle against the system all the time.\n",
    "\n",
    "Another thing you should look for is the positive side of things instead of the negative. I have already given you several examples, and there are many, many more; how, given the situation, by changing the way I looked at it, I converted what was apparently a defect to an asset. I'll give you another example. I am an egotistical person; there is no doubt about it. I knew that most people who took a sabbatical to write a book, didn't finish it on time. So before I left, I told all my friends that when I come back, that book was going to be done! Yes, I would have it done â€” I'd have been ashamed to come back without it! I used my ego to make myself behave the way I wanted to. I bragged about something so I'd have to perform. I found out many times, like a cornered rat in a real trap, I was surprisingly capable. I have found that it paid to say, ``Oh yes, I'll get the answer for you Tuesday,'' not having any idea how to do it. By Sunday night I was really hard thinking on how I was going to deliver by Tuesday. I often put my pride on the line and sometimes I failed, but as I said, like a cornered rat I'm surprised how often I did a good job. I think you need to learn to use yourself. I think you need to know how to convert a situation from one view to another which would increase the chance of success.\n",
    "\n",
    "Now self-delusion in humans is very, very common. There are innumerable ways of you changing a thing and kidding yourself and making it look some other way. When you ask, \"Why didn't you do such and such,\" the person has a thousand alibis. If you look at the history of science, usually these days there are ten people right there ready, and we pay off for the person who is there first. The other nine fellows say, \"Well, I had the idea but I didn't do it and so on and so on.\" There are so many alibis. Why weren't you first? Why didn't you do it right? Don't try an alibi. Don't try and kid yourself. You can tell other people all the alibis you want. I don't mind. But to yourself try to be honest.\n",
    "\n",
    "If you really want to be a first-class scientist you need to know yourself, your weaknesses, your strengths, and your bad faults, like my egotism. How can you convert a fault to an asset? How can you convert a situation where you haven't got enough manpower to move into a direction when that's exactly what you need to do? I say again that I have seen, as I studied the history, the successful scientist changed the viewpoint and what was a defect became an asset.\n",
    "\n",
    "In summary, I claim that some of the reasons why so many people who have greatness within their grasp don't succeed are: they don't work on important problems, they don't become emotionally involved, they don't try and change what is difficult to some other situation which is easily done but is still important, and they keep giving themselves alibis why they don't. They keep saying that it is a matter of luck. I've told you how easy it is; furthermore I've told you how to reform. Therefore, go forth and become great scientists!\n",
    "\n",
    "\n",
    "\n",
    "Questions and Answers\n",
    "\n",
    "A. G. Chynoweth: Well that was 50 minutes of concentrated wisdom and observations accumulated over a fantastic career; I lost track of all the observations that were striking home. Some of them are very very timely. One was the plea for more computer capacity; I was hearing nothing but that this morning from several people, over and over again. So that was right on the mark today even though here we are 20 â€“ 30 years after when you were making similar remarks, Dick. I can think of all sorts of lessons that all of us can draw from your talk. And for one, as I walk around the halls in the future I hope I won't see as many closed doors in Bellcore. That was one observation I thought was very intriguing.\n",
    "\n",
    "Thank you very, very much indeed Dick; that was a wonderful recollection. I'll now open it up for questions. I'm sure there are many people who would like to take up on some of the points that Dick was making.\n",
    "\n",
    "Hamming: First let me respond to Alan Chynoweth about computing. I had computing in research and for 10 years I kept telling my management, ``Get that !&@#% machine out of research. We are being forced to run problems all the time. We can't do research because were too busy operating and running the computing machines.'' Finally the message got through. They were going to move computing out of research to someplace else. I was persona non grata to say the least and I was surprised that people didn't kick my shins because everybody was having their toy taken away from them. I went in to Ed David's office and said, ``Look Ed, you've got to give your researchers a machine. If you give them a great big machine, we'll be back in the same trouble we were before, so busy keeping it going we can't think. Give them the smallest machine you can because they are very able people. They will learn how to do things on a small machine instead of mass computing.'' As far as I'm concerned, that's how UNIX arose. We gave them a moderately small machine and they decided to make it do great things. They had to come up with a system to do it on. It is called UNIX!\n",
    "\n",
    "A. G. Chynoweth: I just have to pick up on that one. In our present environment, Dick, while we wrestle with some of the red tape attributed to, or required by, the regulators, there is one quote that one exasperated AVP came up with and I've used it over and over again. He growled that, \"UNIX was never a deliverable!\"\n",
    "\n",
    "Question: What about personal stress? Does that seem to make a difference?\n",
    "\n",
    "Hamming: Yes, it does. If you don't get emotionally involved, it doesn't. I had incipient ulcers most of the years that I was at Bell Labs. I have since gone off to the Naval Postgraduate School and laid back somewhat, and now my health is much better. But if you want to be a great scientist you're going to have to put up with stress. You can lead a nice life; you can be a nice guy or you can be a great scientist. But nice guys end last, is what Leo Durocher said. If you want to lead a nice happy life with a lot of recreation and everything else, you'll lead a nice life.\n",
    "\n",
    "Question: The remarks about having courage, no one could argue with; but those of us who have gray hairs or who are well established don't have to worry too much. But what I sense among the young people these days is a real concern over the risk taking in a highly competitive environment. Do you have any words of wisdom on this?\n",
    "\n",
    "Hamming: I'll quote Ed David more. Ed David was concerned about the general loss of nerve in our society. It does seem to me that we've gone through various periods. Coming out of the war, coming out of Los Alamos where we built the bomb, coming out of building the radars and so on, there came into the mathematics department, and the research area, a group of people with a lot of guts. They've just seen things done; they've just won a war which was fantastic. We had reasons for having courage and therefore we did a great deal. I can't arrange that situation to do it again. I cannot blame the present generation for not having it, but I agree with what you say; I just cannot attach blame to it. It doesn't seem to me they have the desire for greatness; they lack the courage to do it. But we had, because we were in a favorable circumstance to have it; we just came through a tremendously successful war. In the war we were looking very, very bad for a long while; it was a very desperate struggle as you well know. And our success, I think, gave us courage and self confidence; that's why you see, beginning in the late forties through the fifties, a tremendous productivity at the labs which was stimulated from the earlier times. Because many of us were earlier forced to learn other things â€” we were forced to learn the things we didn't want to learn, we were forced to have an open door â€” and then we could exploit those things we learned. It is true, and I can't do anything about it; I cannot blame the present generation either. It's just a fact.\n",
    "\n",
    "Question: Is there something management could or should do?\n",
    "\n",
    "Hamming: Management can do very little. If you want to talk about managing research, that's a totally different talk. I'd take another hour doing that. This talk is about how the individual gets very successful research done in spite of anything the management does or in spite of any other opposition. And how do you do it? Just as I observe people doing it. It's just that simple and that hard!\n",
    "\n",
    "Question: Is brainstorming a daily process?\n",
    "\n",
    "Hamming: Once that was a very popular thing, but it seems not to have paid off. For myself I find it desirable to talk to other people; but a session of brainstorming is seldom worthwhile. I do go in to strictly talk to somebody and say, \"Look, I think there has to be something here. Here's what I think I see ...\" and then begin talking back and forth. But you want to pick capable people. To use another analogy, you know the idea called the `critical mass.' If you have enough stuff you have critical mass. There is also the idea I used to call `sound absorbers'. When you get too many sound absorbers, you give out an idea and they merely say, \"Yes, yes, yes.\" What you want to do is get that critical mass in action; \"Yes, that reminds me of so and so,\" or, \"Have you thought about that or this?\" When you talk to other people, you want to get rid of those sound absorbers who are nice people but merely say, \"Oh yes,\" and to find those who will stimulate you right back.\n",
    "\n",
    "For example, you couldn't talk to John Pierce without being stimulated very quickly. There were a group of other people I used to talk with. For example there was Ed Gilbert; I used to go down to his office regularly and ask him questions and listen and come back stimulated. I picked my people carefully with whom I did or whom I didn't brainstorm because the sound absorbers are a curse. They are just nice guys; they fill the whole space and they contribute nothing except they absorb ideas and the new ideas just die away instead of echoing on. Yes, I find it necessary to talk to people. I think people with closed doors fail to do this so they fail to get their ideas sharpened, such as \"Did you ever notice something over here?\" I never knew anything about it â€” I can go over and look. Somebody points the way. On my visit here, I have already found several books that I must read when I get home. I talk to people and ask questions when I think they can answer me and give me clues that I do not know about. I go out and look!\n",
    "\n",
    "Question: What kind of tradeoffs did you make in allocating your time for reading and writing and actually doing research?\n",
    "\n",
    "Hamming: I believed, in my early days, that you should spend at least as much time in the polish and presentation as you did in the original research. Now at least 50% of the time must go for the presentation. It's a big, big number.\n",
    "\n",
    "Question: How much effort should go into library work?\n",
    "\n",
    "Hamming: It depends upon the field. I will say this about it. There was a fellow at Bell Labs, a very, very, smart guy. He was always in the library; he read everything. If you wanted references, you went to him and he gave you all kinds of references. But in the middle of forming these theories, I formed a proposition: there would be no effect named after him in the long run. He is now retired from Bell Labs and is an Adjunct Professor. He was very valuable; I'm not questioning that. He wrote some very good Physical Review articles; but there's no effect named after him because he read too much. If you read all the time what other people have done you will think the way they thought. If you want to think new thoughts that are different, then do what a lot of creative people do â€” get the problem reasonably clear and then refuse to look at any answers until you've thought the problem through carefully how you would do it, how you could slightly change the problem to be the correct one. So yes, you need to keep up. You need to keep up more to find out what the problems are than to read to find the solutions. The reading is necessary to know what is going on and what is possible. But reading to get the solutions does not seem to be the way to do great research. So I'll give you two answers. You read; but it is not the amount, it is the way you read that counts.\n",
    "\n",
    "Question: How do you get your name attached to things?\n",
    "\n",
    "Hamming: By doing great work. I'll tell you the hamming window one. I had given Tukey a hard time, quite a few times, and I got a phone call from him from Princeton to me at Murray Hill. I knew that he was writing up power spectra and he asked me if I would mind if he called a certain window a \"hamming window.\" And I said to him, \"Come on, John; you know perfectly well I did only a small part of the work but you also did a lot.\" He said, \"Yes, Hamming, but you contributed a lot of small things; you're entitled to some credit.\" So he called it the hamming window. Now, let me go on. I had twitted John frequently about true greatness. I said true greatness is when your name is like ampere, watt, and fourier â€” when it's spelled with a lower case letter. That's how the hamming window came about.\n",
    "\n",
    "Question: Dick, would you care to comment on the relative effectiveness between giving talks, writing papers, and writing books?\n",
    "\n",
    "Hamming: In the short-haul, papers are very important if you want to stimulate someone tomorrow. If you want to get recognition long-haul, it seems to me writing books is more contribution because most of us need orientation. In this day of practically infinite knowledge, we need orientation to find our way. Let me tell you what infinite knowledge is. Since from the time of Newton to now, we have come close to doubling knowledge every 17 years, more or less. And we cope with that, essentially, by specialization. In the next 340 years at that rate, there will be 20 doublings, i.e. a million, and there will be a million fields of specialty for every one field now. It isn't going to happen. The present growth of knowledge will choke itself off until we get different tools. I believe that books which try to digest, coordinate, get rid of the duplication, get rid of the less fruitful methods and present the underlying ideas clearly of what we know now, will be the things the future generations will value. Public talks are necessary; private talks are necessary; written papers are necessary. But I am inclined to believe that, in the long-haul, books which leave out what's not essential are more important than books which tell you everything because you don't want to know everything. I don't want to know that much about penguins is the usual reply. You just want to know the essence.\n",
    "\n",
    "Question: You mentioned the problem of the Nobel Prize and the subsequent notoriety of what was done to some of the careers. Isn't that kind of a much more broad problem of fame? What can one do?\n",
    "\n",
    "Hamming: Some things you could do are the following. Somewhere around every seven years make a significant, if not complete, shift in your field. Thus, I shifted from numerical analysis, to hardware, to software, and so on, periodically, because you tend to use up your ideas. When you go to a new field, you have to start over as a baby. You are no longer the big mukity muk and you can start back there and you can start planting those acorns which will become the giant oaks. Shannon, I believe, ruined himself. In fact when he left Bell Labs, I said, \"That's the end of Shannon's scientific career.\" I received a lot of flak from my friends who said that Shannon was just as smart as ever. I said, \"Yes, he'll be just as smart, but that's the end of his scientific career,\" and I truly believe it was.\n",
    "\n",
    "You have to change. You get tired after a while; you use up your originality in one field. You need to get something nearby. I'm not saying that you shift from music to theoretical physics to English literature; I mean within your field you should shift areas so that you don't go stale. You couldn't get away with forcing a change every seven years, but if you could, I would require a condition for doing research, being that you will change your field of research every seven years with a reasonable definition of what it means, or at the end of 10 years, management has the right to compel you to change. I would insist on a change because I'm serious. What happens to the old fellows is that they get a technique going; they keep on using it. They were marching in that direction which was right then, but the world changes. There's the new direction; but the old fellows are still marching in their former direction.\n",
    "\n",
    "You need to get into a new field to get new viewpoints, and before you use up all the old ones. You can do something about this, but it takes effort and energy. It takes courage to say, ``Yes, I will give up my great reputation.'' For example, when error correcting codes were well launched, having these theories, I said, \"Hamming, you are going to quit reading papers in the field; you are going to ignore it completely; you are going to try and do something else other than coast on that.\" I deliberately refused to go on in that field. I wouldn't even read papers to try to force myself to have a chance to do something else. I managed myself, which is what I'm preaching in this whole talk. Knowing many of my own faults, I manage myself. I have a lot of faults, so I've got a lot of problems, i.e. a lot of possibilities of management.\n",
    "\n",
    "Question: Would you compare research and management?\n",
    "\n",
    "Hamming: If you want to be a great researcher, you won't make it being president of the company. If you want to be president of the company, that's another thing. I'm not against being president of the company. I just don't want to be. I think Ian Ross does a good job as President of Bell Labs. I'm not against it; but you have to be clear on what you want. Furthermore, when you're young, you may have picked wanting to be a great scientist, but as you live longer, you may change your mind. For instance, I went to my boss, Bode, one day and said, \"Why did you ever become department head? Why didn't you just be a good scientist?\" He said, \"Hamming, I had a vision of what mathematics should be in Bell Laboratories. And I saw if that vision was going to be realized, I had to make it happen; I had to be department head.\" When your vision of what you want to do is what you can do single-handedly, then you should pursue it. The day your vision, what you think needs to be done, is bigger than what you can do single-handedly, then you have to move toward management. And the bigger the vision is, the farther in management you have to go. If you have a vision of what the whole laboratory should be, or the whole Bell System, you have to get there to make it happen. You can't make it happen from the bottom very easily. It depends upon what goals and what desires you have. And as they change in life, you have to be prepared to change. I chose to avoid management because I preferred to do what I could do single-handedly. But that's the choice that I made, and it is biased. Each person is entitled to their choice. Keep an open mind. But when you do choose a path, for heaven's sake be aware of what you have done and the choice you have made. Don't try to do both sides.\n",
    "\n",
    "Question: How important is one's own expectation or how important is it to be in a group or surrounded by people who expect great work from you?\n",
    "\n",
    "Hamming: At Bell Labs everyone expected good work from me â€” it was a big help. Everybody expects you to do a good job, so you do, if you've got pride. I think it's very valuable to have first-class people around. I sought out the best people. The moment that physics table lost the best people, I left. The moment I saw that the same was true of the chemistry table, I left. I tried to go with people who had great ability so I could learn from them and who would expect great results out of me. By deliberately managing myself, I think I did much better than laissez faire.\n",
    "\n",
    "Question: You, at the outset of your talk, minimized or played down luck; but you seemed also to gloss over the circumstances that got you to Los Alamos, that got you to Chicago, that got you to Bell Laboratories.\n",
    "\n",
    "Hamming: There was some luck. On the other hand I don't know the alternate branches. Until you can say that the other branches would not have been equally or more successful, I can't say. Is it luck the particular thing you do? For example, when I met Feynman at Los Alamos, I knew he was going to get a Nobel Prize. I didn't know what for. But I knew darn well he was going to do great work. No matter what directions came up in the future, this man would do great work. And sure enough, he did do great work. It isn't that you only do a little great work at this circumstance and that was luck, there are many opportunities sooner or later. There are a whole pail full of opportunities, of which, if you're in this situation, you seize one and you're great over there instead of over here. There is an element of luck, yes and no. Luck favors a prepared mind; luck favors a prepared person. It is not guaranteed; I don't guarantee success as being absolutely certain. I'd say luck changes the odds, but there is some definite control on the part of the individual.\n",
    "\n",
    "Go forth, then, and do great work!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "pg = \"\"\"If you collected lists of techniques for doing great work in a lot of different fields, what would the intersection look like? I decided to find out by making it.\n",
    "\n",
    "Partly my goal was to create a guide that could be used by someone working in any field. But I was also curious about the shape of the intersection. And one thing this exercise shows is that it does have a definite shape; it's not just a point labelled \"work hard.\"\n",
    "\n",
    "The following recipe assumes you're very ambitious.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The first step is to decide what to work on. The work you choose needs to have three qualities: it has to be something you have a natural aptitude for, that you have a deep interest in, and that offers scope to do great work.\n",
    "\n",
    "In practice you don't have to worry much about the third criterion. Ambitious people are if anything already too conservative about it. So all you need to do is find something you have an aptitude for and great interest in. [1]\n",
    "\n",
    "That sounds straightforward, but it's often quite difficult. When you're young you don't know what you're good at or what different kinds of work are like. Some kinds of work you end up doing may not even exist yet. So while some people know what they want to do at 14, most have to figure it out.\n",
    "\n",
    "The way to figure out what to work on is by working. If you're not sure what to work on, guess. But pick something and get going. You'll probably guess wrong some of the time, but that's fine. It's good to know about multiple things; some of the biggest discoveries come from noticing connections between different fields.\n",
    "\n",
    "Develop a habit of working on your own projects. Don't let \"work\" mean something other people tell you to do. If you do manage to do great work one day, it will probably be on a project of your own. It may be within some bigger project, but you'll be driving your part of it.\n",
    "\n",
    "What should your projects be? Whatever seems to you excitingly ambitious. As you grow older and your taste in projects evolves, exciting and important will converge. At 7 it may seem excitingly ambitious to build huge things out of Lego, then at 14 to teach yourself calculus, till at 21 you're starting to explore unanswered questions in physics. But always preserve excitingness.\n",
    "\n",
    "There's a kind of excited curiosity that's both the engine and the rudder of great work. It will not only drive you, but if you let it have its way, will also show you what to work on.\n",
    "\n",
    "What are you excessively curious about â€” curious to a degree that would bore most other people? That's what you're looking for.\n",
    "\n",
    "Once you've found something you're excessively interested in, the next step is to learn enough about it to get you to one of the frontiers of knowledge. Knowledge expands fractally, and from a distance its edges look smooth, but once you learn enough to get close to one, they turn out to be full of gaps.\n",
    "\n",
    "The next step is to notice them. This takes some skill, because your brain wants to ignore such gaps in order to make a simpler model of the world. Many discoveries have come from asking questions about things that everyone else took for granted. [2]\n",
    "\n",
    "If the answers seem strange, so much the better. Great work often has a tincture of strangeness. You see this from painting to math. It would be affected to try to manufacture it, but if it appears, embrace it.\n",
    "\n",
    "Boldly chase outlier ideas, even if other people aren't interested in them â€” in fact, especially if they aren't. If you're excited about some possibility that everyone else ignores, and you have enough expertise to say precisely what they're all overlooking, that's as good a bet as you'll find. [3]\n",
    "\n",
    "Four steps: choose a field, learn enough to get to the frontier, notice gaps, explore promising ones. This is how practically everyone who's done great work has done it, from painters to physicists.\n",
    "\n",
    "Steps two and four will require hard work. It may not be possible to prove that you have to work hard to do great things, but the empirical evidence is on the scale of the evidence for mortality. That's why it's essential to work on something you're deeply interested in. Interest will drive you to work harder than mere diligence ever could.\n",
    "\n",
    "The three most powerful motives are curiosity, delight, and the desire to do something impressive. Sometimes they converge, and that combination is the most powerful of all.\n",
    "\n",
    "The big prize is to discover a new fractal bud. You notice a crack in the surface of knowledge, pry it open, and there's a whole world inside.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's talk a little more about the complicated business of figuring out what to work on. The main reason it's hard is that you can't tell what most kinds of work are like except by doing them. Which means the four steps overlap: you may have to work at something for years before you know how much you like it or how good you are at it. And in the meantime you're not doing, and thus not learning about, most other kinds of work. So in the worst case you choose late based on very incomplete information. [4]\n",
    "\n",
    "The nature of ambition exacerbates this problem. Ambition comes in two forms, one that precedes interest in the subject and one that grows out of it. Most people who do great work have a mix, and the more you have of the former, the harder it will be to decide what to do.\n",
    "\n",
    "The educational systems in most countries pretend it's easy. They expect you to commit to a field long before you could know what it's really like. And as a result an ambitious person on an optimal trajectory will often read to the system as an instance of breakage.\n",
    "\n",
    "It would be better if they at least admitted it â€” if they admitted that the system not only can't do much to help you figure out what to work on, but is designed on the assumption that you'll somehow magically guess as a teenager. They don't tell you, but I will: when it comes to figuring out what to work on, you're on your own. Some people get lucky and do guess correctly, but the rest will find themselves scrambling diagonally across tracks laid down on the assumption that everyone does.\n",
    "\n",
    "What should you do if you're young and ambitious but don't know what to work on? What you should not do is drift along passively, assuming the problem will solve itself. You need to take action. But there is no systematic procedure you can follow. When you read biographies of people who've done great work, it's remarkable how much luck is involved. They discover what to work on as a result of a chance meeting, or by reading a book they happen to pick up. So you need to make yourself a big target for luck, and the way to do that is to be curious. Try lots of things, meet lots of people, read lots of books, ask lots of questions. [5]\n",
    "\n",
    "When in doubt, optimize for interestingness. Fields change as you learn more about them. What mathematicians do, for example, is very different from what you do in high school math classes. So you need to give different types of work a chance to show you what they're like. But a field should become increasingly interesting as you learn more about it. If it doesn't, it's probably not for you.\n",
    "\n",
    "Don't worry if you find you're interested in different things than other people. The stranger your tastes in interestingness, the better. Strange tastes are often strong ones, and a strong taste for work means you'll be productive. And you're more likely to find new things if you're looking where few have looked before.\n",
    "\n",
    "One sign that you're suited for some kind of work is when you like even the parts that other people find tedious or frightening.\n",
    "\n",
    "But fields aren't people; you don't owe them any loyalty. If in the course of working on one thing you discover another that's more exciting, don't be afraid to switch.\n",
    "\n",
    "If you're making something for people, make sure it's something they actually want. The best way to do this is to make something you yourself want. Write the story you want to read; build the tool you want to use. Since your friends probably have similar interests, this will also get you your initial audience.\n",
    "\n",
    "This should follow from the excitingness rule. Obviously the most exciting story to write will be the one you want to read. The reason I mention this case explicitly is that so many people get it wrong. Instead of making what they want, they try to make what some imaginary, more sophisticated audience wants. And once you go down that route, you're lost. [6]\n",
    "\n",
    "There are a lot of forces that will lead you astray when you're trying to figure out what to work on. Pretentiousness, fashion, fear, money, politics, other people's wishes, eminent frauds. But if you stick to what you find genuinely interesting, you'll be proof against all of them. If you're interested, you're not astray.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Following your interests may sound like a rather passive strategy, but in practice it usually means following them past all sorts of obstacles. You usually have to risk rejection and failure. So it does take a good deal of boldness.\n",
    "\n",
    "But while you need boldness, you don't usually need much planning. In most cases the recipe for doing great work is simply: work hard on excitingly ambitious projects, and something good will come of it. Instead of making a plan and then executing it, you just try to preserve certain invariants.\n",
    "\n",
    "The trouble with planning is that it only works for achievements you can describe in advance. You can win a gold medal or get rich by deciding to as a child and then tenaciously pursuing that goal, but you can't discover natural selection that way.\n",
    "\n",
    "I think for most people who want to do great work, the right strategy is not to plan too much. At each stage do whatever seems most interesting and gives you the best options for the future. I call this approach \"staying upwind.\" This is how most people who've done great work seem to have done it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Even when you've found something exciting to work on, working on it is not always straightforward. There will be times when some new idea makes you leap out of bed in the morning and get straight to work. But there will also be plenty of times when things aren't like that.\n",
    "\n",
    "You don't just put out your sail and get blown forward by inspiration. There are headwinds and currents and hidden shoals. So there's a technique to working, just as there is to sailing.\n",
    "\n",
    "For example, while you must work hard, it's possible to work too hard, and if you do that you'll find you get diminishing returns: fatigue will make you stupid, and eventually even damage your health. The point at which work yields diminishing returns depends on the type. Some of the hardest types you might only be able to do for four or five hours a day.\n",
    "\n",
    "Ideally those hours will be contiguous. To the extent you can, try to arrange your life so you have big blocks of time to work in. You'll shy away from hard tasks if you know you might be interrupted.\n",
    "\n",
    "It will probably be harder to start working than to keep working. You'll often have to trick yourself to get over that initial threshold. Don't worry about this; it's the nature of work, not a flaw in your character. Work has a sort of activation energy, both per day and per project. And since this threshold is fake in the sense that it's higher than the energy required to keep going, it's ok to tell yourself a lie of corresponding magnitude to get over it.\n",
    "\n",
    "It's usually a mistake to lie to yourself if you want to do great work, but this is one of the rare cases where it isn't. When I'm reluctant to start work in the morning, I often trick myself by saying \"I'll just read over what I've got so far.\" Five minutes later I've found something that seems mistaken or incomplete, and I'm off.\n",
    "\n",
    "Similar techniques work for starting new projects. It's ok to lie to yourself about how much work a project will entail, for example. Lots of great things began with someone saying \"How hard could it be?\"\n",
    "\n",
    "This is one case where the young have an advantage. They're more optimistic, and even though one of the sources of their optimism is ignorance, in this case ignorance can sometimes beat knowledge.\n",
    "\n",
    "Try to finish what you start, though, even if it turns out to be more work than you expected. Finishing things is not just an exercise in tidiness or self-discipline. In many projects a lot of the best work happens in what was meant to be the final stage.\n",
    "\n",
    "Another permissible lie is to exaggerate the importance of what you're working on, at least in your own mind. If that helps you discover something new, it may turn out not to have been a lie after all. [7]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Since there are two senses of starting work â€” per day and per project â€” there are also two forms of procrastination. Per-project procrastination is far the more dangerous. You put off starting that ambitious project from year to year because the time isn't quite right. When you're procrastinating in units of years, you can get a lot not done. [8]\n",
    "\n",
    "One reason per-project procrastination is so dangerous is that it usually camouflages itself as work. You're not just sitting around doing nothing; you're working industriously on something else. So per-project procrastination doesn't set off the alarms that per-day procrastination does. You're too busy to notice it.\n",
    "\n",
    "The way to beat it is to stop occasionally and ask yourself: Am I working on what I most want to work on? When you're young it's ok if the answer is sometimes no, but this gets increasingly dangerous as you get older. [9]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Great work usually entails spending what would seem to most people an unreasonable amount of time on a problem. You can't think of this time as a cost, or it will seem too high. You have to find the work sufficiently engaging as it's happening.\n",
    "\n",
    "There may be some jobs where you have to work diligently for years at things you hate before you get to the good part, but this is not how great work happens. Great work happens by focusing consistently on something you're genuinely interested in. When you pause to take stock, you're surprised how far you've come.\n",
    "\n",
    "The reason we're surprised is that we underestimate the cumulative effect of work. Writing a page a day doesn't sound like much, but if you do it every day you'll write a book a year. That's the key: consistency. People who do great things don't get a lot done every day. They get something done, rather than nothing.\n",
    "\n",
    "If you do work that compounds, you'll get exponential growth. Most people who do this do it unconsciously, but it's worth stopping to think about. Learning, for example, is an instance of this phenomenon: the more you learn about something, the easier it is to learn more. Growing an audience is another: the more fans you have, the more new fans they'll bring you.\n",
    "\n",
    "The trouble with exponential growth is that the curve feels flat in the beginning. It isn't; it's still a wonderful exponential curve. But we can't grasp that intuitively, so we underrate exponential growth in its early stages.\n",
    "\n",
    "Something that grows exponentially can become so valuable that it's worth making an extraordinary effort to get it started. But since we underrate exponential growth early on, this too is mostly done unconsciously: people push through the initial, unrewarding phase of learning something new because they know from experience that learning new things always takes an initial push, or they grow their audience one fan at a time because they have nothing better to do. If people consciously realized they could invest in exponential growth, many more would do it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Work doesn't just happen when you're trying to. There's a kind of undirected thinking you do when walking or taking a shower or lying in bed that can be very powerful. By letting your mind wander a little, you'll often solve problems you were unable to solve by frontal attack.\n",
    "\n",
    "You have to be working hard in the normal way to benefit from this phenomenon, though. You can't just walk around daydreaming. The daydreaming has to be interleaved with deliberate work that feeds it questions. [10]\n",
    "\n",
    "Everyone knows to avoid distractions at work, but it's also important to avoid them in the other half of the cycle. When you let your mind wander, it wanders to whatever you care about most at that moment. So avoid the kind of distraction that pushes your work out of the top spot, or you'll waste this valuable type of thinking on the distraction instead. (Exception: Don't avoid love.)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Consciously cultivate your taste in the work done in your field. Until you know which is the best and what makes it so, you don't know what you're aiming for.\n",
    "\n",
    "And that is what you're aiming for, because if you don't try to be the best, you won't even be good. This observation has been made by so many people in so many different fields that it might be worth thinking about why it's true. It could be because ambition is a phenomenon where almost all the error is in one direction â€” where almost all the shells that miss the target miss by falling short. Or it could be because ambition to be the best is a qualitatively different thing from ambition to be good. Or maybe being good is simply too vague a standard. Probably all three are true. [11]\n",
    "\n",
    "Fortunately there's a kind of economy of scale here. Though it might seem like you'd be taking on a heavy burden by trying to be the best, in practice you often end up net ahead. It's exciting, and also strangely liberating. It simplifies things. In some ways it's easier to try to be the best than to try merely to be good.\n",
    "\n",
    "One way to aim high is to try to make something that people will care about in a hundred years. Not because their opinions matter more than your contemporaries', but because something that still seems good in a hundred years is more likely to be genuinely good.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Don't try to work in a distinctive style. Just try to do the best job you can; you won't be able to help doing it in a distinctive way.\n",
    "\n",
    "Style is doing things in a distinctive way without trying to. Trying to is affectation.\n",
    "\n",
    "Affectation is in effect to pretend that someone other than you is doing the work. You adopt an impressive but fake persona, and while you're pleased with the impressiveness, the fakeness is what shows in the work. [12]\n",
    "\n",
    "The temptation to be someone else is greatest for the young. They often feel like nobodies. But you never need to worry about that problem, because it's self-solving if you work on sufficiently ambitious projects. If you succeed at an ambitious project, you're not a nobody; you're the person who did it. So just do the work and your identity will take care of itself.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"Avoid affectation\" is a useful rule so far as it goes, but how would you express this idea positively? How would you say what to be, instead of what not to be? The best answer is earnest. If you're earnest you avoid not just affectation but a whole set of similar vices.\n",
    "\n",
    "The core of being earnest is being intellectually honest. We're taught as children to be honest as an unselfish virtue â€” as a kind of sacrifice. But in fact it's a source of power too. To see new ideas, you need an exceptionally sharp eye for the truth. You're trying to see more truth than others have seen so far. And how can you have a sharp eye for the truth if you're intellectually dishonest?\n",
    "\n",
    "One way to avoid intellectual dishonesty is to maintain a slight positive pressure in the opposite direction. Be aggressively willing to admit that you're mistaken. Once you've admitted you were mistaken about something, you're free. Till then you have to carry it. [13]\n",
    "\n",
    "Another more subtle component of earnestness is informality. Informality is much more important than its grammatically negative name implies. It's not merely the absence of something. It means focusing on what matters instead of what doesn't.\n",
    "\n",
    "What formality and affectation have in common is that as well as doing the work, you're trying to seem a certain way as you're doing it. But any energy that goes into how you seem comes out of being good. That's one reason nerds have an advantage in doing great work: they expend little effort on seeming anything. In fact that's basically the definition of a nerd.\n",
    "\n",
    "Nerds have a kind of innocent boldness that's exactly what you need in doing great work. It's not learned; it's preserved from childhood. So hold onto it. Be the one who puts things out there rather than the one who sits back and offers sophisticated-sounding criticisms of them. \"It's easy to criticize\" is true in the most literal sense, and the route to great work is never easy.\n",
    "\n",
    "There may be some jobs where it's an advantage to be cynical and pessimistic, but if you want to do great work it's an advantage to be optimistic, even though that means you'll risk looking like a fool sometimes. There's an old tradition of doing the opposite. The Old Testament says it's better to keep quiet lest you look like a fool. But that's advice for seeming smart. If you actually want to discover new things, it's better to take the risk of telling people your ideas.\n",
    "\n",
    "Some people are naturally earnest, and with others it takes a conscious effort. Either kind of earnestness will suffice. But I doubt it would be possible to do great work without being earnest. It's so hard to do even if you are. You don't have enough margin for error to accommodate the distortions introduced by being affected, intellectually dishonest, orthodox, fashionable, or cool. [14]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Great work is consistent not only with who did it, but with itself. It's usually all of a piece. So if you face a decision in the middle of working on something, ask which choice is more consistent.\n",
    "\n",
    "You may have to throw things away and redo them. You won't necessarily have to, but you have to be willing to. And that can take some effort; when there's something you need to redo, status quo bias and laziness will combine to keep you in denial about it. To beat this ask: If I'd already made the change, would I want to revert to what I have now?\n",
    "\n",
    "Have the confidence to cut. Don't keep something that doesn't fit just because you're proud of it, or because it cost you a lot of effort.\n",
    "\n",
    "Indeed, in some kinds of work it's good to strip whatever you're doing to its essence. The result will be more concentrated; you'll understand it better; and you won't be able to lie to yourself about whether there's anything real there.\n",
    "\n",
    "Mathematical elegance may sound like a mere metaphor, drawn from the arts. That's what I thought when I first heard the term \"elegant\" applied to a proof. But now I suspect it's conceptually prior â€” that the main ingredient in artistic elegance is mathematical elegance. At any rate it's a useful standard well beyond math.\n",
    "\n",
    "Elegance can be a long-term bet, though. Laborious solutions will often have more prestige in the short term. They cost a lot of effort and they're hard to understand, both of which impress people, at least temporarily.\n",
    "\n",
    "Whereas some of the very best work will seem like it took comparatively little effort, because it was in a sense already there. It didn't have to be built, just seen. It's a very good sign when it's hard to say whether you're creating something or discovering it.\n",
    "\n",
    "When you're doing work that could be seen as either creation or discovery, err on the side of discovery. Try thinking of yourself as a mere conduit through which the ideas take their natural shape.\n",
    "\n",
    "(Strangely enough, one exception is the problem of choosing a problem to work on. This is usually seen as search, but in the best case it's more like creating something. In the best case you create the field in the process of exploring it.)\n",
    "\n",
    "Similarly, if you're trying to build a powerful tool, make it gratuitously unrestrictive. A powerful tool almost by definition will be used in ways you didn't expect, so err on the side of eliminating restrictions, even if you don't know what the benefit will be.\n",
    "\n",
    "Great work will often be tool-like in the sense of being something others build on. So it's a good sign if you're creating ideas that others could use, or exposing questions that others could answer. The best ideas have implications in many different areas.\n",
    "\n",
    "If you express your ideas in the most general form, they'll be truer than you intended.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "True by itself is not enough, of course. Great ideas have to be true and new. And it takes a certain amount of ability to see new ideas even once you've learned enough to get to one of the frontiers of knowledge.\n",
    "\n",
    "In English we give this ability names like originality, creativity, and imagination. And it seems reasonable to give it a separate name, because it does seem to some extent a separate skill. It's possible to have a great deal of ability in other respects â€” to have a great deal of what's often called \"technical ability\" â€” and yet not have much of this.\n",
    "\n",
    "I've never liked the term \"creative process.\" It seems misleading. Originality isn't a process, but a habit of mind. Original thinkers throw off new ideas about whatever they focus on, like an angle grinder throwing off sparks. They can't help it.\n",
    "\n",
    "If the thing they're focused on is something they don't understand very well, these new ideas might not be good. One of the most original thinkers I know decided to focus on dating after he got divorced. He knew roughly as much about dating as the average 15 year old, and the results were spectacularly colorful. But to see originality separated from expertise like that made its nature all the more clear.\n",
    "\n",
    "I don't know if it's possible to cultivate originality, but there are definitely ways to make the most of however much you have. For example, you're much more likely to have original ideas when you're working on something. Original ideas don't come from trying to have original ideas. They come from trying to build or understand something slightly too difficult. [15]\n",
    "\n",
    "Talking or writing about the things you're interested in is a good way to generate new ideas. When you try to put ideas into words, a missing idea creates a sort of vacuum that draws it out of you. Indeed, there's a kind of thinking that can only be done by writing.\n",
    "\n",
    "Changing your context can help. If you visit a new place, you'll often find you have new ideas there. The journey itself often dislodges them. But you may not have to go far to get this benefit. Sometimes it's enough just to go for a walk. [16]\n",
    "\n",
    "It also helps to travel in topic space. You'll have more new ideas if you explore lots of different topics, partly because it gives the angle grinder more surface area to work on, and partly because analogies are an especially fruitful source of new ideas.\n",
    "\n",
    "Don't divide your attention evenly between many topics though, or you'll spread yourself too thin. You want to distribute it according to something more like a power law. [17] Be professionally curious about a few topics and idly curious about many more.\n",
    "\n",
    "Curiosity and originality are closely related. Curiosity feeds originality by giving it new things to work on. But the relationship is closer than that. Curiosity is itself a kind of originality; it's roughly to questions what originality is to answers. And since questions at their best are a big component of answers, curiosity at its best is a creative force.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Having new ideas is a strange game, because it usually consists of seeing things that were right under your nose. Once you've seen a new idea, it tends to seem obvious. Why did no one think of this before?\n",
    "\n",
    "When an idea seems simultaneously novel and obvious, it's probably a good one.\n",
    "\n",
    "Seeing something obvious sounds easy. And yet empirically having new ideas is hard. What's the source of this apparent contradiction? It's that seeing the new idea usually requires you to change the way you look at the world. We see the world through models that both help and constrain us. When you fix a broken model, new ideas become obvious. But noticing and fixing a broken model is hard. That's how new ideas can be both obvious and yet hard to discover: they're easy to see after you do something hard.\n",
    "\n",
    "One way to discover broken models is to be stricter than other people. Broken models of the world leave a trail of clues where they bash against reality. Most people don't want to see these clues. It would be an understatement to say that they're attached to their current model; it's what they think in; so they'll tend to ignore the trail of clues left by its breakage, however conspicuous it may seem in retrospect.\n",
    "\n",
    "To find new ideas you have to seize on signs of breakage instead of looking away. That's what Einstein did. He was able to see the wild implications of Maxwell's equations not so much because he was looking for new ideas as because he was stricter.\n",
    "\n",
    "The other thing you need is a willingness to break rules. Paradoxical as it sounds, if you want to fix your model of the world, it helps to be the sort of person who's comfortable breaking rules. From the point of view of the old model, which everyone including you initially shares, the new model usually breaks at least implicit rules.\n",
    "\n",
    "Few understand the degree of rule-breaking required, because new ideas seem much more conservative once they succeed. They seem perfectly reasonable once you're using the new model of the world they brought with them. But they didn't at the time; it took the greater part of a century for the heliocentric model to be generally accepted, even among astronomers, because it felt so wrong.\n",
    "\n",
    "Indeed, if you think about it, a good new idea has to seem bad to most people, or someone would have already explored it. So what you're looking for is ideas that seem crazy, but the right kind of crazy. How do you recognize these? You can't with certainty. Often ideas that seem bad are bad. But ideas that are the right kind of crazy tend to be exciting; they're rich in implications; whereas ideas that are merely bad tend to be depressing.\n",
    "\n",
    "There are two ways to be comfortable breaking rules: to enjoy breaking them, and to be indifferent to them. I call these two cases being aggressively and passively independent-minded.\n",
    "\n",
    "The aggressively independent-minded are the naughty ones. Rules don't merely fail to stop them; breaking rules gives them additional energy. For this sort of person, delight at the sheer audacity of a project sometimes supplies enough activation energy to get it started.\n",
    "\n",
    "The other way to break rules is not to care about them, or perhaps even to know they exist. This is why novices and outsiders often make new discoveries; their ignorance of a field's assumptions acts as a source of temporary passive independent-mindedness. Aspies also seem to have a kind of immunity to conventional beliefs. Several I know say that this helps them to have new ideas.\n",
    "\n",
    "Strictness plus rule-breaking sounds like a strange combination. In popular culture they're opposed. But popular culture has a broken model in this respect. It implicitly assumes that issues are trivial ones, and in trivial matters strictness and rule-breaking are opposed. But in questions that really matter, only rule-breakers can be truly strict.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "An overlooked idea often doesn't lose till the semifinals. You do see it, subconsciously, but then another part of your subconscious shoots it down because it would be too weird, too risky, too much work, too controversial. This suggests an exciting possibility: if you could turn off such filters, you could see more new ideas.\n",
    "\n",
    "One way to do that is to ask what would be good ideas for someone else to explore. Then your subconscious won't shoot them down to protect you.\n",
    "\n",
    "You could also discover overlooked ideas by working in the other direction: by starting from what's obscuring them. Every cherished but mistaken principle is surrounded by a dead zone of valuable ideas that are unexplored because they contradict it.\n",
    "\n",
    "Religions are collections of cherished but mistaken principles. So anything that can be described either literally or metaphorically as a religion will have valuable unexplored ideas in its shadow. Copernicus and Darwin both made discoveries of this type. [18]\n",
    "\n",
    "What are people in your field religious about, in the sense of being too attached to some principle that might not be as self-evident as they think? What becomes possible if you discard it?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "People show much more originality in solving problems than in deciding which problems to solve. Even the smartest can be surprisingly conservative when deciding what to work on. People who'd never dream of being fashionable in any other way get sucked into working on fashionable problems.\n",
    "\n",
    "One reason people are more conservative when choosing problems than solutions is that problems are bigger bets. A problem could occupy you for years, while exploring a solution might only take days. But even so I think most people are too conservative. They're not merely responding to risk, but to fashion as well. Unfashionable problems are undervalued.\n",
    "\n",
    "One of the most interesting kinds of unfashionable problem is the problem that people think has been fully explored, but hasn't. Great work often takes something that already exists and shows its latent potential. Durer and Watt both did this. So if you're interested in a field that others think is tapped out, don't let their skepticism deter you. People are often wrong about this.\n",
    "\n",
    "Working on an unfashionable problem can be very pleasing. There's no hype or hurry. Opportunists and critics are both occupied elsewhere. The existing work often has an old-school solidity. And there's a satisfying sense of economy in cultivating ideas that would otherwise be wasted.\n",
    "\n",
    "But the most common type of overlooked problem is not explicitly unfashionable in the sense of being out of fashion. It just doesn't seem to matter as much as it actually does. How do you find these? By being self-indulgent â€” by letting your curiosity have its way, and tuning out, at least temporarily, the little voice in your head that says you should only be working on \"important\" problems.\n",
    "\n",
    "You do need to work on important problems, but almost everyone is too conservative about what counts as one. And if there's an important but overlooked problem in your neighborhood, it's probably already on your subconscious radar screen. So try asking yourself: if you were going to take a break from \"serious\" work to work on something just because it would be really interesting, what would you do? The answer is probably more important than it seems.\n",
    "\n",
    "Originality in choosing problems seems to matter even more than originality in solving them. That's what distinguishes the people who discover whole new fields. So what might seem to be merely the initial step â€” deciding what to work on â€” is in a sense the key to the whole game.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Few grasp this. One of the biggest misconceptions about new ideas is about the ratio of question to answer in their composition. People think big ideas are answers, but often the real insight was in the question.\n",
    "\n",
    "Part of the reason we underrate questions is the way they're used in schools. In schools they tend to exist only briefly before being answered, like unstable particles. But a really good question can be much more than that. A really good question is a partial discovery. How do new species arise? Is the force that makes objects fall to earth the same as the one that keeps planets in their orbits? By even asking such questions you were already in excitingly novel territory.\n",
    "\n",
    "Unanswered questions can be uncomfortable things to carry around with you. But the more you're carrying, the greater the chance of noticing a solution â€” or perhaps even more excitingly, noticing that two unanswered questions are the same.\n",
    "\n",
    "Sometimes you carry a question for a long time. Great work often comes from returning to a question you first noticed years before â€” in your childhood, even â€” and couldn't stop thinking about. People talk a lot about the importance of keeping your youthful dreams alive, but it's just as important to keep your youthful questions alive. [19]\n",
    "\n",
    "This is one of the places where actual expertise differs most from the popular picture of it. In the popular picture, experts are certain. But actually the more puzzled you are, the better, so long as (a) the things you're puzzled about matter, and (b) no one else understands them either.\n",
    "\n",
    "Think about what's happening at the moment just before a new idea is discovered. Often someone with sufficient expertise is puzzled about something. Which means that originality consists partly of puzzlement â€” of confusion! You have to be comfortable enough with the world being full of puzzles that you're willing to see them, but not so comfortable that you don't want to solve them. [20]\n",
    "\n",
    "It's a great thing to be rich in unanswered questions. And this is one of those situations where the rich get richer, because the best way to acquire new questions is to try answering existing ones. Questions don't just lead to answers, but also to more questions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The best questions grow in the answering. You notice a thread protruding from the current paradigm and try pulling on it, and it just gets longer and longer. So don't require a question to be obviously big before you try answering it. You can rarely predict that. It's hard enough even to notice the thread, let alone to predict how much will unravel if you pull on it.\n",
    "\n",
    "It's better to be promiscuously curious â€” to pull a little bit on a lot of threads, and see what happens. Big things start small. The initial versions of big things were often just experiments, or side projects, or talks, which then grew into something bigger. So start lots of small things.\n",
    "\n",
    "Being prolific is underrated. The more different things you try, the greater the chance of discovering something new. Understand, though, that trying lots of things will mean trying lots of things that don't work. You can't have a lot of good ideas without also having a lot of bad ones. [21]\n",
    "\n",
    "Though it sounds more responsible to begin by studying everything that's been done before, you'll learn faster and have more fun by trying stuff. And you'll understand previous work better when you do look at it. So err on the side of starting. Which is easier when starting means starting small; those two ideas fit together like two puzzle pieces.\n",
    "\n",
    "How do you get from starting small to doing something great? By making successive versions. Great things are almost always made in successive versions. You start with something small and evolve it, and the final version is both cleverer and more ambitious than anything you could have planned.\n",
    "\n",
    "It's particularly useful to make successive versions when you're making something for people â€” to get an initial version in front of them quickly, and then evolve it based on their response.\n",
    "\n",
    "Begin by trying the simplest thing that could possibly work. Surprisingly often, it does. If it doesn't, this will at least get you started.\n",
    "\n",
    "Don't try to cram too much new stuff into any one version. There are names for doing this with the first version (taking too long to ship) and the second (the second system effect), but these are both merely instances of a more general principle.\n",
    "\n",
    "An early version of a new project will sometimes be dismissed as a toy. It's a good sign when people do this. That means it has everything a new idea needs except scale, and that tends to follow. [22]\n",
    "\n",
    "The alternative to starting with something small and evolving it is to plan in advance what you're going to do. And planning does usually seem the more responsible choice. It sounds more organized to say \"we're going to do x and then y and then z\" than \"we're going to try x and see what happens.\" And it is more organized; it just doesn't work as well.\n",
    "\n",
    "Planning per se isn't good. It's sometimes necessary, but it's a necessary evil â€” a response to unforgiving conditions. It's something you have to do because you're working with inflexible media, or because you need to coordinate the efforts of a lot of people. If you keep projects small and use flexible media, you don't have to plan as much, and your designs can evolve instead.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Take as much risk as you can afford. In an efficient market, risk is proportionate to reward, so don't look for certainty, but for a bet with high expected value. If you're not failing occasionally, you're probably being too conservative.\n",
    "\n",
    "Though conservatism is usually associated with the old, it's the young who tend to make this mistake. Inexperience makes them fear risk, but it's when you're young that you can afford the most.\n",
    "\n",
    "Even a project that fails can be valuable. In the process of working on it, you'll have crossed territory few others have seen, and encountered questions few others have asked. And there's probably no better source of questions than the ones you encounter in trying to do something slightly too hard.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Use the advantages of youth when you have them, and the advantages of age once you have those. The advantages of youth are energy, time, optimism, and freedom. The advantages of age are knowledge, efficiency, money, and power. With effort you can acquire some of the latter when young and keep some of the former when old.\n",
    "\n",
    "The old also have the advantage of knowing which advantages they have. The young often have them without realizing it. The biggest is probably time. The young have no idea how rich they are in time. The best way to turn this time to advantage is to use it in slightly frivolous ways: to learn about something you don't need to know about, just out of curiosity, or to try building something just because it would be cool, or to become freakishly good at something.\n",
    "\n",
    "That \"slightly\" is an important qualification. Spend time lavishly when you're young, but don't simply waste it. There's a big difference between doing something you worry might be a waste of time and doing something you know for sure will be. The former is at least a bet, and possibly a better one than you think. [23]\n",
    "\n",
    "The most subtle advantage of youth, or more precisely of inexperience, is that you're seeing everything with fresh eyes. When your brain embraces an idea for the first time, sometimes the two don't fit together perfectly. Usually the problem is with your brain, but occasionally it's with the idea. A piece of it sticks out awkwardly and jabs you when you think about it. People who are used to the idea have learned to ignore it, but you have the opportunity not to. [24]\n",
    "\n",
    "So when you're learning about something for the first time, pay attention to things that seem wrong or missing. You'll be tempted to ignore them, since there's a 99% chance the problem is with you. And you may have to set aside your misgivings temporarily to keep progressing. But don't forget about them. When you've gotten further into the subject, come back and check if they're still there. If they're still viable in the light of your present knowledge, they probably represent an undiscovered idea.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "One of the most valuable kinds of knowledge you get from experience is to know what you don't have to worry about. The young know all the things that could matter, but not their relative importance. So they worry equally about everything, when they should worry much more about a few things and hardly at all about the rest.\n",
    "\n",
    "But what you don't know is only half the problem with inexperience. The other half is what you do know that ain't so. You arrive at adulthood with your head full of nonsense â€” bad habits you've acquired and false things you've been taught â€” and you won't be able to do great work till you clear away at least the nonsense in the way of whatever type of work you want to do.\n",
    "\n",
    "Much of the nonsense left in your head is left there by schools. We're so used to schools that we unconsciously treat going to school as identical with learning, but in fact schools have all sorts of strange qualities that warp our ideas about learning and thinking.\n",
    "\n",
    "For example, schools induce passivity. Since you were a small child, there was an authority at the front of the class telling all of you what you had to learn and then measuring whether you did. But neither classes nor tests are intrinsic to learning; they're just artifacts of the way schools are usually designed.\n",
    "\n",
    "The sooner you overcome this passivity, the better. If you're still in school, try thinking of your education as your project, and your teachers as working for you rather than vice versa. That may seem a stretch, but it's not merely some weird thought experiment. It's the truth, economically, and in the best case it's the truth intellectually as well. The best teachers don't want to be your bosses. They'd prefer it if you pushed ahead, using them as a source of advice, rather than being pulled by them through the material.\n",
    "\n",
    "Schools also give you a misleading impression of what work is like. In school they tell you what the problems are, and they're almost always soluble using no more than you've been taught so far. In real life you have to figure out what the problems are, and you often don't know if they're soluble at all.\n",
    "\n",
    "But perhaps the worst thing schools do to you is train you to win by hacking the test. You can't do great work by doing that. You can't trick God. So stop looking for that kind of shortcut. The way to beat the system is to focus on problems and solutions that others have overlooked, not to skimp on the work itself.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Don't think of yourself as dependent on some gatekeeper giving you a \"big break.\" Even if this were true, the best way to get it would be to focus on doing good work rather than chasing influential people.\n",
    "\n",
    "And don't take rejection by committees to heart. The qualities that impress admissions officers and prize committees are quite different from those required to do great work. The decisions of selection committees are only meaningful to the extent that they're part of a feedback loop, and very few are.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "People new to a field will often copy existing work. There's nothing inherently bad about that. There's no better way to learn how something works than by trying to reproduce it. Nor does copying necessarily make your work unoriginal. Originality is the presence of new ideas, not the absence of old ones.\n",
    "\n",
    "There's a good way to copy and a bad way. If you're going to copy something, do it openly instead of furtively, or worse still, unconsciously. This is what's meant by the famously misattributed phrase \"Great artists steal.\" The really dangerous kind of copying, the kind that gives copying a bad name, is the kind that's done without realizing it, because you're nothing more than a train running on tracks laid down by someone else. But at the other extreme, copying can be a sign of superiority rather than subordination. [25]\n",
    "\n",
    "In many fields it's almost inevitable that your early work will be in some sense based on other people's. Projects rarely arise in a vacuum. They're usually a reaction to previous work. When you're first starting out, you don't have any previous work; if you're going to react to something, it has to be someone else's. Once you're established, you can react to your own. But while the former gets called derivative and the latter doesn't, structurally the two cases are more similar than they seem.\n",
    "\n",
    "Oddly enough, the very novelty of the most novel ideas sometimes makes them seem at first to be more derivative than they are. New discoveries often have to be conceived initially as variations of existing things, even by their discoverers, because there isn't yet the conceptual vocabulary to express them.\n",
    "\n",
    "There are definitely some dangers to copying, though. One is that you'll tend to copy old things â€” things that were in their day at the frontier of knowledge, but no longer are.\n",
    "\n",
    "And when you do copy something, don't copy every feature of it. Some will make you ridiculous if you do. Don't copy the manner of an eminent 50 year old professor if you're 18, for example, or the idiom of a Renaissance poem hundreds of years later.\n",
    "\n",
    "Some of the features of things you admire are flaws they succeeded despite. Indeed, the features that are easiest to imitate are the most likely to be the flaws.\n",
    "\n",
    "This is particularly true for behavior. Some talented people are jerks, and this sometimes makes it seem to the inexperienced that being a jerk is part of being talented. It isn't; being talented is merely how they get away with it.\n",
    "\n",
    "One of the most powerful kinds of copying is to copy something from one field into another. History is so full of chance discoveries of this type that it's probably worth giving chance a hand by deliberately learning about other kinds of work. You can take ideas from quite distant fields if you let them be metaphors.\n",
    "\n",
    "Negative examples can be as inspiring as positive ones. In fact you can sometimes learn more from things done badly than from things done well; sometimes it only becomes clear what's needed when it's missing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "If a lot of the best people in your field are collected in one place, it's usually a good idea to visit for a while. It will increase your ambition, and also, by showing you that these people are human, increase your self-confidence. [26]\n",
    "\n",
    "If you're earnest you'll probably get a warmer welcome than you might expect. Most people who are very good at something are happy to talk about it with anyone who's genuinely interested. If they're really good at their work, then they probably have a hobbyist's interest in it, and hobbyists always want to talk about their hobbies.\n",
    "\n",
    "It may take some effort to find the people who are really good, though. Doing great work has such prestige that in some places, particularly universities, there's a polite fiction that everyone is engaged in it. And that is far from true. People within universities can't say so openly, but the quality of the work being done in different departments varies immensely. Some departments have people doing great work; others have in the past; others never have.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Seek out the best colleagues. There are a lot of projects that can't be done alone, and even if you're working on one that can be, it's good to have other people to encourage you and to bounce ideas off.\n",
    "\n",
    "Colleagues don't just affect your work, though; they also affect you. So work with people you want to become like, because you will.\n",
    "\n",
    "Quality is more important than quantity in colleagues. It's better to have one or two great ones than a building full of pretty good ones. In fact it's not merely better, but necessary, judging from history: the degree to which great work happens in clusters suggests that one's colleagues often make the difference between doing great work and not.\n",
    "\n",
    "How do you know when you have sufficiently good colleagues? In my experience, when you do, you know. Which means if you're unsure, you probably don't. But it may be possible to give a more concrete answer than that. Here's an attempt: sufficiently good colleagues offer surprising insights. They can see and do things that you can't. So if you have a handful of colleagues good enough to keep you on your toes in this sense, you're probably over the threshold.\n",
    "\n",
    "Most of us can benefit from collaborating with colleagues, but some projects require people on a larger scale, and starting one of those is not for everyone. If you want to run a project like that, you'll have to become a manager, and managing well takes aptitude and interest like any other kind of work. If you don't have them, there is no middle path: you must either force yourself to learn management as a second language, or avoid such projects. [27]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Husband your morale. It's the basis of everything when you're working on ambitious projects. You have to nurture and protect it like a living organism.\n",
    "\n",
    "Morale starts with your view of life. You're more likely to do great work if you're an optimist, and more likely to if you think of yourself as lucky than if you think of yourself as a victim.\n",
    "\n",
    "Indeed, work can to some extent protect you from your problems. If you choose work that's pure, its very difficulties will serve as a refuge from the difficulties of everyday life. If this is escapism, it's a very productive form of it, and one that has been used by some of the greatest minds in history.\n",
    "\n",
    "Morale compounds via work: high morale helps you do good work, which increases your morale and helps you do even better work. But this cycle also operates in the other direction: if you're not doing good work, that can demoralize you and make it even harder to. Since it matters so much for this cycle to be running in the right direction, it can be a good idea to switch to easier work when you're stuck, just so you start to get something done.\n",
    "\n",
    "One of the biggest mistakes ambitious people make is to allow setbacks to destroy their morale all at once, like a balloon bursting. You can inoculate yourself against this by explicitly considering setbacks a part of your process. Solving hard problems always involves some backtracking.\n",
    "\n",
    "Doing great work is a depth-first search whose root node is the desire to. So \"If at first you don't succeed, try, try again\" isn't quite right. It should be: If at first you don't succeed, either try again, or backtrack and then try again.\n",
    "\n",
    "\"Never give up\" is also not quite right. Obviously there are times when it's the right choice to eject. A more precise version would be: Never let setbacks panic you into backtracking more than you need to. Corollary: Never abandon the root node.\n",
    "\n",
    "It's not necessarily a bad sign if work is a struggle, any more than it's a bad sign to be out of breath while running. It depends how fast you're running. So learn to distinguish good pain from bad. Good pain is a sign of effort; bad pain is a sign of damage.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "An audience is a critical component of morale. If you're a scholar, your audience may be your peers; in the arts, it may be an audience in the traditional sense. Either way it doesn't need to be big. The value of an audience doesn't grow anything like linearly with its size. Which is bad news if you're famous, but good news if you're just starting out, because it means a small but dedicated audience can be enough to sustain you. If a handful of people genuinely love what you're doing, that's enough.\n",
    "\n",
    "To the extent you can, avoid letting intermediaries come between you and your audience. In some types of work this is inevitable, but it's so liberating to escape it that you might be better off switching to an adjacent type if that will let you go direct. [28]\n",
    "\n",
    "The people you spend time with will also have a big effect on your morale. You'll find there are some who increase your energy and others who decrease it, and the effect someone has is not always what you'd expect. Seek out the people who increase your energy and avoid those who decrease it. Though of course if there's someone you need to take care of, that takes precedence.\n",
    "\n",
    "Don't marry someone who doesn't understand that you need to work, or sees your work as competition for your attention. If you're ambitious, you need to work; it's almost like a medical condition; so someone who won't let you work either doesn't understand you, or does and doesn't care.\n",
    "\n",
    "Ultimately morale is physical. You think with your body, so it's important to take care of it. That means exercising regularly, eating and sleeping well, and avoiding the more dangerous kinds of drugs. Running and walking are particularly good forms of exercise because they're good for thinking. [29]\n",
    "\n",
    "People who do great work are not necessarily happier than everyone else, but they're happier than they'd be if they didn't. In fact, if you're smart and ambitious, it's dangerous not to be productive. People who are smart and ambitious but don't achieve much tend to become bitter.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "It's ok to want to impress other people, but choose the right people. The opinion of people you respect is signal. Fame, which is the opinion of a much larger group you might or might not respect, just adds noise.\n",
    "\n",
    "The prestige of a type of work is at best a trailing indicator and sometimes completely mistaken. If you do anything well enough, you'll make it prestigious. So the question to ask about a type of work is not how much prestige it has, but how well it could be done.\n",
    "\n",
    "Competition can be an effective motivator, but don't let it choose the problem for you; don't let yourself get drawn into chasing something just because others are. In fact, don't let competitors make you do anything much more specific than work harder.\n",
    "\n",
    "Curiosity is the best guide. Your curiosity never lies, and it knows more than you do about what's worth paying attention to.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Notice how often that word has come up. If you asked an oracle the secret to doing great work and the oracle replied with a single word, my bet would be on \"curiosity.\"\n",
    "\n",
    "That doesn't translate directly to advice. It's not enough just to be curious, and you can't command curiosity anyway. But you can nurture it and let it drive you.\n",
    "\n",
    "Curiosity is the key to all four steps in doing great work: it will choose the field for you, get you to the frontier, cause you to notice the gaps in it, and drive you to explore them. The whole process is a kind of dance with curiosity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Believe it or not, I tried to make this essay as short as I could. But its length at least means it acts as a filter. If you made it this far, you must be interested in doing great work. And if so you're already further along than you might realize, because the set of people willing to want to is small.\n",
    "\n",
    "The factors in doing great work are factors in the literal, mathematical sense, and they are: ability, interest, effort, and luck. Luck by definition you can't do anything about, so we can ignore that. And we can assume effort, if you do in fact want to do great work. So the problem boils down to ability and interest. Can you find a kind of work where your ability and interest will combine to yield an explosion of new ideas?\n",
    "\n",
    "Here there are grounds for optimism. There are so many different ways to do great work, and even more that are still undiscovered. Out of all those different types of work, the one you're most suited for is probably a pretty close match. Probably a comically close match. It's just a question of finding it, and how far into it your ability and interest can take you. And you can only answer that by trying.\n",
    "\n",
    "Many more people could try to do great work than do. What holds them back is a combination of modesty and fear. It seems presumptuous to try to be Newton or Shakespeare. It also seems hard; surely if you tried something like that, you'd fail. Presumably the calculation is rarely explicit. Few people consciously decide not to try to do great work. But that's what's going on subconsciously; they shy away from the question.\n",
    "\n",
    "So I'm going to pull a sneaky trick on you. Do you want to do great work, or not? Now you have to decide consciously. Sorry about that. I wouldn't have done it to a general audience. But we already know you're interested.\n",
    "\n",
    "Don't worry about being presumptuous. You don't have to tell anyone. And if it's too hard and you fail, so what? Lots of people have worse problems than that. In fact you'll be lucky if it's the worst problem you have.\n",
    "\n",
    "Yes, you'll have to work hard. But again, lots of people have to work hard. And if you're working on something you find very interesting, which you necessarily will if you're on the right path, the work will probably feel less burdensome than a lot of your peers'.\n",
    "\n",
    "The discoveries are out there, waiting to be made. Why not by you?\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "pg=pg.split('.')\n",
    "rh=rh.split('.')\n",
    "#above .7 = semantically similar\n",
    "#below 0 = different subject different verb\n",
    "#below .3 similar subject\n",
    "#cs=computeSimilarity(pg, rh)\n",
    "\n",
    "\n",
    "# pge = encode(pg)\n",
    "# rhe = encode(rh)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sorted_pge, indices = pge.sort()\n",
    "sorted_rhe,indices =  rhe.sort()\n",
    "\n",
    "#use indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68003089",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs\n",
    "\n",
    "def display_pair(pair):\n",
    "    i, j = pair['index']\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(rh[i], pg[j], pair['score']))\n",
    "    #print(\"{} \\t\\t  \\t\\t Score: {:.4f}\".format(pg[j] if len(rh[i]) > len(pg[j]) else rh[i], pair['score']))\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    display_pair(pairs[i])\n",
    "    \n",
    "    \n",
    "#you know they are similar enough -> how do you merge - merge by relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677bfc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSimilarity(s1, s2):\n",
    "    return util.cos_sim(s1, s2)\n",
    "computeSimilarity(sorted_pge, sorted_rhe[:664])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fec22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = 'i went to the grocery store. I found a box of tomatoes. it was cool.'\n",
    "\n",
    "p2 = 'i went to the grocery store. i dropped a box of tomatoes. it was cool.'\n",
    "\n",
    "computeSimilarity(p1, p2)\n",
    "\n",
    "\n",
    "#along with similarity -> compute how they are different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49f8d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "one= 'i went to the mall with my cousins and ate subway'\n",
    "\n",
    "two = 'my cousins and i ate subway at the mall.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c554c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag(word_tokenize(one)), pos_tag(word_tokenize(two)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec441de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "computeSimilarity('ate soup', 'love soup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer,util\n",
    "\n",
    "computeSimilarity('ate a carrot', 'ate some carrots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8703e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "person1 = 'i want the government to add a railroad between montreal and kansas'\n",
    "person5 = 'i want the government to add a railroad between kansas and montreal'\n",
    "person2 = 'i want to build a registry for synthetic plants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6df128",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encode([person1, person2], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea54beca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "trythisone = 'why do you call it an xbox 360? because when you see it, you turn 360 degrees and walk away.'\n",
    "\n",
    "data = \"\"\"I used to play piano by ear, but now I use my hands.\n",
    "\n",
    "I'm reading a book about anti-gravity, and it's impossible to put down.\n",
    "\n",
    "The math book looked sad, so I asked what's the problem. It said it had too many problems.\n",
    "\n",
    "A bicycle can't stand on its own because it's two-tired.\n",
    "\n",
    "I told my friend 10 jokes to make him laugh, but no pun in ten did.\n",
    "\n",
    "Did you hear about the guy who lost his left side? He's all right now.\n",
    "\n",
    "I used to be a baker, but I couldn't make enough dough.\n",
    "\n",
    "When a clock is hungry, it goes back four seconds.\n",
    "\n",
    "I got a job at a bakery because I kneaded the dough.\n",
    "\n",
    "I'm friends with all electricians. We have good current connections.\"\"\".split('\\n')\n",
    "\n",
    "data2 = \"\"\"\n",
    "How do you throw a space party? You planet.\n",
    "\n",
    "How was Rome split in two? With a pair of Ceasars.\n",
    "\n",
    "Nope. Unintended.\n",
    "\n",
    "The shovel was a ground breaking invention, but everyone was blow away by the leaf blower.\n",
    "\n",
    "A scarecrow says, \"This job isn't for everyone, but hay, it's in my jeans.\"\n",
    "\n",
    "A Buddhist walks up to a hot dog stand and says \"Make me one with everything.\"\n",
    "\n",
    "Did you hear about the guy who lost the left side of his body? He's alright now.\n",
    "\n",
    "What do you call a girl with one leg that's shorter than the other? Ilene.\n",
    "\n",
    "The broom swept the nation away.\n",
    "\n",
    "I did a theatrical performance on puns. It was a play on words.\n",
    "\n",
    "What does a clock do when it's hungry? It goes back for seconds.\n",
    "\n",
    "What do you do with a dead chemist? You barium.\n",
    "\n",
    "I bet the person who created the door knocker won a Nobel prize.\n",
    "\n",
    "Towels canâ€™t tell jokes. They have a dry sense of humor.\n",
    "\n",
    "Two birds are sitting on a perch and one says â€œDo you smell fish?â€\n",
    "\n",
    "Did you hear about the cheese factory that exploded in france? There was nothing but des brie.\n",
    "\n",
    "Do you know sign language? You should learn it, itâ€™s pretty handy.\n",
    "\n",
    "What do you call a beautiful pumpkin? GOURDgeous.\n",
    "\n",
    "Why did one banana spy on the other? Because she was appealing.\n",
    "\n",
    "What do you call a cow with no legs? Ground beef.\n",
    "\n",
    "What do you call a cow with two legs? Lean beef.\n",
    "\n",
    "What do you call a cow with all of its legs? High steaks.\n",
    "\n",
    "A cross eyed teacher couldnâ€™t control his pupils.\n",
    "\n",
    "After the accident, the juggler didnâ€™t have the balls to do it.\n",
    "\n",
    "I used to be afraid of hurdles, but I got over it.\n",
    "\n",
    "To write with a broken pencil is pointless.\n",
    "\n",
    "I read a book on anti-gravity. I couldnâ€™t put it down.\n",
    "\n",
    "I couldnâ€™t remember how to throw a boomerang but it came back to me.\n",
    "\n",
    "What did the buffalo say to his son? Bison.\n",
    "\n",
    "What should you do if youâ€™re cold? Stand in the corner. Itâ€™s 90 degrees.\n",
    "\n",
    "How does Moses make coffee? Hebrews it.\n",
    "\n",
    "The energizer bunny went to jail. He was charged with battery.\n",
    "\n",
    "What did the alien say to the pitcher of water? Take me to your liter.\n",
    "\n",
    "What happens when you eat too many spaghettiOs? You have a vowel movement.\n",
    "\n",
    "The soldier who survived mustard gas and pepper spray was a seasoned veteran.\n",
    "\n",
    "Sausage puns are the wurst.\n",
    "\n",
    "What do you call a bear with no teeth? A gummy bear.\n",
    "\n",
    "How did Darth Vader know what luke was getting him for his birthday? He could sense his presence.\n",
    "\n",
    "Why shouldnâ€™t you trust atoms? They make up everything.\n",
    "\n",
    "Whatâ€™s the difference between a bench, a fish, and a bucket of glue? You canâ€™t tune a bench but you can tuna fish. I bet you got stuck on the bucket of glue part.\n",
    "\n",
    "Whatâ€™s it called when you have too many aliens? Extraterrestrials.\n",
    "\n",
    "Want to hear a pizza joke? Nevermind, itâ€™s too cheesy.\n",
    "\n",
    "What do you call a fake noodle? An impasta.\n",
    "\n",
    "What do cows tell each other at bedtime? Dairy tales.\n",
    "\n",
    "Why canâ€™t you take inventory in Afghanistan? Because of the tally ban.\n",
    "\n",
    "Why didnâ€™t the lion win the race? Because he was racing a cheetah.\n",
    "\n",
    "Why did the man dig a hole in his neighborâ€™s backyard and fill it with water? Because he meant well.\n",
    "\n",
    "What happens to nitrogen when the sun comes up? It becomes daytrogen.\n",
    "\n",
    "Whatâ€™s it called when you put a cow in an elevator? Raising the steaks.\n",
    "\n",
    "Whatâ€™s americaâ€™s favorite soda? Mini soda.\n",
    "\n",
    "Why did the tomato turn red? Because it saw the salad dressing.\n",
    "\n",
    "What kind of car does a sheep drive? A lamborghini, but if that breaks down they drive their SuBAHHru.\n",
    "\n",
    "What do you call a spanish pig? Porque.\n",
    "\n",
    "What do you call a line of rabbits marching backwards? A receding hairline.\n",
    "\n",
    "Why donâ€™t vampires go to barbecues? They donâ€™t like steak.\n",
    "\n",
    "A cabbage and celery walk into a bar and the cabbage gets served first because he was a head.\n",
    "\n",
    "How do trees access the internet? They log on.\n",
    "\n",
    "Why should you never trust a train? They have loco motives.\n",
    "\n",
    "\"\"\".split('\\n')\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "\n",
    "def isPun(one, two):\n",
    "    return two in prouncing.rhymes(one)\n",
    "\n",
    "def isPunSentence(sentence):\n",
    "    tokens = sentence.split(' ')\n",
    "    hasNotFoundRhyme = False\n",
    "    for i in tokens:\n",
    "        for j in tokens:\n",
    "            if i ==j: continue\n",
    "            if isPun(one, two): hasNotFoundRhyme = True\n",
    "    return hasNotFoundRhyme\n",
    "\n",
    "\n",
    "data3 = 'What do you call a fake noodle? An impasta.'\n",
    "\n",
    "[correction(data) for data in data3.split(' ')]\n",
    "correction('impasta')\n",
    "#get the original source code to humor \n",
    "\n",
    "#anaylze humor + scene understaning in every transcript in modern family, seinfeld, parks+rec, south park\n",
    "\n",
    "#train LLM from most upvoted comments from hacker news, reddit and so on -> give tptacek+ patio11 personality + etc\n",
    "\n",
    "fix_spelling = pipeline(\"text2text-generation\",model=\"oliverguhr/spelling-correction-english-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarize(text) :\n",
    "    summarizer = pipeline(\"summarization\", model=\"stevhliu/my_awesome_billsum_model\")\n",
    "    return summarizer(text)\n",
    "\n",
    "summarize('im going to go for a walk and type type tpye until text editor is cooler than pasta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
